[
["index.html", "Science des données biologiques 2 Préambule", " Science des données biologiques 2 Philippe Grosjean &amp; Guyliann Engels (avec des contributions de Raphaël Conotte) 2020-08-21 Préambule Cet ouvrage a été écrit pour le cours de science des données II : analyse et modélisation à Mons pour l’année académique 2019-2020 (UMONS). Afin de trouver la dernière version disponible de cet ouvrage suivez le lien suivant : - https://wp.sciviews.org/sdd-umons2 Cet ouvrage interactif est le second d’une série de trois ouvrages traitant de la science des données biologiques. L’écriture de cette suite de livres a débuté au cours de l’année académique 2018-2019. Pour l’année académique 2019-2020, cet ouvrage interactif sera le support du cours suivant : Science des données II : Analyse et modélisation, UMONS dont le responsable est Philippe Grosjean Cet ouvrage est conçu pour être utilisé de manière interactive en ligne. En effet, nous y ajoutons des vidéos, des démonstrations interactives, et des exercices sous forme de questionnaires interactifs également. Ces différents éléments ne sont, bien évidemment, utilisables qu’en ligne. Le matériel dans cet ouvrage est distribué sous licence CC BY-NC-SA 4.0. "],
["vue-generale-des-cours.html", "Vue générale des cours", " Vue générale des cours Le cours de Science des données II: analyse et modélisation est dispensé aux biologistes de troisième Bachelier en Faculté des Sciences de l’Université de Mons à partir de l’année académique 2019-2020. La matière est divisée en huit modules de 6h chacun en présentiel. Il nécessitera environ un tiers de ce temps (voir plus, en fonction de votre rythme et de votre technique d’apprentissage) en travail à domicile. Cette matière fait suite au premier cours dont le contenu est considéré comme assimilé (voir https://biodatascience-course.sciviews.org/sdd-umons/). La première moitié du cours est consacrée à la modélisation, un domaine particulièrement important de la science des données qui étend les concepts déjà vu au cours 1 d’analyse de variance et de corrélation entre deux variables. Ces quatre modules formeront aussi un socle sur lequel nous pourrons élaborer les techniques d’apprentissage machine (classification supervisée), et puis ensuite l’apprentissage profond à la base de l’intelligence artificielle qui seront abordées plus tard dans le cours 3. Cette partie est dense, mais ultra importante ! La seconde moitié s’intéressera à l’exploration des données, encore appelée analyse des données qui vise à découvrir des caractéristiques intéressantes dans des très gros jeux de données. Ces techniques sont d’autant plus utiles que les données volumineuses deviennent de plus en plus courantes en biologie. "],
["materiel-pedagogique.html", "Matériel pédagogique", " Matériel pédagogique Le matériel pédagogique, rassemblé dans ce syllabus interactif est aussi varié que possible. Vous pourrez ainsi piocher dans l’offre en fonction de vos envies et de votre profil d’apprenant pour optimiser votre travail. Vous trouverez: le présent ouvrage en ligne, des tutoriaux interactifs (réalisés avec un logiciel appelé learnr). Vous pourrez exécuter ces tutoriaux directement sur votre ordinateur, et vous aurez alors accès à des pages Web réactives contenant des explications, des exercices et des quizzs en ligne, des slides de présentations, des dépôts Github Classroom dans la section BioDataScience-Course pour réaliser et documenter vos travaux personnels. des renvois vers des documents externes en ligne, types vidéos youtube ou vimeo, des ouvrages en ligne en anglais ou en français, des blogs, des tutoriaux, des parties gratuites de cours Datacamp ou équivalents, des questions sur des sites comme “Stackoverflow” ou issues des “mailing lists” R, … Tout ce matériel est accessible à partir du site Web du cours, du présent syllabus interactif (et de Moodle pour les étudiants de l’UMONS). Ces derniers ont aussi accès au dossier SDD sur StudentTemp en Intranet à l’UMONS. Les aspects pratiques seront à réaliser en utilisant la ‘SciViews Box’, une machine virtuelle préconfigurée. Nous installerons ensemble la nouvelle version de cette SciViews Box au premier cours. Il est donc très important que vous soyez présent à ce cours, et vous pouvez venir aussi si vous le souhaitez avec votre propre ordinateur portable comme pour le cours 1. Enfin, vous pourrez poser vos questions par mail à l’adresse sdd@sciviews.org. System information sessioninfo::session_info() # ─ Session info ────────────────────────────────────────────────────────── # setting value # version R version 3.5.3 (2019-03-11) # os Ubuntu 18.04.2 LTS # system x86_64, linux-gnu # ui X11 # language (EN) # collate en_US.UTF-8 # ctype en_US.UTF-8 # tz Europe/Brussels # date 2020-08-21 # # ─ Packages ────────────────────────────────────────────────────────────── # package * version date lib source # assertthat 0.2.1 2019-03-21 [2] CRAN (R 3.5.3) # bookdown 0.9 2018-12-21 [2] CRAN (R 3.5.3) # cli 1.1.0 2019-03-19 [2] CRAN (R 3.5.3) # colorspace 1.4-1 2019-03-18 [2] CRAN (R 3.5.3) # crayon 1.3.4 2017-09-16 [2] CRAN (R 3.5.3) # digest 0.6.18 2018-10-10 [2] CRAN (R 3.5.3) # dplyr 0.8.0.1 2019-02-15 [2] CRAN (R 3.5.3) # evaluate 0.13 2019-02-12 [2] CRAN (R 3.5.3) # farver 1.1.0 2018-11-20 [2] CRAN (R 3.5.3) # gganimate 1.0.3 2019-04-02 [2] CRAN (R 3.5.3) # ggplot2 3.1.1 2019-04-07 [2] CRAN (R 3.5.3) # glue 1.3.1 2019-03-12 [2] CRAN (R 3.5.3) # gtable 0.3.0 2019-03-25 [2] CRAN (R 3.5.3) # hms 0.4.2 2018-03-10 [2] CRAN (R 3.5.3) # htmltools 0.3.6 2017-04-28 [2] CRAN (R 3.5.3) # inline 0.3.15 2018-05-18 [2] CRAN (R 3.5.3) # knitr 1.22 2019-03-08 [2] CRAN (R 3.5.3) # lazyeval 0.2.2 2019-03-15 [2] CRAN (R 3.5.3) # magick 2.0 2018-10-05 [2] CRAN (R 3.5.3) # magrittr 1.5 2014-11-22 [2] CRAN (R 3.5.3) # munsell 0.5.0 2018-06-12 [2] CRAN (R 3.5.3) # pillar 1.3.1 2018-12-15 [2] CRAN (R 3.5.3) # pkgconfig 2.0.2 2018-08-16 [2] CRAN (R 3.5.3) # plyr 1.8.4 2016-06-08 [2] CRAN (R 3.5.3) # prettyunits 1.0.2 2015-07-13 [2] CRAN (R 3.5.3) # progress 1.2.0 2018-06-14 [2] CRAN (R 3.5.3) # purrr 0.3.2 2019-03-15 [2] CRAN (R 3.5.3) # R6 2.4.0 2019-02-14 [2] CRAN (R 3.5.3) # Rcpp 1.0.1 2019-03-17 [2] CRAN (R 3.5.3) # rlang 0.3.4 2019-04-07 [2] CRAN (R 3.5.3) # rmarkdown 1.12 2019-03-14 [2] CRAN (R 3.5.3) # rstudioapi 0.10 2019-03-19 [2] CRAN (R 3.5.3) # scales 1.0.0 2018-08-09 [2] CRAN (R 3.5.3) # sessioninfo 1.1.1 2018-11-05 [2] CRAN (R 3.5.3) # stringi 1.4.3 2019-03-12 [2] CRAN (R 3.5.3) # stringr 1.4.0 2019-02-10 [2] CRAN (R 3.5.3) # tibble 2.1.1 2019-03-16 [2] CRAN (R 3.5.3) # tidyselect 0.2.5 2018-10-11 [2] CRAN (R 3.5.3) # tweenr 1.0.1 2018-12-14 [2] CRAN (R 3.5.3) # withr 2.1.2 2018-03-15 [2] CRAN (R 3.5.3) # xfun 0.6 2019-04-02 [2] CRAN (R 3.5.3) # yaml 2.2.0 2018-07-25 [2] CRAN (R 3.5.3) # # [1] /home/sv/R/x86_64-pc-linux-gnu-library/3.5 # [2] /usr/local/lib/R/site-library # [3] /usr/lib/R/site-library # [4] /usr/lib/R/library "],
["lm.html", "Module 1 Régression linéaire I", " Module 1 Régression linéaire I Objectifs Retrouver ses marques avec R, RStudio et la SciViews Box et découvrir les fonctions supplémentaires de la nouvelle version. Découvrir la régression linaire de manière intuitive. Découvrir les outils de diagnostic de la régression linéaire, en particulier l’analyse des résidus. Prérequis Avant de nous lancer tête baissée dans de la matière nouvelle, nous allons installer la dernière version de la SciViews Box. Une nouvelle version est disponible chaque année début septembre. Reportez-vous à l’appendice A pour son installation et pour la migration éventuelle de vos projets depuis la version précédente. Une fois la box installée, consacrez un petit quart d’heure à repérer les icônes nouvelles dans le Dock et dans le menu Applications. Vous retrouverez R et RStudio, mais dans des version plus récentes qui apportent également leur lot de nouveautés. Lancez RStudio et repérez ici aussi les nouveaux onglets et les nouvelles entrées de menu. Aidez-vous de l’aide en ligne ou de recherches sur le net pour vous familiariser avec ces nouvelles fonctionnalités. Une fois la nouvelle SciViews Box fonctionnelle sur votre ordinateur, vous allez réaliser une séance d’exercice couvrant les points essentiels des notions abordées dans le livre science des données biologiques partie 1, histoire de rafraîchir vos connaissances. Les tutoriaux learnr auxquels vous êtes maintenant habitués seront là pour vous aider à auto-évaluer votre progression. Pour le cours 2, ces tutoriaux sont dans le package BioDataScience2 que vous venez normalement d’installer si vous avez bien suivi toutes les instructions de configuration de votre SciViews Box (sinon, vérifiez votre configuration). Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["modele.html", "1.1 Modèle", " 1.1 Modèle Qu’est-ce qu’un “modèle” en science des données et en statistique ? Il s’agit d’une représentation simplifiée sous forme mathématique du mécanisme responsable de la distribution des observations. Rassurez-vous, dans ce module, le côté mathématique du problème sera volontairement peu développé pour laisser une large place à une compréhension intuitive du modèle. Les seules notions clés à connaître ici concernent l’équation qui définit une droite quelconque dans le plan \\(xy\\) : \\[y = a \\ x + b\\] Cette équation comporte : deux variables \\(x\\) et \\(y\\) qui sont matérialisées par les axes des abscisses et des ordonnées dans le plan \\(xy\\). Ces variables prennent des valeurs bien définies pour les observations réalisées sur chaque individu du jeu de données. deux paramètres \\(a\\) et \\(b\\), respectivement la pente de la droite (\\(a\\)) et son ordonnée à l’origine (\\(b\\)). Ecrit de la sorte, \\(a\\) et \\(b\\) peuvent prendre n’importe quelle valeur et l’équation définit de manière généraliste toutes les droites possibles qui existent dans le plan \\(xy\\). Paramétrer ou paramétriser le modèle consiste à définir une et une seule droite en fixant les valeurs de \\(a\\) et de \\(b\\). Par exemple, si je décide de fixer \\(a = 0.35\\) et \\(b = -1.23\\), mon équation définit maintenant une droite bien précise dans le plan \\(xy\\) : \\[y = 0,35 \\ x - 1.23\\] La distinction entre variable et paramètre dans les équations précédentes semble difficile pour certaines personnes. C’est pourtant crucial de pouvoir le faire pour bien comprendre la suite. Alors, c’est le bon moment de relire attentivement ce qui est écrit ci-dessus et de le mémoriser avant d’aller plus avant ! 1.1.1 Pourquoi modéliser ? Le but de la modélisation consiste à découvrir l’équation mathématique de la droite (ou plus généralement, de la fonction) qui décrit au mieux la forme du nuage de points matérialisant les observations dans le plan \\(xy\\) (ou plus généralement dans un hyper-espace représenté par les différentes variables mesurées). Cette équation mathématique peut ensuite être utilisée de différentes façons, toutes plus utiles les unes que les autres : Aide à la compréhension du mécanisme sous-jacent qui a généré les données. Par exemple, si une droite représente bien la croissance pondérale d’un organisme dans le plan représenté par le logarithme du poids (P) en ordonnée et le temps (t) en abscisse, nous pourrons déduire que la croissance de cet organisme est probablement un mécanisme de type exponentiel (puisqu’une transformation inverse, c’est-à-dire logarithmique, linéarise alors le nuage de points). Attention ! Le modèle n’est pos le mécanisme sous-jacent de génération des données, mais utilisé habilement, ce modèle peut donner des indices utiles pour aider à découvrir ce mécanisme. Effectuer des prédictions. Le modèle paramétré pourra être utilisé pour prédire, par exemple, le poids probable d’un individu de la même population après un certain laps de temps. Comparer différents modèles. En présence de plusieurs populations, nous pourrons ajuster un modèle linéaire pour chacune d’elles et comparer ensuite les pentes des droites pour déterminer quelle population a le meilleur ou le moins bon taux de croissance. Explorer les relations entre variables. Sans aucunes connaissances sur le contexte qui a permit d’obtenir nos données, un modèle peut fournir des informations utiles pour orienter les recherches futures. Idéalement, un modèle devrait pouvoir servir à ces différentes applications. En pratique, comme le modèle est forcément une simplification de la réalité, des compromis doivent être concédés pour arriver à cette simplification. En fonction de son usage, les compromis possibles vont différer. Il s’en suit une spécialisation des modèles en modèles mécanistiques qui décrivent particulièrement bien le mécanisme sous-jacent (fréquents en physique, par exemple), les modèles prédictifs conçus pour calculer des nouvelles valeurs (que l’intelligence artificielle affectionne particulièrement), les modèles comparatifs, et enfin, les modèles exploratroires (utilisés dans la phase initiale de découverte et de description des données). Retenez simplement qu’un même modèle est rarement efficace sur les quatre tableaux simultanément. 1.1.2 Quand modéliser ? A chaque fois que deux ou plusieurs variables (quantitatives dans le cas de la régression) forment un nuage de points qui présente une forme particulière non sphérique, autrement dit, qu’une corrélation significative existe dans les données, un modèle peut être utile. Etant donné deux variables quantitatives, trois niveaux d’association de force croissante peuvent être définies entre ces deux variables : La corrélation quantifie juste l’allongement dans une direction préférentielle du nuage de points à l’aide des coefficients de corrélation linéaire de Pearson ou non linéaire de Spearman. Ce niveau d’association a été traité dans le module 12 du cours 1. Il est purement descriptif et n’implique aucunes autres hypothèses sur les données observées. La relation considère que la corrélation observée entre les deux variables est issue d’un mécanisme sous-jacent qui nous intéresse. Un modèle mathématique de l’association entre les deux variables matérialise de manière éventuellement simplifiée, ce mécanisme. Il permet de réaliser ensuite des calculs utiles. Nous verrons plus loin que des contraintes plus fortes doivent être supposées concernant le distribution des deux variables. La causalité précise encore le mécanisme sous-jacent dans le sens qu’elle exprime le fait que c’est la variation de l’une de ces variables qui est directement ou indirectement la cause de la variation de la seconde variable. Bien que des outils statistiques existent pour inférer une causalité (nous ne les aborderons pas dans ce cours), la causalité est plutôt étudiée via l’expérimentation : le biologiste contrôle et fait varier la variable supposée causale, toutes autres conditions par ailleurs invariables dans l’expérience. Il mesure alors et constate si la seconde variable répond ou non à ces variations1 et en déduit une causalité éventuelle. La distinction entre ces trois degrés d’association de deux variables est cruciale. Il est fréquent d’observer une confusion entre corrélation (ou relation) et causalité chez ceux qui ne comprennent pas bien la différence. Cela peut mener à des interprétations complètement erronées ! Comme ceci est à la fois crucial mais subtil, voici une vidéo issue de la série “les statistiques expliquées à mon chat” qui explique clairement le problème. Une troisième variable confondante peut en effet expliquer une corrélation, rendant alors la relation et/ou la causalité entre les deux variables fallacieuse… 1.1.3 Entraînement et confirmation En statistique, une règle universelle veut qu’une observation ne peut servir qu’une seule fois. Ainsi, toutes les données utilisées pour calculer le modèle ne peuvent pas servir simultanément à la confirmer. Il faut échantillonner d’autres valeurs pour effectuer cette confirmation. Il s’en suit une spécialisation des jeux de données en : jeu d’entraînement qui sert à établir le modèle jeu de confirmation ou de test qui sert à vérifier que le modèle est génaralisable car il est capable de prédire le comportement d’un autre jeu de données indépendant issu de la même population statistique. C’est une pratique cruciale de toujours confirmer son modèle, et donc, de prendre soin de séparer ses données en jeu d’entraînement et de test. Les bonnes façons de faire cela seront abordées au cours 3 dans la partie consacrée à l’apprentissage machine. Ici, nous nous focaliserons uniquement sur l’établissement du modèle dans la phase d’entraînement. Par conséquent, nous utiliserons toutes nos données pour cet entraînement, mais qu’il soit d’emblée bien clair qu’une confirmation du modèle est une seconde phase également indispensable. En biologie, le vivant peut être étudié essentiellement de deux manières complémentaires : par l’observation du monde qui nous entoure sans interférer, ou le moins possible, et par l’expérimentation où le biologiste fixe alors très précisément les conditions dans lesquelles il étudie ses organismes cibles. Les deux approches se prêtent à la modélisation mais seule l’expérimentation permet d’inférer avec certitude la causalité.↩ "],
["regression-lineaire-simple.html", "1.2 Régression linéaire simple", " 1.2 Régression linéaire simple Nous allons découvrir les bases de la régression linéaire de façon intuitive. Nous utilisons le jeu de données trees qui rassemble la mesure du diamètre, de la hauteur et du volume de bois de cerisiers noirs. # importation des données trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) Rapellons-nous que dans le chapitre 12 du livre science des données 1, nous avons étudié l’association de deux variables quantitatives (ou numériques). Nous utilisons donc une matrice de corrélation afin de mettre en évidence la corrélation entre nos trois variables qui composent le jeu de donnée trees. La fonction correlation() nous renvoie un tableau de la matrice de correlation avec l’indice de Pearson (corrélation linéaire) par défaut. C’est précisement ce coefficient qui nous intéresse dans le cadre d’une régression linéaire comme description préalable des données autant que pour nous guider dans le choix de nos variables. (trees_corr &lt;- correlation(trees)) # Matrix of Pearson&#39;s product-moment correlation: # (calculation uses everything) # diameter height volume # diameter 1.000 0.519 0.967 # height 0.519 1.000 0.597 # volume 0.967 0.597 1.000 Nous pouvons également observer cette matrice sous la forme d’un graphique plus convivial. plot(trees_corr, type = &quot;lower&quot;) Cependant, n’oubliez pas qu’il est indispensable de visualiser les nuages de points pour ne pas tomber dans le piège mis en avant par le jeu de données artificiel appelé “quartet d’Anscombe” qui montre très bien comment des données très différentes peuvent avoir même moyenne, même variance et même coefficient de corrélation. Un graphique de type matrice de nuages de points est tout indiqué ici. GGally::ggscatmat(as.data.frame(trees), 1:3) Nous observons une plus forte corrélation linéaire entre le volume et le diamètre. Intéressons nous à cette association. chart(trees, volume ~ diameter) + geom_point() Si vous deviez ajouter une droite permettant de représenter au mieux les données, où est ce que vous la placeriez ? Pour rappel, une droite respecte l’équation mathématique suivante : \\[y = a \\ x + b\\] dont a est la pente (slope en anglais) et b est l’ordonnée à l’origine (intercept en anglais). # Sélection de pentes et d&#39;ordonnées à l&#39;origine models &lt;- tibble( model = paste(&quot;model&quot;, 1:4, sep = &quot;-&quot;), slope = c(5, 5.5, 6, 0), intercept = c(-0.5, -0.95, -1.5, 0.85) ) chart(trees, volume ~ diameter) + geom_point() + geom_abline(data = models, aes(slope = slope, intercept = intercept, color = model)) + labs( color = &quot;Modèle&quot;) Nous avons quatre droites candidates pour représenter au mieux les observations. Quel est la meilleure d’entre elles selon vous ? 1.2.1 Quantifier l’ajustement d’un modèle Nous voulons identifier la meilleure régression, c’est-à-dire la régression le plus proche de nos données. Nous avons besoin d’une règle qui va nous permettre de quantifier la distance de nos observations à notre modèle afin d’obtenir la régression avec la plus faible distance possible de l’ensemble de nos observations. Décomposons le problème étape par étape et intéressons nous au model-1 (droite en rouge sur le graphique précédent). Calculer les valeurs de \\(y_i\\) prédites par le modèle que nous noterons par convention \\(\\hat y_i\\) (prononcez “y chapeau” ou “y hat” en anglais) pour chaque observation \\(i\\). # Calculer la valeur de y pour chaque valeur de x suivant le model souhaité # Création de notre fonction model &lt;- function(slope, intercept, x) { prediction &lt;- intercept + slope * x attributes(prediction) &lt;- NULL prediction } # Application de notre fonction yhat &lt;- model(slope = 5, intercept = -0.5, x = trees$diameter) # Affichage des résultats yhat # [1] 0.555 0.590 0.620 0.835 0.860 0.870 0.895 0.895 0.910 0.920 0.935 # [12] 0.950 0.950 0.985 1.025 1.140 1.140 1.190 1.240 1.255 1.280 1.305 # [23] 1.340 1.530 1.570 1.695 1.720 1.775 1.785 1.785 2.115 Calculer la distance entre les observations \\(y_i\\) et les prédictions par notre modèle \\(\\hat y_i\\), soit \\(y_i - \\hat y_i\\) Les distances que nous souhaitons calculer, sont appelées les résidus du modèle et sont notés \\(\\epsilon_i\\) (epsilon). Nous pouvons premièrement visualiser ces résidus graphiquement (ici en rouge par rapport à model-1) : Nous pouvons ensuite facilement calculer leurs valeurs comme ci-dessous : # Calculer la distance entre y et y barre # Création de notre fonction de calcul des résidus distance &lt;- function(observations, predictions) { residus &lt;- observations - predictions attributes(residus) &lt;- NULL residus } # Utilisation de la fonction resid &lt;- distance(observations = trees$volume, predictions = yhat) # Impression des résultats resid # [1] -0.263 -0.298 -0.331 -0.371 -0.328 -0.312 -0.453 -0.380 -0.270 -0.357 # [11] -0.250 -0.355 -0.344 -0.382 -0.484 -0.511 -0.183 -0.414 -0.512 -0.550 # [21] -0.303 -0.407 -0.312 -0.445 -0.364 -0.126 -0.143 -0.124 -0.327 -0.341 # [31] 0.065 Définir une règle pour obtenir une valeur unique qui résume l’ensemble des distances de nos observations par rapport aux prédictions du modèle. Une première idée serait de sommer l’ensemble de nos résidus comme ci-dessous : sum(resid) # [1] -10.175 Appliquons ces calculs sur nos quatre modèles afin de les comparer… Le modèle pour lequel notree critère serait le plus proche de zéro serait alors considéré comme le meilleur. model-1 model-2 model-3 model-4 -10.175 -1.441 10.393 0.135 Selon notre méthode, il en ressort que le modèle 4 est le plus approprié pour représenter au mieux nos données. Qu’en pensez vous ? Intuitivement, nous nous aperçevons que le modèle 4 est loin d’être le meilleur. Nous pouvons en déduire que la somme des résidus n’est pas un bon critère pour ajuster un modèle linéaire. Le problème lorsque nous sommons des résidus, est la présence de résidus positifs et de résidus négatifs (ici par rapport à model-4). Ainsi avec notre première méthode naïve de somme des résidus, il suffit d’avoir autant de résidus positifs que négatifs pour avoir un résultat proche de zéro. Mais cela n’implique pas que les observations soient prochent de la droite pour autant. Avez-vous une autre idée que de sommer les résidus ? Sommer le carré des résidus aurait des propriétés intéressantes car d’une part les carrés de nombres positifs et négatifs sont tous positifs, et d’autre part, plus une observation est éloignée plus sa distance au carré pèse fortement dans la somme2. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 3.842211 0.4931095 3.929195 6.498511 Sommer les valeurs absolues des résidus mène également à des contributions toutes positives, mais sans pénaliser outre mesure les observations les plus éloignées. Nous obtenons les résultats suivants : model-1 model-2 model-3 model-4 10.305 3.186 10.393 11.525 Nous avons trouvé deux solutions intéressantes pour quantifier la distance de notre droite par rapport à nos observations. En effet, dans les deux cas, la valeur minimale est obtenue pour le model-2 (en vert sur le graphique) qui est visuellement le meilleur des quatre. La méthode utilisant les carrés des résidus s’appelle une régression par les moindres carrés. Notre objectif est donc de trouver les meilleures valeurs des paramètres \\(a\\) et \\(b\\) de la droite pour minimiser ce critère. Il en résulte une fonction dite objective qui dépend de \\(x\\) et de nos paramètres \\(a\\) et \\(b\\) à minimiser. Cette approche s’appelle la régression par les moindres carrés et elle est la plus utilisée. L’approche utilisant la somme de la valeur absolue des résidus est également utilisable (et elle est d’ailleurs préférable en présence de valeurs extrêmes potentiellement suspectes). Elle s’apppelle régression par la médiane, un cas particulier de la régression quantile, une approche intéressante dans le cas de non normalité des résidus et/ou de présence de valeurs extrêmes suspectes. 1.2.2 Trouver la meilleure droite Essayez de trouver le meilleur modèle par vous-même dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;01a_lin_mod&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/01a_lin_mod&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. Nous pouvons nous demander si notre modèle 2 qui est la meilleure droite de nos quatre modèles est le meilleur modèle possible dans l’absolu. Pour se faire nous allons devoir définir unez technique d’optimisation qui nous permet de déterminer quelle est la droite qui minimise notre fonction objective. Dans la suite, nous garderonsq le critère des moindres carrés (des résidus). Imaginons que nous n’avons pas quatre mais 5000 modèles linéaires avec des pentes et des ordonnées à l’origine différentes. Quelle est la meilleure droite ? set.seed(34643) models1 &lt;- tibble( intercept = runif(5000, -5, 4), slope = runif(5000, -5, 15)) chart(trees, volume ~ diameter) + geom_point() + geom_abline(aes(intercept = intercept, slope = slope), data = models1, alpha = 1/6) Nous voyons sur le graphique qu’un grand nombre de droites différentes sont testées, mais nous ne distinguons pas grand chose de plus. Cependant, sur ces 5000 modèles, nous pouvons maintenant calculer la somme des carrés des résidus et ensuite déterminer quel est le meilleur d’entre eux. # Fonction de calcul de la somme des carrés des résidus measure_distance &lt;- function(slope, intercept, x, y) { ybar &lt;- x * slope + intercept resid &lt;- y - ybar sum(resid^2) } # Test de la fonction #measure_distance(slope = 0, intercept = 0.85, x = trees$diameter, y = trees$volume) # Fonction adaptée pour être employé avec purrr:map() pour distribuer le calcul trees_dist &lt;- function(intercept, slope) { measure_distance(slope = slope, intercept = intercept, x = trees$diameter, y = trees$volume) } models1 &lt;- models1 %&gt;% mutate(dist = purrr::map2_dbl(intercept, slope, trees_dist)) Si nous réalisons un graphique de valeurs de pentes, d’ordonnées à l’origine et de la valeur de la fonction objective (distance) en couleur, nous obtenons le graphique ci-dessous. plot &lt;- chart(models1, slope ~ intercept %col=% dist) + geom_point() + geom_point(data = filter(models1, rank(dist) &lt;= 10), shape = 1, color = &#39;red&#39;, size = 3) + labs( y = &quot;Pente&quot;, x = &quot;Ordonnée à l&#39;origine&quot;, color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) plotly::ggplotly(plot) Les 10 valeurs les plus faibles sont mises en évidence sur le graphique par des cercles rouges. Le modèle optimal que nous recherchons se trouve dans cette région. best_models &lt;- models1 %&gt;.% filter(., rank(dist) &lt;= 10) Nous pouvons afficher les 10 meilleurs modèles sur notre graphique : chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + labs(color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) En résumé, nous avons besoin d’une fonction qui calcule la distance d’un modèle par rapport à nos observations et d’un algorithme pour la minimiser. Il n’est cependant pas nécessaire de chercher pendant des heures la meilleure fonction et le meilleur algorithme. Il existe dans R, un outil spécifiquement conçu pour adapter des modèles linéaires sur des observations, la fonction lm(). Dans le cas particulier de la régression par les moindres carrés, la solution s’obtient très facilement par un simple calcul : \\[a = \\frac{cov_{x, y}}{var_x} \\ \\ \\ \\textrm{et} \\ \\ \\ b = \\bar y - a \\ \\bar x\\] où \\(\\bar x\\) et \\(\\bar y\\) sont les moyennes pour les deux variables, \\(cov\\) est la covariance et \\(var\\) est la variance. Vous avez à votre disposition des snippets dédiés aux modèles linéaires (tapez ..., ensuite choisissez models, ensuite models : linear et choisissez le snippet qui vous convient dans la liste. (lm. &lt;- lm(data = trees, volume ~ diameter)) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Coefficients: # (Intercept) diameter # -1.047 5.652 Nous pouvons reporter ces valeurs sur notre graphique afin d’observer ce résultat par rapport à nos modèles aléatoires. nous avons en rouge la droite calculée par la fonction lm(). chart(trees, volume ~ diameter) + geom_abline(data = best_models, aes(slope = slope, intercept = intercept, color = dist), alpha = 3/4) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Distance&quot;) + scale_color_viridis_c(direction = -1) 1.2.3 La fonction lm() Suite à notre découverte de manière intuitive de la régression linéaire simple, nous pouvons récapituler quelques points clés : Une droite suit l’équation mathématique suivante : \\[y = a \\ x + b\\] dont \\(a\\) est la pente (slope en anglais) et \\(b\\) est l’ordonnée à l’origine (intercept en anglais), tous deux les paramètres du modèle, alors que \\(x\\) et \\(y\\) en sont les variables. La distance entre une valeur observée \\(y_i\\) et une valeur prédite \\(\\hat y_i\\) se nomme le résidu (\\(\\epsilon_i\\)) et se mesure toujours parallèlement à l’axe \\(y\\). Cela revient à considérer que toute l’erreur du modèle se situe sur \\(y\\) et non sur \\(x\\). Cela donne l’équation complète de notre modèle statistique : \\[y_i = a \\ x_i + b + \\epsilon_i\\] avec \\(y_i\\) est la valeur mesurée pour le point i sur l’axe y, \\(a\\) est la pente, \\(x_i\\) est la valeur mesurée pour le point i sur l’axe x, b est l’ordonnée à l’origine et \\(\\epsilon_i\\) les résidus. On peut montrer (nous ne le ferons pas ici pour limiter les développements mathématiques) que le choix des moindres carrés des résidus comme fonction objective revient à considérer que nos résidus suivent une distribution normale centrée autour de zéro et avec un écart type \\(\\sigma\\) constant/ : \\[\\epsilon_i \\approx N(0, \\sigma)\\] Dans le cas de la régression linéaire simple, la meilleure droite s’obtient très facilement par la minimisation de la somme des carrés des résidus. En effet, la pente \\(a = \\frac{cov_{x, y}}{var_x}\\) et l’ordonnée à l’origine \\(b = \\bar y - a \\ \\bar x\\). La fonction lm() permet de faire ce calcul très facilement dans R. # Régression linéaire lm. &lt;- lm(data = trees, volume ~ diameter) # Graphique de nos observations et de la droite obtenue avec la fonction lm() chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) La fonction lm() crée un objet spécifique qui contient de nombreuses informations pour pouvoir ensuite analyser notre modèle linéaire. La fonction class() permet de mettre en avant la classe de notre objet. class(lm.) # [1] &quot;lm&quot; 1.2.4 Résumé avec summary() Avec la fonction summary() nous obtenons un résumé condensé des informations les plus utiles pour interpréter notre régression linéaire. summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Call : Il s’agit de la formule employée dans la fonction lm(). C’est à dire le volume en fonction du diamètre du jeu de données trees. Residuals : La tableau nous fournit un résumé via les 5 nombres de l’ensemble des résidus (que vous pouvez récupérer à partir de lm.$residuals). fivenum(lm.$residuals) # 20 7 12 9 31 # -0.231210947 -0.087020695 0.003532709 0.100594122 0.271724973 Coefficients : Il s’agit des résultats associés à la pente et à l’ordonnée à l’origine dont les valeurs estimées des paramètres (Estimate). Les mêmes valeurs peuvent être obtenues à partir de lm.$coefficients : lm.$coefficients # (Intercept) diameter # -1.047478 5.651535 On retrouve également les écart-types calculés sur ces valeurs (Std.Error) qui donnent une indication de la précision de leur estimation, les valeurs des distibutions de Student sur les valeurs estimées (t value) et enfin les valeurs p (PR(&gt;|t|)) liées à un test de Student pour déterminer si le paramètre p correspondant est significativement différent de zéro (avec \\(H_0: p = 0\\) et \\(H_a: p \\neq 0\\)). Pour l’instant, nous nous contenterons d’interpréter et d’utiliser les informations issues de summary() dans sa partie supérieure. Le contenu des trois dernières lignes sera détaillé dans le module suivant. A partir des données de ce résumé, nous pouvons maintenant paramétrer l’équation de notre modèle : \\[y = ax + b\\] devient3 : \\[volume \\ de \\ bois = 5.65 \\ diamètre \\ à \\ 1.4 \\ m - 1.05 \\] Utiliser le carré des résidus a aussi d’autres propriétés statistiques intéressantes qui rapprochent ce calcul de la variance (qui vaut la somme de la distance au carré à la moyenne pour une seule variables numérique).↩ Lors de la paramétrisation du modèle, pensez à arrondir la valeur des paramètres à un nombre de chiffres significatifs raisonnables. Inutile de garder 5, ou même 3 chiffres derrière la virgule si vous n’avez que quelques dizaines d’obserrvations pour ajuster votre modèle.↩ "],
["outils-de-diagnostic.html", "1.3 Outils de diagnostic", " 1.3 Outils de diagnostic Une fois la meilleure droite de régression obtenue, le travail est loin d’être terminé. Il se peut que le nuage de point ne soit pas tout-à-fait linéaire, que sa dispersion ne soit pas homogène, que les résidus n’aient pas une distribution normale, qu’il existe des valeurs extrêmes aberrantes, ou qui tirent la droite vers elle de manière excessive. Nous allons maintenant devoir diagnostiquer ces possibles problèmes. L’analyse des résidus permet de le faire. Ensuite, si deux ou plusieurs modèles sont utilisable, il nous faut décider lequel conserver. Enfin, nous pouvons aussi calculer et visualiser l’enveloppe de confiance du modèle et extraire une série de données de ce modèle. 1.3.1 Analyse des résidus Le tableau numérique obtenu à l’aide de summary() peut faire penser que l’étude d’une régression linéaire se limite à quelques valeurs numériques et divers tests d’hypothèses associés. C’est un premier pas, mais c’est oublier que la technique est essentiellement visuelle. Le graphique du nuage de points avec la droite superposée est un premier outil diagnostic visuel indispensable, mais il n’est pas le seul ! Plusieurs graphiques spécifiques existent pour mettre en évidence diverses propriétés des résidus qui peuvent révéler des problèmes. Leur inspection est indispensable et s’appelle l’analyse des résidus. Les différents graphiques sont faciles à obtenir à partir des snippets. Le premier de ces graphique permet de vérifier la distribution homogène des résidus. Dans une bonne régression, nous aurons une distribution équilibrée de part et d’autre du zéro sur l’axe Y. Que pensez-vous de notre graphique d’anayse des résidus ? Nous avons une valeur plus éloignée du zéro qui est mise en avant par la courbe en bleu qui montre l’influence générale des résidus. #plot(lm., which = 1) lm. %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) Le second graphique permet de vérifier la normalité des résidus (comparaison par graphique quantile-quantile à une distribution normale). #plot(lm., which = 2) lm. %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) Le troisième graphique va standardiser les résidus, et surtout, en prendre la racine carrée. Cela a pour effet de superposer les résidus négatifs sur les résidus positifs. Nous y diagnostiquons beaucoup plus facilement des problèmes de distribution de ces résidus. A nouveau, nous pouvons observer qu’une valeur influence fortement la régression. #plot(lm., which = 3) lm. %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) Le quatrième graphique met en évidence l’influence des individus sur la régression linéaire. Effectivement, la régression linéaire est sensible aux valeurs extrêmes. Nous pouvons observer que nous avons une valeur qui influence fortement notre régression. On utilise pour ce faire la distance de Cook que nous ne détaillerons pas dans le cadre de ce cours. #plot(lm., which = 4) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Le cinquième graphique utilise l’effet de levier (Leverage) qui met également en avant l’influence des individus sur notre régression. Il répond à la question suivante : “est-ce qu’un ou plusieurs points sont tellement influents qu’ils tirent la régression vers eux de manière abusive ?” Nous avons à nouveau une valeur qui influence fortement notre modèle. #plot(lm., which = 5) lm. %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) Le sixième graphique met en relation la distance de Cooks et l’effet de levier. Notre unique point d’une valeur supérieur à 0.5 m de diamètre influence fortement notre modèle. #plot(lm., which = 6) lm. %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) A l’issue de l’analyse des résidus, nous abservons donc différents problèmes qui suggèrent que le modèle choisi n’est peut être pas le plus adapté. Nous comprendrons pourquoi plus loin. A vous de jouer Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01b_reg_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Pièges et astuces : extrapolation Notre régression linéaire a été réalisée sur des cerisiers noirs dont le diamètre est compris entre 0.211 et 0.523 mètre. Pensez vous qu’il soit acceptable de prédire des volumes de bois pour des arbres dont le diamètre est inférieur ou supérieur à nos valeurs minimales et maximales mesurées (extrapolation) ? Utilisons notre régression linéaire afin de prédire 10 volumes de bois à partir d’arbre dont le diamètre varie entre 0.1 et 0.8m. new &lt;- data.frame(diameter = seq(0.1, 0.7, length.out = 8)) Ajoutons une variable pred qui contient les prédictions en volume de bois. Observez-vous un problème particulier sur base du tableau ci-dessous ? new %&gt;.% modelr::add_predictions(., lm.) -&gt; new new # diameter pred # 1 0.1000000 -0.482324424 # 2 0.1857143 0.002092891 # 3 0.2714286 0.486510206 # 4 0.3571429 0.970927522 # 5 0.4428571 1.455344837 # 6 0.5285714 1.939762152 # 7 0.6142857 2.424179468 # 8 0.7000000 2.908596783 Il est peut-être plus simple de voir le problème sur un nuage de points. Pour un diamètre de 0.1857143 m de diamètre, le volume de bois est de 0 mis en avant par l’intersection des lignes pointillées bleues. chart(trees, volume~diameter) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) + geom_vline(xintercept = new$diameter[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_hline(yintercept = new$pred[2], linetype = &quot;twodash&quot;, color = &quot;blue&quot;) + geom_point() + geom_abline(aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2])) + geom_point(data = new, f_aes(pred~diameter), color = &quot;red&quot;) Le volume de bois prédit est négatif ! Notre modèle est-il alors complètement faux ? Rappelons-nous qu’un modèle est nécessairement une vision simplifiée de la réalité. En particulier, notre modèle a été entraîné avec des données comprises dans un intervalle. Il est alors valable pour effectuer des interpolations à l’intérieur de cet intervalle, mais ne peut pas être utilisé pour effectuer des extrapolations en dehors, comme nous venons de le faire. trees %&gt;.% modelr::add_predictions(., lm.) -&gt; trees chart(trees, volume~diameter) + geom_point() + geom_line(f_aes(pred ~ diameter)) Pièges et astuces : significativité fortuite Gardez toujours à l’esprit qu’il est possible que votre jeu de données donne une régression significative, mais purement fortuite. Les données supplémentaires de test devraient alors démasquer le problème. D’où l’importance de vérifier/valider votre modèle. Le principe de parcimonie veut que l’on ne teste pas toutes les combinaisons possibles deux à deux des variables d’un gros jeu de données, mais que l’on restreigne les explorations à des relations qui ont un sens biologique afin de minimiser le risque d’obtenir une telle régression de manière fortuite. 1.3.2 Enveloppe de confiance De même que l’on peut définir un intervalle de confiance dans lequel la moyenne d’un échantillon se situe avec une probabilité donnée, il est aussi possible de calculer et de tracer une enveloppe de confiance qui indique la région dans laquelle le “vrai” modèle se trouve avec une probabilité donnée (généralement, on choisi cette probabilité à 95%). Voici ce que cela donne : lm. %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x))(.) Cette enveloppe de confiance est en réalité basée sur l’écart type conditionnel (écart type de \\(y\\) sachant quelle est la valeur de \\(x\\)) qui se calcule comme suit : \\[s_{y|x}\\ =\\ \\sqrt{ \\frac{\\sum_{i = 0}^n\\left(y_i - \\hat y_i\\right)^2}{n-2}}\\] A partir de là, il est possible de définir également un intervalle de confiance conditionnel à \\(x\\) : \\[CI_{1-\\alpha}\\ =\\ \\hat y_i\\ \\ \\pm \\ t_{\\frac{\\alpha}{2}}^{n-2} \\frac{s_{y|x}\\ }{\\sqrt{n}}\\] C’est cet intervalle de confiance conditionnel qui est matérialisé par l’enveloppe de confiance autour de la droite de régression représentée sur le graphique. 1.3.3 Extraire les données d’un modèle La fonction tidy() du package broom extrait facilement et rapidement sous la forme d’un tableau différentes valeurs associées à votre régression linéaire. (DF &lt;- broom::tidy(lm.)) # # A tibble: 2 x 5 # term estimate std.error statistic p.value # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 (Intercept) -1.05 0.0955 -11.0 7.85e-12 # 2 diameter 5.65 0.276 20.4 9.09e-19 Pour extraire facilement et rapidement sous la forme d’un tableau de données les paramètres de votre modèle vouys pouvez aussi utiliser la fonction glance(). (DF &lt;- broom::glance(lm.)) # # A tibble: 1 x 11 # r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.935 0.933 0.121 418. 9.09e-19 2 22.6 -39.2 -34.9 # # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Vous avez des snippets à votre disposition pour ces deux fonctions : ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtmoddf ou encore ... -&gt; models ..m -&gt; .models tools .mt -&gt; .mtpardf A vous de jouer Vous avez à votre disposition la première assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/bvqsukEO Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt eucalyptus. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. "],
["lm2.html", "Module 2 Régression linéaire II", " Module 2 Régression linéaire II Objectifs Savoir utiliser les outils de diagnostic de la régression linéaire correctement, en particulier l’analyse des résidus. Appréhender les différentes formes de régressions linéaires par les moindres carrés. Choisir sa régression linéaire de manière judicieuse. Prérequis Le module précédent est une entrée en matière indispensable qui est complétée par le contenu du présent module. "],
["outils-de-diagnostic-suite.html", "2.1 Outils de diagnostic (suite)", " 2.1 Outils de diagnostic (suite) La régression linéaire est une matière complexe et de nombreux outils existent pour vous aider à déterminer si le modèle que vous ajustez tient la route ou non. Il est très important de le vérifier avant d’utiliser un modèle. Ajuster un modèle quelconque dans des données est à la portée de tout le monde, mais choisir un modèle pertinent et pouvoir expliquer pourquoi est nettement plus difficile ! 2.1.1 Résumé avec summary()(suite) Reprenons la sortie renvoyée par summary() appliqué à un objet lm. trees &lt;- read(&quot;trees&quot;, package = &quot;datasets&quot;, lang = &quot;fr&quot;) lm. &lt;- lm(data = trees, volume ~ diameter) summary(lm.) # # Call: # lm(formula = volume ~ diameter, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.231211 -0.087021 0.003533 0.100594 0.271725 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.04748 0.09553 -10.96 7.85e-12 *** # diameter 5.65154 0.27649 20.44 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1206 on 29 degrees of freedom # Multiple R-squared: 0.9351, Adjusted R-squared: 0.9329 # F-statistic: 417.8 on 1 and 29 DF, p-value: &lt; 2.2e-16 Nous n’avons pas encore étudié la signification des trois dernières lignes de ce résumé. Voici de quoi il s’agit. Residual standard error : Il s’agit de l’écart-type résiduel, considérant que les degrés de liberté du modèle est le nombre d’observations \\(n\\) (ici 31) soustrait du nombre de paramètres à estimer (ici 2, la pente et l’ordonnée à l’origine de la droite). C’est donc une mesure globale de l’importance (c’est-à-dire de l’étendue) des résidus de manière générale. \\[\\sqrt{\\frac{\\sum(y_i - ŷ_i)^2}{n-2}}\\] Multiple R-squared : Il s’agit de la valeur du coefficient de détermination du modèle noté R^2 de manière générale ou r2 dans le cas d’une régression linéaire simple. Il exprime la fraction de variance exprimée par le modèle. Autrement dit, le R2 quantifie la capacité du modèle à prédire la valeur de \\(y\\) connaissant la valeur \\(x\\) pour le même individu. C’est dons une indication du pouvoir prédictif de notre modèle autant que de sa qualité d’ajustement (goodness-of-fit en anglais). Souvenons-nous que la variance totale respecte la propiété d’additivité. La variance est composée au numérateur d’une somme de carrés, et au dénominateur de degrés de liberté. La somme des carrés totaux (de la variance) peut elle-même être décomposée en une fraction expliquée par notre modèle, et la fraction qui ne l’est pas (les résidus) : \\[SC(total) = SC(rég) + SC(résidus)\\] avec : \\[SC(total) = \\sum_{i=0}^n(y_i - \\bar y_i)^2\\] \\[SC(rég) = \\sum_{i=0}^n(ŷ_i - \\bar y_i)^2\\] \\[SC(résidus) = \\sum_{i=0}^n(y_i - ŷ_i)^2\\] A partir de la décomposition de ces sommes de carrés, le coefficient R2 (ou r2) se définit comme : \\[R^2 = \\frac{SC(rég)}{SC(total)} = 1 - \\frac{SC(résidus)}{SC(total)}\\] La valeur du R2 est comprise entre 0 (lorsque le modèle est très mauvais et n’explique rien) et 1 (lorsque le modèle est parfait et “capture” toute la variance des données ; dans ce cas, tous les résidus valent zéro). Donc, plus le coefficient R2 se rapproche de un, plus le modèle explique bien les données et aura un bon pouvoir de prédiction. Dans R, le R2 multiple se réfère simplement au R2 (ou au r2 pour les régressions linéaires simples) calculé de cette façon. L’adjectif multiple indique simplement que le calcul est valable pour une régression multiple telle que nous verrons plus loin. Par contre, le terme au dénominateur considère en fait la somme des carrés totale par rapport à un modèle de référence lorsque la variable dépendante \\(y\\) ne dépend pas de la ou des variables indépendantes \\(x_i\\). Les équations indiquées plus haut sont valables lorsque l’ordonnée à l’origine n’est pas figée (\\(y = a \\ x + b\\)). Dans ce cas, la valeur de référence pour \\(y\\) est bien sa moyenne, \\(\\bar y\\). D’un autre côté, si l’ordonnée à l’origine est fixée à zéro dans le modèle simplifié \\(y = a \\ x\\) (avec \\(b = 0\\) obtenu en indiquant la formule y ~ x + 0 ou y ~ x - 1), alors le zéro sur l’axe \\(y\\) est considéré comme une valeur appartenant d’office au modèle et devient valeur de référence. Ainsi, dans les équations ci-dessus il faut remplacer \\(\\bar y\\) par 0 partout. Le R2 est alors calculé différemment, et sa valeur peut brusquement augmenter si le nuage de points est très éloigné du zéro sur l’axe y. Ne comparez donc jamais les R2 obtenus avec et sans forçage à zéro de l’ordonnée à l’origine ! Adjusted R-squared : La valeur du coefficient R2 ajustée, noté \\(\\bar{R^2}\\) n’est pas utile dans le cadre de la régression linéaire simple, mais est indispensable avec la régression multiple. En effet, à chaque fois que vous rendez votre modèle plus complexe en ajoutant une ou plusieurs variables indépendantes, le modèle s’ajustera de mieux en mieux dans les données, même par pur hasard. C’est un phénomène que l’on appelle l’inflation du R2. A la limite, si nous ajoutons une nouvelle variable fortement corrélée avec les précédentes4, l’apport en terme d’information nouvelle sera négligeable, mais le R2 augmentera malgré tout un tout petit peu. Alors dans quel cas l’ajout d’une nouvelle variable est-il pertinent ou non ? Le R2 ajusté apporte l’information désirée ici. Sa valeur n’augmentera pour l’ajout d’un nouveau prédicteur que si l’ajustement est meilleur que ce que l’on obtiendrait par le pur hasard. Le R2 ajusté se calcule comme suit (il n’est pas nécessaire de retenir cette formule, mais juste de constater que l’ajustement fait intervenir p, le nombre de paramètres du modèle et n, la taille de l’échantillon) : \\[ \\bar{R^2} = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} \\] F-statistic : Tout comme pour l’ANOVA, le test de la significativité de la régression car \\(MS(rég)/MS(résidus)\\) suit une distribution F à respectivement 1 et \\(n-2\\) degré de liberté, avec \\(MS\\) les carrés moyens, c’est-à-dire les sommes des carrés \\(SC\\) divisés par leurs degrés de liberté respectifs. p-value : Il s’agit de la valeur p associé à la statistique de F, donc à l’ANOVA associée à la régression linéaire. Pour cette ANOVA particulière, l’hypothèse nulle est que la droite n’apporte pas plus d’explication des valeurs de y à partir des valeurs de x que la valeur moyenne de y (ou zéro, dans le cas paerticulier d’un modèle dont l’ordonnée à l’origine est forcé à zéro). L’hypothèse alternative est donc que le modèle est significatif au seuil \\(\\alpha\\) considéré. Donc, notre objectif est de rejetter H0 pour cet test ANOVA pour que le modèle ait un sens (valeur p plus petite quez le seuil \\(\\alpha\\) choisi). Le tableau complet de l’ANOVA associée au modèle peut aussi être obtenu à l’aide de la fonction anova() : anova(lm.) # Analysis of Variance Table # # Response: volume # Df Sum Sq Mean Sq F value Pr(&gt;F) # diameter 1 6.0762 6.0762 417.8 &lt; 2.2e-16 *** # Residuals 29 0.4218 0.0145 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 On y retrouve les mêmes informations, fortement résumées en une ligne à la fin de la sortie de summary(), mais ici sous une forme plus classique de tableau de l’analyse de la variance. 2.1.2 Comparaison de régressions Vous pouvez à présent comparer ces résultats avec un tableau et les six graphiques d’analyse des résidus sans la valeur supérieure à 0.5m de diamètre. Attention, On ne peut supprimer une valeur sans raison valable. La suppression de points aberrants doit en principe être faite avant de débuter l’analyse. La raison de la suppression de ce point est liée au fait qu’il soit seul et unique point supérieur à 0.5m de diamètre. Nous le faisons ici à titre de comparaison. trees_red &lt;- filter(trees, diameter &lt; 0.5) lm1 &lt;- lm(data = trees_red, volume ~ diameter) chart(trees, volume ~ diameter) + geom_point() + geom_abline( aes(intercept = lm.$coefficients[1], slope = lm.$coefficients[2]), color = &quot;red&quot;, size = 1.5) + labs( color = &quot;Modèle&quot;) + scale_color_viridis_c(direction = -1) + geom_abline( aes(intercept = lm1$coefficients[1], slope = lm1$coefficients[2]), color = &quot;blue&quot;, size = 1.5) La droite en bleu correspond à la régression sans utiliser l’arbre de diamètre supérieur à 0,5m. Tentez d’analyser le tableau de notre régression en bleu (astuce : comparez avec ce que la régeression précédente donnait). summary(lm1) # # Call: # lm(formula = volume ~ diameter, data = trees_red) # # Residuals: # Min 1Q Median 3Q Max # -0.215129 -0.068502 -0.001149 0.070522 0.181398 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.94445 0.09309 -10.15 6.98e-11 *** # diameter 5.31219 0.27540 19.29 &lt; 2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1082 on 28 degrees of freedom # Multiple R-squared: 0.93, Adjusted R-squared: 0.9275 # F-statistic: 372.1 on 1 and 28 DF, p-value: &lt; 2.2e-16 Tentez d’analyser également les graphiques d’analyse des résidus ci-dessous. #plot(lm1, which = 1) lm1 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm1, which = 2) lm1 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm1, which = 3) lm1 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm1, which = 4) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) #plot(lm1, which = 5) lm1 %&gt;.% chart(broom::augment(.), .std.resid ~ .hat %size=% .cooksd) + geom_point() + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + labs(x = &quot;Leverage&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Residuals vs Leverage&quot;) #plot(lm1, which = 6) lm1 %&gt;.% chart(broom::augment(.), .cooksd ~ .hat %size=% .cooksd) + geom_point() + geom_vline(xintercept = 0, colour = NA) + geom_abline(slope = seq(0, 3, by = 0.5), colour = &quot;darkgray&quot;) + geom_smooth(se = FALSE, size = 0.5, method = &quot;loess&quot;, formula = y ~ x) + labs(x = expression(&quot;Leverage h&quot;[ii]), y = &quot;Cook&#39;s distance&quot;) + ggtitle(expression(&quot;Cook&#39;s dist vs Leverage h&quot;[ii] / (1 - h[ii]))) Au travers de cet exemple, nous constatons que la comparaison de modèles, dans le but de choisir le meilleur est un travail utile. Cela apparaitra d’autant plus utile que la situation va passablement se complexifier (dans le bon sens) avec l’introduction de la régression multiple et polynomiale ci-dessous. Heureusement, nous terminerons ce module avec la découverte d’une métrique qui va nous permettre d’effectuer le choix du meilleur modèle de manière fiable : le critère d’Akaike. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Vous avez à votre disposition la première assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/jkh3ruyX Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt anscombe. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. La corrélation entre les prédicteurs dans un modèle linéaire multiple est un gros problème et doit être évité le plus possible. Cela s’appelle la colinéarité ou encore multicollinéairité. Ainsi, il est toujours préférable de choisir un ensemble de variables indépendantes peu corrélées entre elles dans un même modèle, mais ce n’est pas toujours possible.↩ "],
["regression-lineaire-multiple.html", "2.2 Régression linéaire multiple", " 2.2 Régression linéaire multiple Dans le cas de la régression linéaire simple, nous considèrions le modèle stqatistique suivant (avec \\(\\epsilon\\) représentant les résidus, terme statistique dans l’équation) : \\[y = a \\ x + b + \\epsilon \\] Dans le cas de la régression, nous introduirons plusieurs variables indépendantes notés \\(x_1\\), \\(x_2\\), …, \\(x_n\\) : \\[y = a_1 \\ x_1 + a_2 \\ x_2 + ... + a_n \\ x_n + b + \\epsilon \\] La bonne nouvelle, c’est que tous les calculs, les métriques et les tests d’hypothèses relatifs à la régression linéaire simple se généraliser simplement et naturellement, tout comme nous sommes passés dans le cours SDD 1 de l’ANOVA à 1 facteur à un modèle plus complexe à 2 ou plusoieurs facteurs. Voyons tout de suite ce que cela donne si nous voulions utiliser à la fois le diamètree et la hauteur des cerisiers noirs pour prédire leur volume de bois : summary(lm2 &lt;- lm(data = trees, volume ~ diameter + height)) # # Call: # lm(formula = volume ~ diameter + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.180423 -0.074919 -0.006874 0.062244 0.241801 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -1.63563 0.24462 -6.686 2.95e-07 *** # diameter 5.25643 0.29594 17.762 &lt; 2e-16 *** # height 0.03112 0.01209 2.574 0.0156 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1104 on 28 degrees of freedom # Multiple R-squared: 0.9475, Adjusted R-squared: 0.9438 # F-statistic: 252.7 on 2 and 28 DF, p-value: &lt; 2.2e-16 D’un point de vue pratique, nous voyons que la formule qui spécifie le modèle peut très bien comporter plusieurs variables séparées par des +. Nous avons ici trois paramètres dans notre modèle : l’ordonnée à l’origine qui vaut -1,63, la pente relative au diamètre de 5,25, et la pente relative à la hauteur de 0,031. Le modèle lm2 sera donc paramétré comme suit : volume de bois = 5,25 . diamètre + 0,031 . hauteur - 1,63. Notons que la pente relative à la hauteur (0,031) n’est pas significativement différente de zéro au seuil \\(\\alpha\\) de 5% (mais l’est seulement pour \\(\\alpha\\) = 1%). En effet, la valeur t du test de Student associé (H0 : le paramètre vaut zéro, H1 : le paramètre est différent de zéro) vaut 2,574. Cela correspond à une valeur p du test de 0,0156, une valeur moyennement significative donc, matérialisée par une seule astérisque à la droite du tableau. Cela dénote un plus faible pouvoir de prédiction du volume de bois via la hauteur que via le diamètre de l’arbre. Nous l’avions déjà observé sur le graphique matrice de nuages de points réalisé initialement, ainsi que via les coefficients de correlation respectifs. La représentation de cette régression nécessite un graphique à trois dimensions (diamètre, hauteur et volume) et le modèle représente en fait le meilleur plan dans cet espace à 3 dimensions. Pour un modèle comportant plus de deux variables indépendantes, il n’est plus possible de représenter graphiquement la régression. library(rgl) knitr::knit_hooks$set(webgl = hook_webgl) car::scatter3d(data = trees, volume ~ diameter + height, fit = &quot;linear&quot;, residuals = TRUE, bg = &quot;white&quot;, axis.scales = TRUE, grid = TRUE, ellipsoid = FALSE) # Loading required namespace: mgcv rgl::rglwidget(width = 800, height = 800) Utilisez la souris pour zoomer (molette) et pour retourner le graphique (cliquez et déplacer la souris en maintenant le bouton enfoncé) pour comprendre ce graphique 3D. La régression est matérialisée par un plan en bleu. Les observations sont les boules jaunes et les résidus sont des traits cyans lorsqu’ils sont positifs et magenta lorsqu’ils sont négatifs. Les graphes d’analyse des résidus sont toujours disponibles (nous ne représentons ici que les quatre premiers) : #plot(lm2, which = 1) lm2 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm2, which = 2) lm2 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm2, which = 3) lm2 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm2, which = 4) lm2 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Est-ce que ce modèle est préférable à celui n’utilisant que le diamètre ? Le R2 ajusté est passé de 0,933 avec le modèle simple lm. utilisant uniquement le diamètre à 0,944 dans le présent modèle lm2 utilisant le diamètre et la hauteur. Cela semble une amélioration, mais le test de significativité de la pente pour la hauteur ne nous indique pas un résultat très significatif. De plus, cela a un coût en pratique de devoir mesurer deux variables au lieu d’une seule pour estimer le volume de bois. Cela en vaut-il la peine ? Nous sommes encore une fois confrontés à la question de comparer deux modèles, cette fois-ci ayant une complexité croissante. Dans le cas particulier de modèles imbriqués (un modèle contient l’autre, mais rajoute un ou plusieurs termes), une ANOVA est possible en décomposant la variance selon les composantes reprises respectivement par chacun des deux modèles. La fonction anova() est programmée pour faire ce calcul en lui indiquant chacun des deux objets contenant les modèles à comparer : anova(lm., lm2) # Analysis of Variance Table # # Model 1: volume ~ diameter # Model 2: volume ~ diameter + height # Res.Df RSS Df Sum of Sq F Pr(&gt;F) # 1 29 0.42176 # 2 28 0.34104 1 0.080721 6.6274 0.01562 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notez que dans le cas de l’ajout d’un seul terme, la valeur p de cette ANOVA est identique à la valeur p de test de significativité du paramètre (ici, cette valeur p est de 0,0156 dans les deux cas). Donc, le choix peut se faire directement à partir de summary() pour ce terme unique. La conclusion est similaire : l’ANOVA donne un résultat seulement moyennent significatif entre les 2 modèles. Dans un cas plus complexe, la fonction anova() de comparaison pourra être utile. Enfin, tous les modèles ne sont pas nécessairement imbriqués. Dans ce cas, il nous faudra un autre moyen de les départager, … mais avant d’aborder cela, étudions une variante intéressante de la régression multiple : la régression polynomiale. A vous de jouer ! Réalisez une nouvelle assignation individuelle : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02a_reg_multi&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["regression-lineaire-polynomiale.html", "2.3 Régression linéaire polynomiale", " 2.3 Régression linéaire polynomiale Pour rappel, un polynome est une expression mathématique du type (notez la ressemblance avec l’équation de la régression multiple) : \\[ a_0 + a_1 . x + a_2 . x^2 + ... + a_n . x^n \\] Un polynome d’ordre 2 (terme jusqu’au \\(x^2\\)) correspond à une parabole dans le plan xy. Que se passe-t-il si nous calculons une variable diametre2 qui est le carré du diamètre et que nous prétendons faire une régression multiple en utilisant à la fois diamètre et diamètre2/ ? trees %&gt;.% mutate(., diameter2 = diameter^2) -&gt; trees summary(lm(data = trees, volume ~ diameter + diameter2)) # # Call: # lm(formula = volume ~ diameter + diameter2, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # diameter2 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 Il semble que R ait pu réaliser cette analyse. Cette fois-ci, nous n’avons cependant pas une droite ou un plan ajusté, mais par ce subterfuge, nous avons pu ajuster une courbe dans les données ! Nous pourrions augmenter le degré du polynome (ajouter un terme en diameter^3, voire encore des puissance supérieures). Dans ce cas, nous obtiendrons une courbe de plus en plus flexible, toujours dans le plan xy. Ceci illustre parfaitement d’ailleurs l’ambiguité de la complexité du modèle qui s’ajuste de mieux en mieux dans les données, mais qui ce faisant, perd également progressivement son pouvoir explicatif. En effet, on sait qu’il existe toujours une droite qui passe entre deux points dans le plan. De même, il existe toujours une parabole qui passe par 3 points quelconques dans le plan. Et par extension, il existe une courbe correspondant à un polynome d’ordre n - 1 qui passe par n’importe quel ensemble de n points dans le plan. Un modèle construit à l’aide d’un tel polynome aura toujours un R2 égal à un, … mais en même temps ce modèle ne sera d’aucune utilité car il ne contient plus aucune information pertinente. C’est ce qu’on appelle le surajustement (overfitting en anglais). La figure ci-dessous (issue d’un article écrit par Anup Bhande ici) illuste bien ce phénomène. Devoir calculer les différentes puissance des variables au préalable devient rapidement fastidieux. Heureusement, R autorise de glisser ce cacul directement dans la formule, mais à condition de lui indiquer qu’il ne s’agit pas du nom d’une variable diameter^2, mais d’un calcul effectué sur diameter en utilisant la fonction, d’identité I(). Ainsi, sans rien calculer au préalable, nous pouvons utiliser la formule volume ~ diameter + I(diameter^2). Un snippet est d’ailleurs disponible pour ajuster un polynome d’ordre 2 ou d’ordre 3, et il est accompagné du code nécessaire pour représenter également graphiquement cette régression polynomiale. Le code ci-dessous qui construit le modèle lm3 l’utilise. summary(lm3 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2))) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 lm3 %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2)))(.) Remarquez sur le graphique comment, à présent, la courbe s’ajuste bien mieux dans le nuage de point et comme l’arbre le plus grand avec un diamètre supérieur à 0,5m est à présent presque parfaitement ajusté par le modèle. Faites donc très attention que des points influents ou extrêmes peuvent apparaitre également comme tel à cause d’un mauvais choix de modèle ! L’analyse des résidus nous montre aussi un comportement plus sain. #plot(lm3, which = 1) lm3 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm3, which = 2) lm3 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm3, which = 3) lm3 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm3, which = 4) lm3 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Revenons un instant sur le résumé de ce modèle. summary(lm3) # # Call: # lm(formula = volume ~ diameter + I(diameter^2), data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.157938 -0.068324 -0.009027 0.060611 0.214988 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.3111 0.3180 0.978 0.336293 # diameter -2.3718 1.8381 -1.290 0.207489 # I(diameter^2) 11.2363 2.5563 4.396 0.000144 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.09441 on 28 degrees of freedom # Multiple R-squared: 0.9616, Adjusted R-squared: 0.9589 # F-statistic: 350.5 on 2 and 28 DF, p-value: &lt; 2.2e-16 La pente relative au diameter nécessite quelques éléments d’explication. En effet, que signifie une pente pour une courbe dont la dérivée première (“pente locale”) change constamment ? En fait, il faut comprendre ce paramètre comme étant la pente de la courbe au point x = 0. Si le modèle est très nettement significatif (ANOVA, valeur p &lt;&lt;&lt; 0,001), et si le R2 ajusté grimpe maintenant à 0,959, seul le paramètre relatif au diamètre2 est significatif cette fois-ci. Ce résultat suggère que ce modèle pourrait êtrte simplifié en considérant que l’ordonnée à l’origine et la pente pour le terme diameter valent zéro. Cela peut être tenté, mais à condition de refaire l’analyse. On ne peut jamais laisser tomber un paramètre dans une analyse et considérer que les autres sont utilisable tels quels. tous les paramètres caculés sont interconnectés. Voyons ce que cela donne (la formule devient volume ~ I(diameter^2) - 1 ou volume ~ I(diameter^2) + 0, ce qui est identique) : summary(lm4 &lt;- lm(data = trees, volume ~ I(diameter^2) - 1)) # # Call: # lm(formula = volume ~ I(diameter^2) - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # I(diameter^2) 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez bien que quand on réajuste un modèle simplifié, les paramètres restants doivent être recalculés. En effet, le paramètre relatif au diamètre2 vallait 11,2 dans le modèle lm3 plus haut. Un fois les autres termes éliminés, ce paramètre devient 7,30 dans ce modèle lm4 simplifié. Le modèle lm4 revient aussi (autre point de vue) à transformer d’abord le diamètre en diamètre2 et à effectuer ensuite une régression linéaire simple entre deux variables, volume et diametre2 : summary(lm4bis &lt;- lm(data = trees, volume ~ diameter2 - 1)) # # Call: # lm(formula = volume ~ diameter2 - 1, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.19474 -0.07234 -0.04120 0.04522 0.18240 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # diameter2 7.3031 0.1397 52.26 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1027 on 30 degrees of freedom # Multiple R-squared: 0.9891, Adjusted R-squared: 0.9888 # F-statistic: 2731 on 1 and 30 DF, p-value: &lt; 2.2e-16 Notez qu’on obtient bien évidemment exactement les mêmes résultats si nous transformons d’abord les données ou si nous intégrons le calcul à l’intérieur de la formule qui décrit le modèle. Faites bien attention de ne pas comparer le R2 acvec ordonnée à l’origine fixée à zéro ici dans notre modèle lm4 avec les R2 des modèles lm. ou lm3 qui ont ce paramètre estimé. Rappelez-vous que le R2 est calculé différemment dans les deux cas ! Donc, nous voilà une fois de plus face à un nouveau modèle pour lequel il nous est difficile de décider s’il est meilleur que les précédents. Avant de comparer, élaborons un tout dernier modèle, le plus complexe, qui reprend à la fois notre régression polynomiale d’ordre 2 sur le diamètre et la hauteur. Autrement dit, une régression à la fois multiple et polynomiale. summary(lm5 &lt;- lm(data = trees, volume ~ diameter + I(diameter^2) + height)) # # Call: # lm(formula = volume ~ diameter + I(diameter^2) + height, data = trees) # # Residuals: # Min 1Q Median 3Q Max # -0.12169 -0.04806 -0.00237 0.05156 0.12559 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) -0.267097 0.284895 -0.938 0.356798 # diameter -3.281421 1.463401 -2.242 0.033354 * # I(diameter^2) 11.891724 2.019252 5.889 2.83e-06 *** # height 0.034788 0.008169 4.259 0.000223 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.07436 on 27 degrees of freedom # Multiple R-squared: 0.977, Adjusted R-squared: 0.9745 # F-statistic: 382.8 on 3 and 27 DF, p-value: &lt; 2.2e-16 Ah ha, ceci est bizarre ! Le R2 ajusté nous indique que le modèle serait très bon puisqu’il grimpe à 0,975. Le terme en diamètre2 reste très significatif, … mais la pente relative à la hauteur est maintenant elle aussi très significative alors que dans le modèle multiple lm2 ce n’était pas le cas. De plus, la pente à l’origine en face du diamètre semble devenir un peu plus significative. Bienvenue dans les instabilités liées aux intercorrelations entre paramètres dans les modèles linéaires complexes. "],
["rmse-critere-dakaike.html", "2.4 RMSE &amp; critère d’Akaike", " 2.4 RMSE &amp; critère d’Akaike Le R2 (ajusté) n’est pas la seule mesure d’ajustement d’un modèle. Il existe d’autres indicateurs. Par exemple, l’erreur quadratique moyenne, (root mean square error, ou RMSE en anglais) est la racine carrée de la moyenne des résidus au carré. Elle représente en quelque sorte la distance “typique” des résidus. Comme cette distance est exprimée dans les mêmes unités que l’axe y, cette mesure est particulièrement parlante. Nous pouvons l’obtenir par exemple comme ceci : modelr::rmse(lm., trees) # [1] 0.1166409 Cela signifie que l’on peut s’attendre à ce que, en moyenne, les valeurs prédites de volume de bois s’écartent (dans un sens ou dans l’autre) de 0,117 m3 de la valeur effectivement observée. Evidemment, plus un modèle est bon, plus le RMSE est faible, contrairement au R2 qui lui doit être élevé. Si le R2 comme le RMSE sont utiles pour quantifier la qualité d’ajustement d’une régression, ces mesures sont peu adaptées pour la comparaison de modèles entre eux. En effet, nous avons vu que plus le modèle est complexe, mieux il s’ajuste dans les données. Le R2 ajusté tente de remédier partiellement à ce problème, mais cette métrique reste peu fiable pour comparer des modèles très différents. Le critère d’Akaike, du nom du statisticien japonais qui l’a conçu, est une métrique plus adaptée à de telles comparaisons. Elle se base au départ sur encore une autre mesure de la qualité d’ajustement d’un modèle : la log-vraisemblance. Les explications relatives à cette mesure sont obligatoirement complexes d’un point de vue mathématique et nous vous proposons ici d’en retenir la définition sur un plan purement conceptuel. Un estimateur de maximum de vraisemblance est une mesure qui permet d’inférer le meilleur ajustement possible d’une loi de probabilité par rapport à des données. Dans le cas de la régression par les moindres carrés, la distribution de probabilité à ajuster est celle des résidus (pour rappel, il s’agit d’une distribution Normale de moyenne nulle et d’écart type constant \\(\\sigma\\)). La log-vraisemblance, pour des raisons purement techniques est souvent préféré au maximum de vraissemblance. Il s’agit simplement du logarithme de sa valeur. Donc, plus la log-vraisemblance est grande, mieux les données sont compatibles avec le modèle probabiliste considéré. Pour un même jeu de données, ces valeurs sont comparables entre elles… même pour des modèles très différents. Mais cela ne règle pas la question de la complexité du modèle. C’est ici qu’Akaike entre en piste. Il propose le critère suivant : \\[ \\textrm{AIC} = -2 . \\textrm{log-vraisemblance} + 2 . \\textrm{nbrpar} \\] où nbrpar est le nombre de paramètres à estimer dans le modèle. Donc ici, nous prenons comme point de départ moins deux fois la log-vraisemblance, une valeur a priori à minimiser, mais nous lui ajoutons le second terme de pénalisation en fonction de la complexité du modèle valant 2 fois le nombre de paramètres du modèle. Notons d’ailleurs que le terme multiplicateur 2 ici est modifiable. Si nous voulons un modèle le moins complexe possible, nous pourrions très bien multiplier par 3 ou 4 pour pénaliser encore plus. Et si nous voulons être moins restrictifs, nous pouvons aussi diminuer ce facteur multiplicatif. Dans la pratique, le facteur 2 est quand même très majoritairement adapté par les praticiens, mais la possibilité de changer l’impact de complexité du modèle est inclue dans le calcul de facto. Dès lors que ce critère peut être calculé (et R le fait pour pratiquement tous les modèles qu’il propose), une comparaison est possible avec pour objectif de sélectionner le, ou un des modèles qui a l’AIC la plus faible. N’oubliez toutefois pas de comparer visuellement les différents modèles ajustés et d’interpréter les graphiques d’analyse des résidus respectifs en plus des valeurs d’AIC. C’est l’ensemble de ces outils qui vous orientent vers le meilleur modèle, pas l’AIC seul ! Calculons maintenant les critères d’Akaike pour nos 6 modèles lm. à lm5… AIC(lm.) # Linéaire diamètre # [1] -39.24246 AIC(lm2) # Multiple diamètre et hauteur # [1] -43.82811 AIC(lm3) # Polynomial diamètre # [1] -53.50964 AIC(lm4) # Diamètre^2 # [1] -50.15027 AIC(lm5) # Multiple et polynomial # [1] -67.4391 D’après ce critère, le modèle linéaire est le moins bon, et le dernier modèle le plus complexe serait le meilleur. Notez toutefois que la différence est relativement minime (en regard du gain total) entre le modèle polynomial complet lm3 et la version simplifié au seul terme diamètre2 en lm4, ce qui permet de penser que cette simplification est justifiée. Dans l’hypothèse où nous déciderions de conserver le modèle lm5, en voici l’analyse des résidus qui est bonne dans l’ensemble : #plot(lm5, which = 1) lm5 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(lm5, which = 2) lm5 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(lm5, which = 3) lm5 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(lm5, which = 4) lm5 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Naturellement, même si c’est le cas ici, ce n’est pas toujours le modèle le plus complexe qui “gagne” toujours. Même ici, nous pourrions nous demander si le modèle polynomial utilisant uniquement le diamètre ne serait pas plus intéressant en pratique car son ajustement est tout de même relativement bon (même si son critère d’Akaike est nettement moins en sa faveur), mais d’un point de vue pratique, il nous dispense de devoir mesurer la hauteur des arbres pour prédire le volume de bois. Ce n’est peut-être pas négligeable comme gain, pour une erreur de prédiction légèrement supérieure si on compare les valeurs de RMSE. modelr::rmse(lm5, trees) # Multiple et polynomial # [1] 0.06939391 modelr::rmse(lm3, trees) # Polynomial diamètre # [1] 0.08972287 L’erreur moyenne d’estimation du volume de bois passe de 0,07 m3 pour le modèle le plus complexe lm5 utilisant à la fois le diamètre et la hauteur à 0,09 m3. C’est à l’exploitant qu’il appartient de déterminer si le gain de précision vaut la peine de devoir effectuer deux mesures au lieu d’une seule. Mais au moins, nous sommes capables, en qualité de scientifiques des données, de lui proposer les alternatives possible et d’en quantifier les effets respectifs. Différentes méthodes d’ajustement par xkcd. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;02b_reg_poly&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de laboratoire sur la biométrie des oursins avec l’assignation ci-dessous. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/5hI-HSOv Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt urchin. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Réalisez un rapport scientifique sur la croissance des escargots géants d’Afrique. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/_wJZDbNp Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt achatina. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. "],
["mod-lineaire.html", "Module 3 Modèle linéaire", " Module 3 Modèle linéaire Objectifs Comprendre le modèle linéaire (ANOVA et régression linéaire tout en un) Appréhender la logique des matrices de contraste Découvrir l’ANCOVA Comprendre le mécanisme du modèle linéaire généralisé Prérequis L’ANOVA (modules 10 &amp; 11 du cours SDD 1), ainsi que la régression linéaires (modules 1 et 2 du présent cours) doivent être maitrisés avant d’aborder cette matière. "],
["variables-numeriques-ou-facteurs.html", "3.1 Variables numériques ou facteurs", " 3.1 Variables numériques ou facteurs L’ANOVA analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes qualitatives. Ces variables sont dites “facteurs” non ordonnés (objets de classe factor), ou “facteurs” ordonnés (objets de classe ordered) dans R. La régression linéaire analyse une variable dépendante numérique en fonction d’une ou plusieurs variables indépendantes numérique (quantitatives) également. Ce sont des objets de classe numeric (ou éventuellement integer, mais assimilé à numeric concrètement) dans R. Donc, la principale différence entre ANOVA et régression linéaire telles que nous les avnos abordés jusqu’ici réside dans la nature de la ou des variables indépendantes, c’est-à-dire, leur type. Pour rappel, il existe deux grandes catégories de variables : quantitatives et qualitatives, et deux sous-catégories pour chacune d’elle. Cela donne quatyre types principaux de variables, formant plus de 90% des cas rencontrés : variables quantitatives continues représentables par des nombres réels (numeric dans R), variables quantitatives discrètes pour des dénombrements d’événements finis par exemple, et représentables par des nombres entiers (integer dans R), variables qualitatives ordonnées pour des variables prenant un petit nombre de valeurs, mais pouvant être ordonnées de la plus petite à la plus grande (ordered dans R), variables qualitatives non ordonnées prenant également un petit nombre de valeurs possibles, mais sans ordre particulier (factor dans R). Par la suite, un encodage correct des variables sera indispensable afin de distinguer correctement ces différentes situations. En effet, R considèrera automatiquement comment mener l’analyse en fonction de la classe des variables fournies. Donc, si la classe est incorrecte, l’analyse le sera aussi ! Si vous avez des doutes concernant les types de variables, relisez la section type de variables avant de continuer ici. "],
["anova-et-regression-lineaire.html", "3.2 ANOVA et régression linéaire", " 3.2 ANOVA et régression linéaire Avez-vous remarqué une ressemblance particulière entre la régression linéaire que nous avons réalisé précédement et l’analyse de variance (ANOVA) ? Les plus observateurs auront mis en avant que la fonction de base dans R est la même dans les deux cas : lm(). Cette fonction est donc capable de traiter aussi bien des variables réponses qualitatives que quantitatives, et effectue alors une ANOVA dans un cas ou une régression linéaire dans l’autre. Par ailleurs, nous avons vu que l’ANOVA et la régression linéaire se représentent par des modèles semblables : \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire, avec \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\) dans les deux cas. Donc, nous retrouvons bien au niveau du modèle mathématique sous-jacent la différence principale entre les deux qui réside dans le type de variable indépendante (ou explicative) : Variable qualitative pour l’ANOVA, Variable quantitative pour la régression linéaire. Le calcul est, en réalité, identique en interne. Il est donc possible de généraliser ces deux approches en une seule appelée modèle linéaire, mais à condition d’utiliser une astuce pour modifier nos modèles afin qu’ils soient intercompatibles. 3.2.1 Modèle linéaire commun Le nœud du problème revient donc à transformer nos modèles mathématiques pour qu’ils puissent être fusionnés en un seul. Comment homogénéiser ces deux modèles ? \\(y = \\mu + \\tau_i + \\epsilon\\) pour l’ANOVA et \\(y = \\beta_1 + \\beta_2 x + \\epsilon\\) pour la régression linéaire. Avant de poursuivre, réfléchisser un peu par vous-même. Quelles sont les différences qu’il faut contourner ? Est-il possible d’effectuer une ou plusieurs transformations des variables pour qu’elles se comportent de manière similaire dans les deux cas ? 3.2.2 Réencodage des variables de l’ANOVA Considérons dans un premier temps, un cas très simple : une ANOVA à un facteur avec une variable indépendante qualitative (factor) à deux niveaux5. Nous pouvons écrire : \\[ y = \\mu + \\tau_1 I_1 + \\tau_2 I_2 + \\epsilon \\] avec \\(I_i\\), une variable dite indicatrice créée de toute pièce qui prend la valeur 1 lorsque le niveau correspond à i, et 0 dans tous les autres cas. Vous pouvez vérifier par vous-même que l’équation ci-dessus fonctionnera exactement de la même manière que le modèle utilisé jusqu’ici pour l’ANOVA. En effet, poiur un individu de la population 1, \\(I_1\\) vaut 1 et \\(\\tau_1\\) est utilisé, alors que comme \\(I_2\\) vaut 0, \\(\\tau_2\\) est annulé dans l’équation car \\(\\tau_2 I_2\\) vaut également 0. Et c’est exactement l’inverse qui se produit pour un individu de la population 2, de sorte que c’est \\(\\tau_2\\) qui est utilisé cette fois-ci. Notez que notre nouvelle formulation, à l’aide de variables indicatrices ressemble fortement à la régression linéaire. La seule différence par rapport à cette dernière est que nos variables \\(I_i\\) ne peuvent prendre que des valeurs 0 ou 1 (en tous cas, pour l’instant), alors que les \\(x_i\\) dans la régression linéaire multiple sont des variables quantitatives qui peuvent prendre une infinité de valeurs différentes (nombres réels). Nouys pouvons encore réécrire notre équation comme suit pour qu’elle se rapproche encore plus de celle de la régression linéaire simple. Passons par l’introduction de deux termes identiques \\(\\tau_1 I_2\\) additionné et soustrait, ce qui revient au même qu’en leur absence : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 - \\tau_1 I_2 + \\tau_2 I_2 + \\epsilon \\] En considérant \\(\\beta_2 = \\tau_2 - \\tau_1\\), cela donne : \\[ y = \\mu + \\tau_1 I_1 + \\tau_1 I_2 + \\beta_2 I_2 + \\epsilon \\] En considérant \\(\\beta_1 = \\mu + \\tau_1 = \\mu + \\tau_1 I_1 + \\tau_1 I_2\\) (car quelle que soit la population à laquelle notre individu appartient, il n’y a jamais qu’une seule des deux valeurs \\(I_1\\) ou \\(I_2\\) non nulle et dans tous les cas le résultat est donc égal à \\(\\tau_1\\)), on obtient : \\[ y = \\beta_1 + \\beta_2 I_2 + \\epsilon \\] Cette dernière formulation est strictement équivalente au modèle de la régression linéaire simple dans laquelle la variable \\(x\\) a simplement été remplacée par notre variable indicatrice \\(I_2\\). Ceci se généralise pour une variable indépendante à \\(k\\) niveaux, avec \\(k - 1\\) variables indicatrices au final. En prenant soin de réencoder le modèle de l’ANOVA relatif aux variables indépendantes qualitatives, nous pouvons à présent mélanger les termes des deux modèles en un seul : notre fameux modèle linéaire. Nous aurons donc, quelque chose du genre (avec les \\(x_i\\) correspondant aux variables quantitatives et les \\(I_j\\) des variables indicatrices pour les différents niveaux des variables qualitatives) : \\[ y = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + ... + \\beta_n I_1 + \\beta_{n+1} I_2 ... + \\epsilon \\] Concrètement, un cas aussi simple se traite habituellement à l’aide d’un test t de Student, mais pour notre démonstration, nous allons considérer ici utiliser une ANOVA à un facteur plutôt.↩ "],
["matrice-de-contraste.html", "3.3 Matrice de contraste", " 3.3 Matrice de contraste La version que nous avons étudié jusqu’ici pour nos variables indicatrices, à savoir, une seule prend la valeur 1 lorsque toutes les autres prend une valeur zéro, n’est qu’un cas particulier de ce qu’on appelle les contrastes appliqués à ces variables indicatrices. En réalité, nous pouvons leurs donner bien d’autres valeurs (on parle de poids), et cela permettra de considérer dses contrastes différents, eux-mêmes représentatifs de situations différentes. Afin de mieux comprendre les contrastes appliqués à nos modèles linéaires, les statisticiens ont inventé les matrices de contrastes. Ce sont des tableaux à deux entrées indiquant pour chaque niveau de la variable indépendante qualitative quelles sont les valeurs utilisées pour les différentes variables indicatrices présentées en colonne. Dans le cas de notre version simplifiée du modèle mathématique où nous avons fait disparaitre \\(I_1\\) en l’assimilant à la moyenne \\(\\mu\\) pour obteniur \\(\\beta_1\\). Dans le cas où notre variable qualitative a quatre niveaux, nous avons donc le modèle suivant : \\[ y = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + \\beta_4 I_4 + \\epsilon \\] Cela revient à considérer le premier niveau comme niveau de référence et à établir tous les contrastes par rapport à ce niveau de référence. C’est une situation que l’on rencontre fréquemment lorsque nos testons l’effet de différents médicaments ou de différents traitement par rapport à un contrôle (pas de traitement, ou placébo). La matrice de contrastes correspondante, dans un cas où on aurait trois traitements en plus du contrôle (donc, notre variable factor à quatre niveaux) s’obteint facilement dans R à l’aide de la fonction contr.treatment() : contr.treatment(4) # 2 3 4 # 1 0 0 0 # 2 1 0 0 # 3 0 1 0 # 4 0 0 1 Les lignes de cette matrice sobnt numérotées de 1 à 4. Elles correspondent aux quatres niveaux de notre variable factor, avec le niveau 1 qui doit nécessairement correspondre à la situation de référtence, donc au contrôle. Les colonnes de cette matrice correspondent aux trois variables indicatrices \\(I_1\\), \\(I_2\\) et \\(I_3\\) de l’équation au dessus. Nous voyons que pour une individu contrôle, de niveau 1, les trois \\(I_i\\) prennent la valeur 0. Nous sommes bien dans la situation de référence. En d’autres terme, le modèle de base est ajusté sur la moyenne des individus contrôle. Notre modèle se réduit à : \\(y = \\beta_1 + \\epsilon\\). Donc, seule la moyenne des individus contrôles, \\(\\beta_1\\) est considérée, en plus des résidus \\(\\epsilon\\) bien sûr. Pour le niveau deux, nous observons que \\(I_2\\) vaut 1 et les deux autres \\(I_i\\) valent 0. Donc, cela revient à considérer un décalage constant \\(\\beta_2\\) appliqué par rapport au modèle de référence matérialisé par \\(\\beta_1\\). En effezt, notre équation se réduit dans ce cas à : \\(y = \\beta_1 + \\beta_2 + \\epsilon\\). Le même raisonnement peut être fait pour les niveaux 3 et 4, avec des décalages constants par rapport à la situation cxontrôle de respectivement \\(\\beta_3\\) et \\(\\beta_4\\). En d’autres termes, les contrastes qui sont construits ici font tous référence au contrôle, et chaque médicament est explicitement comparté au contrôle (mais les médicaments ne sont pas comparés entre eux). Nous voyons donc que les variables indicatrices etr la matrice de contrastes permet de spécifier quelles sont les contrastes pertinents et éliminent ceux qui ne le sont pas (nous n’utilisons donc pas systématiquement toutes les comparaisons deux à deux des différents niveaux6). 3.3.1 Contraste orthogonaux Les contrastes doivent être de préférence orthogonaux par rapport à l’ordonnée à l’origine, ce qui signifie que la somme de leurs pondérations doit être nulle pour tous les contrastes définis (donc, en colonnes). Bien que n’étant pas obligatoire, cela confère des propriétés intéressantes au modèle (l’explication et la démonstration sortent du cadre de ce cours). Or, les contrastes de type traitement ne sont pas orthogonaux puisque toutes les sommes par colonnes vaut un. 3.3.2 Autres matrices de contrastes courantes Somme à zéro. Ces constraste, toujours pour une variable à quatre niveaux, se définissen t comme suit en utilisant la fonction contr.sum() dans R : contr.sum(4) # [,1] [,2] [,3] # 1 1 0 0 # 2 0 1 0 # 3 0 0 1 # 4 -1 -1 -1 Ici nous avons bien des contrastes orthogonaux puisque toutes les sommes par colonnes valeur zéro. Dans le cas présent, aucun niveau n’est considéré comme référence, mais les n - 1 niveaux sont systématiquement contrastés avec le dernier et nîème^ niveau. Ainsi, un contraste entre deux niveaux particuliers peut s’élaborer en indiquant une pondération de 1 pour le premier niveau à comparer, une pondération de -1 pour le second à comparer et une pondération de 0 pour tous les autres. Matrice de contrastes de Helmert : chaque niveau est comparé à la moyenne des niveaux précédents. La matrice de constrastes correspondant pour une variable à quatre niveaux s’obtient à l’aide de la fonction R contr.helmert() : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 Cette matrice est également orthogonale avec toutes les sommes par colonnes qui valent zéro. Ici, nous découvrons qu’il est possible de créer un contrastye entre un niveau et la moyenne de plusieurs autres niveaux en mettant le poids du premier à m (le nombre de populations à comparer de l’autre côté du contraste), et les poids des autres populations tous à -1. Ainsi, la colonne 4 compare le niveau quatre avec pondération 3 aux trois autres niveaux qui reçoivent tous une pondération -1. Matrice de contrastes polynomiaux : adapté aux facteurs ordonnés (ordered dans R) pourvlesquels on s’attend à une certaine évolution du modèle du niveau le plus petit au plus grand. Donc ici aussi une comparaison deux à deux de tous les niveaux n’est pas souhaitable, mais une progression d’un effet qui se propage de manière graduelle du plus petit niveau au plus grand. A priori cela parait difficile à métérialiser dans une matrice de contraste… et pourtant, c’est parfaitement possible ! Il s’agit de constrastes polynomiaux où nous ajustons de polynomes de degré croissant comme pondération des différents contrastes étudiés. La fonction contr.poly() permet d’obtenir ce type de contraste dans R. Pour une variable ordonnée à quatre niveaux, cela donne : contr.poly(4) # .L .Q .C # [1,] -0.6708204 0.5 -0.2236068 # [2,] -0.2236068 -0.5 0.6708204 # [3,] 0.2236068 -0.5 -0.6708204 # [4,] 0.6708204 0.5 0.2236068 Ici, les pondérations sont plus difficiles à expliquer rien qu’en observant la matrice de contrastes. De plus, les colonnes portent ici des noms particuliers .L pour un contraste linéaire (polynome d’ordre 1), .Q pour un contraste quadratique (polynome d’ordre 2), et .C pour un contraste conique (ou polynome d’ordre 3). Les pondérations appliquées se comprennent mieux lorsqu’on augmente le nombre de niveaux etr que l’on représente graphiquement la valeur des pondérations choisées. Par exemple, pour une variable facteur ordonnée à dix niveaux, nous représentrons graphiquement les 3 premeirs contrastes (linéaire, quadratique et conique) comme suit : plot(contr.poly(10)[, 1], type = &quot;b&quot;) plot(contr.poly(10)[, 2], type = &quot;b&quot;) plot(contr.poly(10)[, 3], type = &quot;b&quot;) Sur le graphique, l’axe X nommé index correspiond en réalité à la succession des 10 niveaux de la variable présentés dans l’ordre du plus petit au plus grand. Nous voyons maintenant clairement comment les contrastes sont construits ici. Pour le conbtraste linéaire, on contraste les petits niveaux avec les grands, et ce, de manière proportionnelle par rapport à la progression d’un niveau à l’autre (polynome d’ordre un = droite). Pour le contraste quadratique, on place “dans le même sac” les petits et greand niveaux qui sont contrastés avec les niveaux moyens (nous avons une parabole ou polynome d’ordre 2). Pour le troisième graphique, la situation se complexifie en encore un peu plus avec un polynome d’ordre 3, et ainsi de suite pour des polynomes d’ordres croissants jusqu’à remplir complètement la matrice de contrastes. R utilise par défaut des contrastes de traitement pour les facteurs non ordonnés et des contrastes polynomiaux pour des facteurs ordonnés. Ces valeurs par défaut sont stockées dans l’option contrasts que l’on peut lire à l’aide de getOption(). Bien sûr, il est possible de changer ces contrastes, tant au niveau global qu’au niveau de la construction d’un modèle en particulier. getOption(&quot;contrasts&quot;) # unordered ordered # &quot;contr.treatment&quot; &quot;contr.poly&quot; Attention : le fait d’utiliser une matrtice de contraste qui restreint ceux utilisés dans le modèle est indépendant des tests post hoc de comparaisons multiples, qui restent utilisables par après. Les comparaisons deux à deux des médicaments restent donc accessibles, mais ils ne sont tout simplement pas mis en évidence dans le modèle de base.↩ "],
["ancova.html", "3.4 ANCOVA", " 3.4 ANCOVA Avant l’apparition du modèle linéaire, une version particulière d’un mélange de régression linéaire et d’une ANOVA avec une variable indépendante quantitative et une autre variable indépendante qualitative s’appelait une ANCOVA (ANalyse de la COVariance). Un tel modèle d’ANCOVA peut naturellement également se résoudre à l’aide de la fonction lm() qui, en outre, peut faire bien plus. Nous allons maintenant ajuster un tel modèle à titre de première application concrète de tout ce que nous venons de voir sur le modèle linéaire et sur les matrices de contrastes associées. 3.4.1 Bébés à la naissance Nous étudions la masse de nouveaux nés en fonction du poids de la mère et du fait qu’elle fume ou non. Cette analyse s’inspire de Verzani (2005). Nous avons donc ici une variable dépendante wt, la masse des bébés qui est quantitative, et deux variables indépendantes ou prédictives wt1, la masse de la mère, et smoke le fait que la mère fume ou non. Or la première de ces variables explicatives est quantitative (wt1) et l’autre (smoke) est une variable facteur à quatre niveaux (0 = la mère n’a jamais fumé, 1 = elle fume y compris pendant la grossesse, 2 = elle fumait mais a arrêté à la grossesses, et 3 = la mère a fumé, mais a arrêté, et ce, bien avant la grossesse. Un dernier niveau 9 = inconnu encode de manière non orthodoxe les valeurs manquantes dans notre tableau de données (valeurs que nous éliminerons). De même les masses des nouveaux nés et des mères sont des des unités impériales (américaines) respectivement en “onces” et en “livres”. Enfin, nous devons prendre soin de bien encoder la variable smoke comme une variable factor (ici nous ne considèrerons pas qu’il s’agit d’un facteur ordonné et nous voulons faire un contraste de type traitement avec comparaison à des mères qui n’ont jamais fumé). Un reminement soigneux des données est donc nécessaire avant de pouvoir appliquer notre modèle ! SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) knitr::kable(head(babies)) id pluralty outcome date gestation sex wt parity race age ed ht wt1 drace dage ded dht dwt marital inc smoke time number 15 5 1 1411 284 1 120 1 8 27 5 62 100 8 31 5 65 110 1 1 0 0 0 20 5 1 1499 282 1 113 2 0 33 5 64 135 0 38 5 70 148 1 4 0 0 0 58 5 1 1576 279 1 128 1 0 28 2 64 115 5 32 1 99 999 1 2 1 1 1 61 5 1 1504 999 1 123 2 0 36 5 69 190 3 43 4 68 197 1 8 3 5 5 72 5 1 1425 282 1 108 1 0 23 5 67 125 0 24 5 99 999 1 1 1 1 5 100 5 1 1673 286 1 136 4 0 25 2 62 93 3 28 2 64 130 1 4 2 2 2 Ce tableau est “brut de décoffrage”. Voyez help(&quot;babies&quot;, package = &quot;UsingR&quot;) pour de plus amples informations. Nous allons maintenant remanier tout cela correctement. # wt = masse du bébé à la naissance en onces et 999 = valeur manquante # wt1 = masse de la mère à la naissance en livres et 999 = valeur manquante # smoke = 0 (non), = 1 (oui), = 2 (jusqu&#39;à grossesse), # = 3 (plus depuis un certain temps) and = 9 (inconnu) babies %&gt;.% select(., wt, wt1, smoke) %&gt;.% # Garder seulement wt, wt1 &amp; smoke filter(., wt1 &lt; 999, wt &lt; 999, smoke &lt; 9) %&gt;.% # Eliminer les valeurs manquantes mutate(., wt = wt * 0.02835) %&gt;.% # Transformer le poids en kg mutate(., wt1 = wt1 * 0.4536) %&gt;.% # Idem mutate(., smoke = as.factor(smoke)) -&gt; # S&#39;assurer d&#39;avoir une variable factor Babies # Enregistrer le résultat dans Babies knitr::kable(head(Babies)) wt wt1 smoke 3.40200 45.3600 0 3.20355 61.2360 0 3.62880 52.1640 1 3.48705 86.1840 3 3.06180 56.7000 1 3.85560 42.1848 2 Description des données : skimr::skim(Babies) # Skim summary statistics # n obs: 1190 # n variables: 3 # # ── Variable type:factor ─────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts # smoke 0 1190 1190 4 0: 531, 1: 465, 3: 102, 2: 92 # ordered # FALSE # # ── Variable type:numeric ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # wt 0 1190 1190 3.39 0.52 1.56 3.06 3.4 3.71 4.99 # wt1 0 1190 1190 58.3 9.49 39.46 51.82 56.7 62.6 113.4 # hist # ▁▁▂▆▇▅▁▁ # ▂▇▆▂▁▁▁▁ chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ smoke) + geom_boxplot() + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt1 ~ smoke) + geom_boxplot() + ylab(&quot;Masse de la mère [kg]&quot;) Visuellement, nous ne voyons pas d’effet marquant. Peut-être la condition 1 de smoke (mère qui fume pendant la grossesse) mène-t-il à des bébés moins gros, mais est-ce significatif ? Pour cela, ajustons notre modèle ANCOVA avec matrice traitement (choix par défaut pour une la variable factor smoke). Comme nous savons déjà utiliser lm(), c’est très simple. Cela fonctionne exactement comme avant7. # ANCOVA Babies_lm &lt;- lm(data = Babies, wt ~ smoke * wt1) summary(Babies_lm) # # Call: # lm(formula = wt ~ smoke * wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.9568 -0.3105 0.0133 0.3136 1.4989 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.128333 23.382 &lt; 2e-16 *** # smoke1 -0.303614 0.196930 -1.542 0.123405 # smoke2 0.901888 0.371393 2.428 0.015314 * # smoke3 -0.035502 0.371379 -0.096 0.923858 # wt1 0.008117 0.002149 3.777 0.000167 *** # smoke1:wt1 0.001153 0.003346 0.345 0.730444 # smoke2:wt1 -0.015340 0.006390 -2.401 0.016523 * # smoke3:wt1 0.001177 0.006147 0.191 0.848258 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4992 on 1182 degrees of freedom # Multiple R-squared: 0.08248, Adjusted R-squared: 0.07705 # F-statistic: 15.18 on 7 and 1182 DF, p-value: &lt; 2.2e-16 anova(Babies_lm) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.9636 1.158e-15 *** # wt1 1 6.162 6.1621 24.7325 7.559e-07 *** # smoke:wt1 3 1.653 0.5511 2.2117 0.08507 . # Residuals 1182 294.497 0.2492 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de variance montre que la masse de la mère a un effet significatif au seuil alpha de 5%, de même si la mère fume. Par contre, il n’y a pas d’interactions entre les deux. Le fait de pouvoir meurer des interactions entre variables qualitatives et quantitatives est ici bien évidemment un plus du modèle linéaire par rapport à ce qu’on pouvait faire avant ! Le résumé de l’analyse nous montre que la régression de la masse des bébés en fonction de la masse de la mère (ligne wt1 dans le tableau des coefficients), bien qu’étant significative, n’explique que 8% de la variance totale (le \\(R^2\\)). Les termes smoke1, smoke2 et smoke3 sont les contrastes appliqués par rapport au contrôle (smoke == 0). On voit ici qu’aucun de ces contrastes n’est significatif au seuil alpha de 5%. Cela signifie que le seul effet significatif est celui lié à une ordonnée à l’origine non nulle (Intercept) matérialisant la condition smoke == 0. Cela signifie que des mères de masse nulle n’ayant jamais fumé engendreraient des bébés pesant environ 3kg. Dans le contexte présent, cette constatation n’a bien sûr aucun sens, et l’interprétation de l’ordonnée à l’origine ne doit pas être faite. Donc, le modèle linéaire, en offrant plus de contrôle dans notre ajustement et une définition de contrastes “utiles” matérialisés par les lignes smoke1, smoke2 et smoke3 du tableau nous permet de faire des tests plus utiles dans le contexte de notre analyse. N’oublions pas non plus la possibilité de déterminer si des interactions entre smoke et wt1 existent pour ces différents contrastes, interactions testées respectivements aux lignes smoke1:wt1, smoke2:wt1, et smoke3:wt1du tableau des coefficients. Dans le cas présent, aucune de ces interactions n’est siginificative au seuil alpha de 5%. Pour comprendre à quoi tout cela fait référence, il faut considérer le modèle de base comme une droite de régression ajustée entre wt et wt1 pour la population de référence smoke == 0. Ainsi, si nous faisons : summary(lm(data = Babies, wt ~ wt1, subset = smoke == 0)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 0) # # Residuals: # Min 1Q Median 3Q Max # -1.95685 -0.25825 0.01476 0.25464 1.49890 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.000663 0.123572 24.283 &lt; 2e-16 *** # wt1 0.008117 0.002069 3.922 9.92e-05 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4806 on 529 degrees of freedom # Multiple R-squared: 0.02826, Adjusted R-squared: 0.02642 # F-statistic: 15.38 on 1 and 529 DF, p-value: 9.924e-05 Nous voyons en effet que les pentes et ordonnées à l’origine sont ici parfaitement identiques au modèle ANCOVA complet (mais pas les tests associés). Maintenant plus difficile : à quoi correspond une régression entre wt et wt1 pour smoke == 1 ? summary(lm(data = Babies, wt ~ wt1, subset = smoke == 1)) # # Call: # lm(formula = wt ~ wt1, data = Babies, subset = smoke == 1) # # Residuals: # Min 1Q Median 3Q Max # -1.70870 -0.35089 0.01034 0.33576 1.39420 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.697048 0.153270 17.597 &lt; 2e-16 *** # wt1 0.009270 0.002632 3.522 0.000471 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.5122 on 463 degrees of freedom # Multiple R-squared: 0.02609, Adjusted R-squared: 0.02399 # F-statistic: 12.4 on 1 and 463 DF, p-value: 0.0004711 Nous avons une ordonnées à l’origine qui vaut 2,70 ici. Notons que cela correspond aussi à (Intercept) + smoke1 = 3,00 - 0,30 = 2,70. Donc, l’ordonnées à l’origine pour smoke == 1 est bien la valeur de référence additionnée de la valeur fournie à la ligne smoke1 dans l’ANCOVA. Cela se vérifie aussi pour les deux autres droites pour smoke2 et smoke3. Maintenant, la pente pour notre droite ajustée sur la population smoke == 1 uniquement vaut 0,00927. Dans l’ANCOVA, nous avions une pente wt1 de 0,00812 et une interaction smoke1:wt1 claculée comme 0,00115. Notez alors que la pente de la droite seule 0,00927 = 0,00812 + 0,00115. Donc, tout comme smoke1 correspond au décalage de l’ordonnée à l’origine du modèle de référence, les interactions smoke1:wt1 correspondent au décalage de la pente par rapport au modèle de référence. Cela se vérifie également pour smoke2:wt1 et smoke3:wt1. Donc, notre modèle complet ne fait rien d’autre que d’ajuster les quatre droites correspondant aux relations linéaires entre wt et wt1, mais en décompose les effets, niveau par niveau de la variable qualitative smoke en fonction de la matrice de contraste que l’on a choisie. En bonnus, nous avons la possibilité de tester si chacune des composantes (tableau coefficient de summary()) ou si globalement chacune des variables (tableau obtenu avec anova()) a un effet significatif ou non dans le modèle. Le graphique correspondant est le même que si nous avions ajusté les 4 régressions linéaires indépendamment l’une de l’autre (mais les tests et les enveloppes de confiance diffèrent). chart(data = Babies, wt ~ wt1 %col=% smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) chart(data = Babies, wt ~ wt1 | smoke) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Comme toujours, lorsqu’un effet n’est pas siugnificatif, nous pouvons décider de simplifier le modèle. Mais attention ! Toujours considérer que les composantes sont interdépendantes. Donc, éliminer une composante du modèle peut avoir des effets parfois surprenants sur les autres. Voyons ce que cela donne si nous éliminons les interactions. Dans ce cas, nous ajustons des droites toutes parallèles avec uniquement un décalage de leur ordonnée à l’origine matérialisé par smoke1, smoke2 et smoke3 par rapport au modèle de référence ajusté pour la population smoke == 0 (notez l’utilisation, du signe + dans la formuile, là où nous utilisions le signe * dans la modèle précédent). # ANCOVA Babies_lm2 &lt;- lm(data = Babies, wt ~ smoke + wt1) summary(Babies_lm2) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 -0.237938 0.031816 -7.478 1.46e-13 *** # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 0.035486 0.054068 0.656 0.512 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm2) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hé, ça c’est intéressant ! Maintenant que nous avons éliminé les interactions qui apparaissent non pertinentes ici, nous avons toujours une régression significative entre wt et wt1 (mais avec un \\(R^2\\) très faible de 7,7%, attention), mais maintenant, nous faisons apparaitre un effet signicfication du contraste avec smoke1 au seuil alpha de 5%. Et du coup, les effets des deux variables deviennent plus clairs dans notre tableau de l’ANOVA. Le graphique correspondant est l’ajustement de droites parallèles les unes aux autres pour les 4 sous-populations en fonction de smoke. Ce graphique est difficile à réaliser. Il faut ruser, et les détails du code vont au delà de ce cours (il n’est pas nécessaire de les comprendre à ce stade). cols &lt;- iterators::iter(scales::hue_pal()(4)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke)) + lapply(c(0, -0.238, 0.0227, 0.0355), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Voyons ce que donne l’analyse post hoc des comparaisons multiples (nous utilisons ici simplement le snippet disponible à partir de ... -&gt; hypothesis tests -&gt; hypothesis tests: means -&gt; hmanovamult : anova - multiple comparaisons [multcomp]) que nous avons déjà employé et qui reste valable ici. summary(anovaComp. &lt;- confint(multcomp::glht(Babies_lm2, linfct = multcomp::mcp(smoke = &quot;Tukey&quot;)))) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = wt ~ smoke + wt1, data = Babies) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # 1 - 0 == 0 -0.23794 0.03182 -7.478 &lt; 1e-05 *** # 2 - 0 == 0 0.02267 0.05651 0.401 0.977 # 3 - 0 == 0 0.03549 0.05407 0.656 0.908 # 2 - 1 == 0 0.26060 0.05704 4.568 3.1e-05 *** # 3 - 1 == 0 0.27342 0.05478 4.991 &lt; 1e-05 *** # 3 - 2 == 0 0.01282 0.07199 0.178 0.998 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) .oma &lt;- par(oma = c(0, 5.1, 0, 0)); plot(anovaComp.); par(.oma); rm(.oma) Ici, comme nous testons tous les contrastes, nous pouvons dire que la population des mères qui ont fumé pendant la grossesse smoke == 1 donne des bébés significativement moins gros au seuil alpha de 5%, et ce, en comparaison de tous les autres niveaux (mère n’ayant jamais fumé, ou ayant fumé mais arrêté avant la grossesse, que ce soit longtemps avant ou juste avant). Il semble évident maintenant qu’il n’est pas utile de préciser si la mère a fumé ou non avant sa grossesse. L’élément déterminant est uniquement le fait de fumer pendant la grossesse ou non. Nous pouvons le montrer également en utilisant des contrastes de Helmert, à condition de recoder smoke avec des niveaux de “gravité” croissants (“0” = n’a jamais fumé, “1” = a arrêté il y a longtemps, “2” = a arrêté juste avant la grossesse et finalement, “1” = a continué à fumé à la grossesse). Il faut donc intervertir les cas “1” et “3”. Nous pouvons utiliser recode() pour cela, mais attention, nous avons ici une variable factor, donc, ce ne sont pas des nombres mais des chaines de caractères (à placer entre guillements). Une fois le recodage réalisé, il faut aussi retrier les niveaux en appelant factor(..., levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;)) sinon l’ancien ordre est conservé. Babies %&gt;.% mutate(., smoke = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;3&quot;, &quot;2&quot; = &quot;2&quot;, &quot;3&quot; = &quot;1&quot;)) %&gt;.% mutate(., smoke = factor(smoke, levels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) -&gt; Babies2 Si cela semble trop compliqué, vous pouvez aussi utiliser l’addins de réencodage dans R (QUESTIONR -&gt; Levels Recoding or Levels Ordering). A présent que l’encodage de smoke est corrigé dans Babies2, nous pouvons modéliser à nouveau, mais cette fois-ci avec des contrastes de Helmert (notez la façon particulière de spécifier des contrastes différents de la valeur pas défaut pour une variable factor) : Babies_lm3 &lt;- lm(data = Babies2, wt ~ smoke + wt1, contrasts = list(smoke = &quot;contr.helmert&quot;)) summary(Babies_lm3) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2, contrasts = list(smoke = &quot;contr.helmert&quot;)) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke1 0.017743 0.027034 0.656 0.512 # smoke2 0.001641 0.019599 0.084 0.933 # smoke3 -0.064330 0.008540 -7.533 9.82e-14 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 anova(Babies_lm3) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke 3 18.659 6.2197 24.887 1.285e-15 *** # wt1 1 6.162 6.1621 24.657 7.853e-07 *** # Residuals 1185 296.150 0.2499 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Ici les valeurs estimées pour smoke1-3 sont à interpréter en fonction des contrastes utilisés, soit : contr.helmert(4) # [,1] [,2] [,3] # 1 -1 -1 -1 # 2 1 -1 -1 # 3 0 2 -1 # 4 0 0 3 smoke1 est le décalage de l’ordonnée à l’origine entre la modèle moyen établi avec les données smoke == 0 et smoke == 1 et celui pour smoke == 1 (non significatif au seuil alpha de 5%), smoke2 est le décalage de l’ordonnée à l’origine pour smoke == 2 par rapport au modèle ajusté sur smoke == 0, smoke == 1 et smoke == 2 avec des pondérations respectives de -1, -1, et 2 (non significatif au seuil alpha de 5%), smoke3 est le décalage de l’ordonnée à l’origine par rapport au modèle ajusté sur l’ensemble des autres observations, donc, avec smoke valant 0, 1, ou 2, et des pondérations respectives comme dans la dernière colonne de la matrice de contraste.. Donc, ce dernier contraste est celui qui nous intéresse car il compare les cas où la mère n’a pas fumé pendant la grossesse avec le cas smoke == 3 où la mère a fumé pendant la grossesse, et il est significatif au seuil alpha de 5%. L’interprétation des vlauers estimées est plus complexe ici. Comparer ce résultat avec le modèle ajusté avec les contrastes traitement par défaut avec smoke réencodé : summary(lm(data = Babies2, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies2) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.030052 0.092861 32.630 &lt; 2e-16 *** # smoke1 0.035486 0.054068 0.656 0.512 # smoke2 0.022666 0.056508 0.401 0.688 # smoke3 -0.237938 0.031816 -7.478 1.46e-13 *** # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Les conclusions sont les mêmes, mais la valeurs estimées pour smoke1, smoke2 et smoke3 diffèrent. Par exemple, dans ce dernier cas, smoke1 est double de la valeur avec les contrastes Helmert, ce qui est logique puisque la référence est ici la droite ajustée pour la sous-population smoke == 0 là où dans le modèle avec les contrastes de Helmert, le décalage est mesuré par rapport au modèle moyen (donc à “mi-chemin” entre les deux droites pour smoke == 0 et smoke == 1). Naturellement, nous pouvons aussi considérer la variable smoke réencodée dans Babies2 comme une variable facteur ordonné (ordered). Dans ce cas, c’est les contrastes polynomiaux qui sont utilisés : Babies2 %&gt;.% mutate(., smoke = as.ordered(smoke)) -&gt; Babies3 summary(lm(data = Babies3, wt ~ smoke + wt1)) # # Call: # lm(formula = wt ~ smoke + wt1, data = Babies3) # # Residuals: # Min 1Q Median 3Q Max # -1.95453 -0.30780 0.01289 0.31108 1.49443 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 2.985106 0.091695 32.555 &lt; 2e-16 *** # smoke.L -0.162480 0.026780 -6.067 1.75e-09 *** # smoke.Q -0.148045 0.039294 -3.768 0.000173 *** # smoke.C -0.044605 0.048790 -0.914 0.360787 # wt1 0.007617 0.001534 4.966 7.85e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4999 on 1185 degrees of freedom # Multiple R-squared: 0.07733, Adjusted R-squared: 0.07422 # F-statistic: 24.83 on 4 and 1185 DF, p-value: &lt; 2.2e-16 Notez comment R est capable d’utiliser automatiquement les contrasts adéquats (polynomiaux) lorsque la variable facteur smoke est encodée en ordered. Nous voyons ici que des contrastes tenant compte d’une variation le long des successions croissante de niveaux de “gravité” de la variable smoke sont maintenant calculés. La ligne smoke.L du tableau Coefficients indique une variation linéaire (significative au seuil alpha de 5%), smoke.Q est une variation quadratique (également significative) et enfin smoke.C est une variation cubique. Voyez la présentation des matrices de contrastes plus haut pour bien comprendre ce qui est calculé ici. Au final, l’élément important relatif à la variable smoke est en définitive le fait de fumer pendant la grossesse ou non, pas l’histoire de la mère avant sa grossesse en matière de tabocologie ! En modélisation, nous avons toujours intérêt à choisir le modèle le plus simple. Donc ici, cela vaut le coup de simplifier smoke à une variable à deux niveaux smoke_preg qui indique uniquement si la mère fume ou non pendant la grossesse. Ensuite, nous ajustons à nouveau un modèle plus simple avec cette nouvelle variable. Babies %&gt;.% mutate(., smoke_preg = recode(smoke, &quot;0&quot; = &quot;0&quot;, &quot;1&quot; = &quot;1&quot;, &quot;2&quot; = &quot;0&quot;, &quot;3&quot; = &quot;0&quot;)) %&gt;.% mutate(., smoke_preg = factor(smoke_preg, levels = c(&quot;0&quot;, &quot;1&quot;))) -&gt; Babies Babies_lm4 &lt;- lm(data = Babies, wt ~ smoke_preg + wt1) summary(Babies_lm4) # # Call: # lm(formula = wt ~ smoke_preg + wt1, data = Babies) # # Residuals: # Min 1Q Median 3Q Max # -1.96243 -0.30708 0.01208 0.31051 1.48662 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 3.037508 0.091896 33.054 &lt; 2e-16 *** # smoke_preg1 -0.245797 0.029747 -8.263 3.76e-16 *** # wt1 0.007624 0.001531 4.981 7.25e-07 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.4996 on 1187 degrees of freedom # Multiple R-squared: 0.07692, Adjusted R-squared: 0.07537 # F-statistic: 49.46 on 2 and 1187 DF, p-value: &lt; 2.2e-16 anova(Babies_lm4) # Analysis of Variance Table # # Response: wt # Df Sum Sq Mean Sq F value Pr(&gt;F) # smoke_preg 1 18.497 18.4971 74.106 &lt; 2.2e-16 *** # wt1 1 6.193 6.1934 24.813 7.253e-07 *** # Residuals 1187 296.281 0.2496 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A présent, tous les termes de notre modèle sont significatifs au seuil alpha de 5%. La ligne smoke_preg1 est le décalage de l’ordonnée à l’origine du poids des bébés issus de mères fumant pendant la grossesse. Il donne donc directement la perte moyenne de poids du à la tabacologie. La représentation graphique de ce dernier modèle est la suivante : cols &lt;- iterators::iter(scales::hue_pal()(2)) # Get colors for lines chart(data = Babies, wt ~ wt1) + geom_point(aes(col = smoke_preg)) + lapply(c(0, -0.246), function(offset) geom_smooth(method = lm, formula = y + offset ~ x, col = iterators::nextElem(cols))) + xlab(&quot;Masse de la mère [kg]&quot;) + ylab(&quot;Masse du bébé [kg]&quot;) Enfin, n’oublions pas que notre modèle n’est valide que si les conditions d’application sont rencontrées, en particulier, une distribution normale des résidus et une homoscédasticité (même variance pour les résidus). Nous vérifions cela visuellement toujours avec les graphiques d’analyse des résidus. En voici les plus importants (pensez à utiliser les snippets pour récupérer le template du code) : #plot(Babies_lm4, which = 1) Babies_lm4 %&gt;.% chart(broom::augment(.), .resid ~ .fitted) + geom_point() + geom_hline(yintercept = 0) + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) + ggtitle(&quot;Residuals vs Fitted&quot;) #plot(Babies_lm4, which = 2) Babies_lm4 %&gt;.% chart(broom::augment(.), aes(sample = .std.resid)) + geom_qq() + geom_qq_line(colour = &quot;darkgray&quot;) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Standardized residuals&quot;) + ggtitle(&quot;Normal Q-Q&quot;) #plot(Babies_lm4, which = 3) Babies_lm4 %&gt;.% chart(broom::augment(.), sqrt(abs(.std.resid)) ~ .fitted) + geom_point() + geom_smooth(se = FALSE, method = &quot;loess&quot;, formula = y ~ x) + labs(x = &quot;Fitted values&quot;, y = expression(bold(sqrt(abs(&quot;Standardized residuals&quot;))))) + ggtitle(&quot;Scale-Location&quot;) #plot(Babies_lm4, which = 4) Babies_lm4 %&gt;.% chart(broom::augment(.), .cooksd ~ seq_along(.cooksd)) + geom_bar(stat = &quot;identity&quot;) + geom_hline(yintercept = seq(0, 0.1, by = 0.05), colour = &quot;darkgray&quot;) + labs(x = &quot;Obs. number&quot;, y = &quot;Cook&#39;s distance&quot;) + ggtitle(&quot;Cook&#39;s distance&quot;) Ici le comportement des résidus est sain. Des petits écarts de la normalité sur le graphique quantile-quantile s’observent peut-être, mais ce n’est pas dramatique et le modèle linéaire est rabuste à ce genre de petis changements d’autant plus qu’ils apparaissent relativement symétriques en haut et et en bas de la distribution. En conclusion de cette analyse, nous pouvons dire que la masse du bébé dépend de la masse de la mère, mais assez faiblement (seulement 7,7% de la variance totale expliquée). Par contre, nous pouvons aussi dire que le fait de fumer pendant la grossesse a un effet significatif sur la réduction de la masse du bébé à la naissance (en moyenne cette réduction est de 0,246kg pour une masse moyenne à la naissance de 3,038kg, soit une réduction de 0,246 / 3,034 * 100 = 8%). Voilà, nous venons d’analyser et d’interpréter notre premier modèle linéaire sous forme d’une ANCOVA. A vous de jouer ! Après cette longue lecture avec énormément de nouvelles matières, nous vous proposons les exercices suivants : Répondez aux questions d’un learnr afin de vérifier vos acquis. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;03a_mod_lin&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Poursuivez l’analyse des données sur la biométrie des oursins en y intégrant vos nouvelles notions sur le modèle linéaire Reprenez votre travail sur la biométrie des oursins et appliquer les nouvelles notions vues Références "],
["modele-lineaire-generalise.html", "3.5 Modèle linéaire généralisé", " 3.5 Modèle linéaire généralisé Le modèle linéaire nous a permis de combiner différent types de variables indépendantes ou explicatives dans un même modèle. Cependant la variable dépendante ou réponse à la gauche de l’équation doit absolument être numérique et une distribution normale est exigée pour la composante statistique du modèle exprimée dans les résidus \\(\\epsilon\\). Donc, si nous voulons modéliser une variable dépendante qui ne répond pas à ces caractéristiques, nous sommes dans l’impasse avec la fonction lm(). Dans certains cas, une transformation des données peut résoudre le problème. Par exemple, prendre le logarithme d’une variable qui a une distribution log-normale. Dans d’autres cas, il semble qu’il n’y ait pas de solution… C’est ici que la modèle linéaire généralisé vient nous sauver la mise. Le modèle linéaire généralisé se représente comme suit : \\[ f(y) = \\beta_1 + \\beta_2 I_2 + \\beta_3 I_3 + ... + \\beta_k I_k + \\beta_l x_1 + \\beta_m x_2 + ... + \\epsilon \\] La différence par rapport au modèle linéaire, c’est que notre variable dépendante \\(y\\) est transformée à l’aide d’une fonction \\(f(y)\\) que l’on appelle fonction de lien. Cette fonction de lien est choisie soigneusement pour transformer une variable qui a une distribution non-normale vers une distribution normale ou quasi-normale. Du coup, il ne faut rien changer à la droite du signe égal par rapport au modèle linéaire, et les outils existants peuvent être réemployés. Toute la difficulté ici tient donc à la définition des fonctions de liens pertinentes par rapport à la distribution de \\(y\\). Le tableau suivant reprend les principales situations prises en compte par la fonction glm() dans R qui calcule le modèle linéaire généralisé. Distribution de Y Fonction de lien Code R Gaussienne (Normale) identité (pas de transfo.) glm(..., family = gaussian(link = &quot;identity&quot;)) Log-Normale log glm(..., family = gaussian(link = &quot;log&quot;)) Binomiale logit glm(..., family = binomial(link = logit)) Binomiale probit (alternative) glm(..., family = binomial(link = probit)) Poisson log glm(..., family = poisson(link = log)) Il en existe bien d’autres. Voyez l’aide de ?family pour plus de détails. Par exemple, pour une variable réponse binaire acceptant seulement deux valeurs possibles et ayant une distribution binomiale, avec une réponse de type logistique (une variation croissante d’une ou plusieurs variables indépendantes fait passer la proportion des individus appartenant au second état selon une courbe logistique en S), une fonction de type logit est à utiliser. \\[ y = 1/(1 + e^{- \\beta x}) \\] La transformation logit calcule alors : \\(\\ln(y / (1 - y)) = \\beta x\\). Les situations correspondant à ce cas de figure concernent par exemple des variables de type (vivant versus mort) par rapport à une situation potentiellement léthale, ou alors, le développement d’une maladie lors d’une épidémie (sain versus malade). 3.5.1 Exemple Continuons à analyser nos données concernant les bébés à la naissance. Un bébé prématuré est un bébé qui nait avant 37 semaines de grossesse. Dans notre jeu de données Babies, nous pouvons déterminer si un enfant est prématuré ou non (variable binaire) à partir de la variable gestation(en jours). Transformons nos données pour obtenir les variables d’intérêt. SciViews::R babies &lt;- read(&quot;babies&quot;, package = &quot;UsingR&quot;) babies %&gt;.% select(., gestation, smoke, wt1, ht, race, age) %&gt;.% # Eliminer les valeurs manquantes filter(., gestation &lt; 999, smoke &lt; 9, wt1 &lt; 999, ht &lt; 999, race &lt; 99, age &lt; 99) %&gt;.% # Transformer wt1 en kg et ht en cm mutate(., wt1 = wt1 * 0.4536) %&gt;.% mutate(., ht = ht / 39.37) %&gt;.% # Transformer smoke en variable facteur mutate(., smoke = as.factor(smoke)) %&gt;.% # Idem pour race mutate(., race = as.factor(race)) %&gt;.% # Déterminer si un bébé est prématuré ou non (en variable facteur) mutate(., premat = as.factor(as.numeric(gestation &lt; 7*37))) %&gt;.% # Calculer le BMI comme meilleur index d&#39;embonpoint des mères que leur masse mutate(., bmi = wt1 / ht^2) -&gt; Babies_prem Comment se répartissent les enfants entre prématurés et nés à terme ? table(Babies_prem$premat) # # 0 1 # 1080 96 Nous avons un nombre relativement faible de prématurés dans l’ensemble. C’était à prévoir. Attention à un plan très mal balancé ici : c’est défavorable à une bonne analyse, mais pas rédhibitoire. Décrivons ces données. Babies_table &lt;- table(Babies_prem$premat, Babies_prem$smoke) knitr::kable(addmargins(Babies_table)) 0 1 2 3 Sum 0 486 420 83 91 1080 1 39 40 9 8 96 Sum 525 460 92 99 1176 Ce tableau de contingence ne nous donne pas encore l’idée de la répartition de prématurés en fonction de statut de fumeuse de la mère, mais le graphique suivant nous le montre. chart(data = Babies_prem, ~smoke %fill=% premat) + geom_bar(position = &quot;fill&quot;) Il ne semble pas y avoir un effet flagrant, même si le niveau smoke == 2 semble contenir une plus forte proportion de prématurés. Qu’en est-il en fonction de l’éthnicité (voir help(babies, packahge = &quot;UsingR&quot;) pour le détail sur les variétés éthniques considérées) de la mère (variable race) ? Babies_table &lt;- table(Babies_prem$premat, Babies_prem$race) knitr::kable(addmargins(Babies_table)) 0 1 2 3 4 5 6 7 8 9 10 Sum 0 491 42 22 58 50 124 31 192 34 24 12 1080 1 24 2 5 1 6 8 3 41 5 1 0 96 Sum 515 44 27 59 56 132 34 233 39 25 12 1176 chart(data = Babies_prem, ~race %fill=% premat) + geom_bar(position = &quot;fill&quot;) Ici, nous voyons déjà un effet semble-t-il plus marqué. Qu’en est-il du BMI ? chart(data = Babies_prem, bmi ~ premat) + geom_boxplot() + ylab(&quot;BMI de la mère&quot;) Sur ce graphique, il ne semble pas y avoir d’influence du BMI sur le fait d’avoir un enfant prématuré ou non. Enfin, l’âge de la mère influence-t-il également ? chart(data = Babies_prem, age ~ premat) + geom_boxplot() + ylab(&quot;Age de la mère (an)&quot;) Il ne sembnle pas y avoir un effet flagrant. Voyons ce que donne le modèle (nous ne considérons pas les interactions possibles ici, mais cela doit être fait dans le cas de plusieurs effets significatifs au moins). Avec 4 variables explicatives, le modèle est déjà très complexe sans interactions. Nous serions trop ambitieux de vouloir ici ajuster un modèle complet ! # Modèle linéaire généralisé avec fonction de lien de type logit Babies_glm &lt;- glm(data = Babies_prem, premat ~ smoke + race + bmi + age, family = binomial(link = logit)) summary(Babies_glm) # # Call: # glm(formula = premat ~ smoke + race + bmi + age, family = binomial(link = logit), # data = Babies_prem) # # Deviance Residuals: # Min 1Q Median 3Q Max # -0.6875 -0.4559 -0.3228 -0.2857 2.8199 # # Coefficients: # Estimate Std. Error z value Pr(&gt;|z|) # (Intercept) -3.349082 0.860945 -3.890 0.00010 *** # smoke1 0.247161 0.244442 1.011 0.31196 # smoke2 0.247474 0.401007 0.617 0.53715 # smoke3 0.245883 0.417970 0.588 0.55634 # race1 -0.037501 0.754621 -0.050 0.96037 # race2 1.510254 0.539895 2.797 0.00515 ** # race3 -1.017780 1.030541 -0.988 0.32334 # race4 0.891191 0.481411 1.851 0.06414 . # race5 0.303990 0.421363 0.721 0.47064 # race6 0.750868 0.644376 1.165 0.24391 # race7 1.495166 0.280313 5.334 9.61e-08 *** # race8 1.155898 0.531592 2.174 0.02967 * # race9 -0.123997 1.043118 -0.119 0.90538 # race10 -13.513140 691.812473 -0.020 0.98442 # bmi -0.001689 0.032621 -0.052 0.95871 # age 0.007806 0.019119 0.408 0.68307 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # (Dispersion parameter for binomial family taken to be 1) # # Null deviance: 665.00 on 1175 degrees of freedom # Residual deviance: 618.84 on 1160 degrees of freedom # AIC: 650.84 # # Number of Fisher Scoring iterations: 15 Nous voyons que le résumé de l’objet glm est très similaire à celui d’un objet lm, notamment avec un tableau des Coefficients identique et qui s’interprète de la même manière. Ici, nous pouvons confirmer que ni le fait de fumer, ni l’âge, ni le BMI de la mère n’a d’incidence sur les bébés prématurés au seuil alpha de 5%. En revanche, certaines éthnies sont significativement plus susceptibles d’accoucher avant terme. Cela suggère soit un facteur génétique, soit un facteur environnmental/culturel lié à ces éthnies. Naturellement, il faudrait ici simplifier le modèle qui se ramène en fin de compte à l’équivalent d’une ANOVA à un facteur, mais en version glm() une fois que les variables non significatives sont éliminées. De même, on pourrait légitimement se demander si la variable premat ne pourrait pas aussi être modélisée avec une autre fonction de lien en considérant une distribution de Poisson par exemple. A vous de voir… A vous de jouer ! Réalisez un rapport scientifique sur la maturation d’ovocytes, en définissant un modèle linéaire généralisé le plus pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/mXAIu4Ir Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt ovocytes. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. Réalisez un rapport scientifique sur la biométrie humaine Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/hS069etL Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt human. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. "],
["reg-non-lin.html", "Module 4 Régression non linéaire", " Module 4 Régression non linéaire Objectifs Comprendre comment ajuster une courbe dans un nuage de points à l’aide de la régression non linéaire par les moindres carrés Apprendre à réaliser une régression non linéaire dans R, éventuellement en utilisant des modèles “self-start” Comparer les modèles à l’aide du coefficient d’Akaike Connaitre quelques unes des courbes mathématiques les plus utilisées en biologie Prérequis Les modules 1 &amp; 2 du présent cours concernant la régression linéaire sont une entrée en matière indispensable puisque la régression va être abordée ici comme une extension de ce qui a déjà été vu. "],
["rendement-photosynthetique.html", "4.1 Rendement photosynthétique", " 4.1 Rendement photosynthétique Afin d’avoir un premier aperçu de ce qu’est une régression non linéaire par les moindres carrés et comment on la calcule dans R, nous allons résoudre un exemple concret. La posidonie (Posidonia oceanica (L.) Delile (1813)) est une plante à fleur marine qui forme des herbiers denses en mer Méditerranée. Ses feuilles sont particulièrement adaptées à l’utilisation de la lumière qui règne à quelques mètres en dessous de la surface où elle prospère en herbiers denses. (un mémoire portant sur l’étude du diméthylsulfoniopropionate et du diméthylsulfoxyde chez Posidonia oceanica (L.) Delile (1813) a été réalisé au sein du service d’Écologie numérique des Milieux aquatiques). Herbier de posidonies Pour étudier le rendement de sa photosynthèse, c’est-à-dire la part du rayonnement lumineux reçu qui est effectivement utilisée pour initier la chaîne de transport d’électrons au sein de son photosystème II, nous pouvons utiliser un appareil spécialisé : le diving PAM. Diving PAM Cet appareil est capable de déterminer le taux de transfert des électrons (ETR en µmol électrons/m2/s) par l’analyse de la fluorescence réémise par la plante lorsque ses photosites sont excités par une lumière monochromatique pulsée (Walz 2018). Une façon de déterminer la réponse de la plante en rendement photosynthétique en fonction de l’intensité de la lumière reçue est de mesurer successivement l’ETR pour différentes intensités de lumière. En anglais cela s’appelle la “Rapid Light Curve” ou RLC en abbrégé. Comme toutes les longueurs d’ondes lumineuses ne sont pas utilisables par la chlorophylle, l’intensité lumineuse est exprimé dans une unité particulière, le “PAR” ou “Photosynthetically Active Radiation” en µmol photons/m2/s. Une RLC represente donc la variation de l’ERT en fonction des PAR8. Une RLC typique commence par une relation quasi-linéaire aux faibles intensités, pour s’infléchir et atteindre un plateau de rendement maximum. Au delà, si l’intensité lumineuse augmente encore, des phénomènes de photoinhibition apparaissent et le rendement diminue dans une troisième phase. Voici une RLC mesurée à l’aide du diving PAM sur une feuille de P. oceanica. rlc &lt;- tribble( ~etr, ~par, 0.0, 0, 0.5, 2, 3.2, 11, 5.7, 27, 7.4, 50, 8.4, 84, 8.9, 170, 8.4, 265, 7.8, 399 ) rlc &lt;- labelise(rlc, self = FALSE, label = list(etr = &quot;ETR&quot;, par = &quot;PAR&quot;), units = list(etr = &quot;µmol électrons/m^2/s&quot;, par = &quot;µmol photons/m^2/s&quot;) ) chart(data = rlc, etr ~ par) + geom_point() Les trois phases successives sont bien visibles ici (linéaire pour des PAR de 0 à 25, plateau à des PAR de 150-200 et photoinhibition à partir de 200 PAR). Naturellement, une régression linéaire dans ces données n’a pas de sens. Une régression polynomiale d’ordre trois donnerait ceci (code issu du snippet correspondant) : rlc_lm &lt;- lm(data = rlc, etr ~ par + I(par^2) + I(par^3)) rlc_lm %&gt;.% (function(lm, model = lm[[&quot;model&quot;]], vars = names(model)) chart(model, aes_string(x = vars[2], y = vars[1])) + geom_point() + stat_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2) + I(x^3)))(.) La régression polynomiale tente maladroitement de s’ajuster dans les données mais est incapable de retranscrire les trois phases correctement. En particulier, la troisième est incorrecte puisque le modèle semble indiquer une reprise du rendement aux intensités les plus élevées. Une représentation incorrecte est à craindre lorsque le modèle mathématique utilisé ne représente pas les différentes caractéristiques du phénomène étudié. L’utilisation d’un modèle adéquat est possible ici seulement par régression non linéaire. En effet, aucune transformation monotone croissante ou décroissante ne peut linéariser ce type de données puisque la courbe monte d’abord pour s’infléchir ensuite. Quand passer à la régression non linéaire ? Nous pouvons être amenés à utiliser une régression non linéaire pour l’une de ces deux raisons, voire les deux en même temps : Lorsque le nuage de points est curvilinéaire, évidemment, mais après avoir tenté de le linéariser (et de résoudre un problème éventuel d’hétéroscédasticité ou de non-normalité des résidus) par transformation sans succès, En fonction de nos connaissances a priori du phénomène. Tout phénomène issu d’un mécanisme dont nous connaissons le mode de fonctionnement menant à une équation mathématique non linéaire. Cela se rencontre fréquemment en physique, en chimie, et même en biologie (courbes de croissance, effet de modifications environmentales, etc.) Les spécialistes de la photosynthèse ont mis au point différents modèles pour représenter les RLC. (Platt, Gallegos, and Harrison 1980) ont proposé une formulation mathématique des phénomènes mis en jeu ici. Leur équation est la suivante : \\[ETR = ETR_{max} \\cdot (1 - e^{-PAR \\cdot \\alpha/ETR_{max}}) \\cdot e^{-PAR \\cdot \\beta/ETR_{max}}\\] avec \\(PAR\\) la variable indépendante, \\(ETR\\), la variable dépendante, et \\(ETR_{max}\\), \\(\\alpha\\) et \\(\\beta\\), les trois paramètres du modèle. \\(ETR_{max}\\) est le rendement maximum possible, \\(\\alpha\\) est la pente de la partie initiale linéaire avant infléchissement vers le maximum et \\(\\beta\\) est le coefficient de photoinhibition. En matière de régression non linéaire, il est tout aussi important de bien comprendre les propriétés mathématiques de la fonction utilisée que de faire un choix judicieux du modèle. En particulier, il faut s’attacher à bien comprendre la signification (biologique) des paramètres du modèle. Non seulement, cela aide à en définir des valeurs intiales plausibles, mais c’est aussi indispensable pour pouvoir ensuite bien interpréter les résultats obtenus. Nous pouvons facilement créer une fonction dans R qui représente ce modèle : pgh_model &lt;- function(x, etr_max, alpha, beta) etr_max * (1 - exp(-x * alpha/etr_max)) * (exp(-x * beta/etr_max)) Le premier argument de la fonction doit être la variable indépendante (notée de manière générique x, mais n’importe quel nom fait l’affaire ici) et les autres arguments correspondent aux paramètres du modèle, donc etr_max, alpha et beta. Ce modèle peut être ajusté dans R à l’aide de la fonction nls() pour “Nonlinear Least Squares” (regression). Par contre, nous devons fournir une information supplémentaire (l’explication sera détaillée plus loin) : des valeurs approximatives de départ pour les paramètres. Nous voyons sur le graphique que etr_max = 9 est une estimation plausible, mais il est difficile de déterminer alpha et beta rien qu’en regardant le graphique. Nous allons fixer alpha = 1 (rendement max), et partir d’un modèle sans photoinhibition en utilisant beta = 0. Voici comment la régression non linéaire par les moindre carrés avec notre fonction pgh_model peut être calculée : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0)) summary(rlc_nls) # # Formula: etr ~ pgh_model(par, etr_max, alpha, beta) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # etr_max 9.351064 0.261969 35.695 3.22e-08 *** # alpha 0.327385 0.013416 24.402 3.11e-07 *** # beta 0.003981 0.001084 3.673 0.0104 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.1729 on 6 degrees of freedom # # Number of iterations to convergence: 7 # Achieved convergence tolerance: 3.619e-06 La dernière ligne, “achieved convergence” indique que le modèle a pu être calculé. Nous avons un tableau des paramètres qui ressemble très fort à celui de la régression linéaire, y compris les tests t de Student sur chacun des paramètres. Nous obtenons etr_max = 9.4, alpha = 0.33 et beta = 0.0040, mais d’après le test de Student ce dernier paramètre n’est pas significativement différent de zéro9. Pour l’instant, nous allons conserver ce modèle tel quel. Notre modèle paramétré donne donc : \\[ETR = 9.4 \\cdot (1 - e^{-PAR \\cdot 0.33/9.3}) \\cdot e^{-PAR \\cdot 0.0040/9.3}\\] Voyons ce que cela donne sur le graphique. Nous utiliserons pour ce faire une petite astuce qui consiste à transformer l’objet nls obtenu en une fonction utilisable par stat_function() pour le graphique ggplot2 réalisé à l’aide de chart()10. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } Maintenant, nous allons pouvoir ajouter la courbe correspondant à notre modèle comme ceci : chart(data = rlc, etr ~ par) + geom_point() + stat_function(fun = as.function(rlc_nls)) Ce modèle représente bien mieux le phénomène étudié, et il s’ajuste d’ailleurs beaucoup mieux également dans les données que notre polynome. Une comparaison sur base du critère d’Akaïke est également en faveur de ce dernier modèle non linéaire (pour rappel, plus la valeur est faible, mieux c’est) : AIC(rlc_lm, rlc_nls) # df AIC # rlc_lm 5 31.834047 # rlc_nls 4 -1.699078 Super ! Nous venons de réaliser ensemble notre première régression non linéaire par les moindres carrés. Étudions un petit peu plus dans le détail cette technique dans la section suivante. En effet, il est utile de connaitre et comprendre les différents pièges qui peuvent se présenter à nous pour les éviter. Références "],
["principe.html", "4.2 Principe", " 4.2 Principe La régression non linéaire consiste à modéliser la variation d’une variable (dite variable réponse ou dépendante) par rapport à la variation d’une ou plusieurs autres variables (dites explicatives ou indépendantes). Le modèle utilisé pour représenter cette relation est une fonction mathématique de forme quelconque. Ceci constitue une généralisation de la régression linéaire où la fonction mathématique était nécessairement une droite (\\(y = a x + b\\) dans le cas de la régression linéaire simple). La fonction est, dans la technique la plus courante, ajustée en minimisant la somme des carrés des résidus (écart entre les observations \\(y_i\\) et les valeurs prédites par la droite, notées \\(\\hat{y_i}\\)). Lorsque le nuage de point ne s’étire pas le long d’une droite, nous pouvons tenter de transformer les données afin de les linéariser. Malheureusement, il existe de nombreux cas où la relation n’est pas linéarisable et la régression non linéaire est alors notre meilleur choix. Il existe, en réalité, une autre raison pour laquelle nous pourrions être amenés à ne pas transformer les données pour les linéariser. Il s’agit du cas où les résidus ont une distribution correcte avec les données non transformées (distribution normale, et variance homogène -homoscédasticité-) lorsqu’on utilise un modèle non linéaire. Dans ce cas précis, une transformation pour linéariser les données devrait permettre d’utiliser une régression linéaire. Mais ce faisant, on rend alors les résidus non normaux et/ou on perd la propriété d’homoscédasticité, ce qui constitue une violation des conditions d’application de la régression par les moindres carrés que nous utilisons ici. Ainsi, dans ce cas-là, il vaut alors mieux ne pas transformer et utiliser plutôt une régression non linéaire à la place d’une régression linéaire pourtant plus simple d’emploi. 4.2.1 Fonction objective Nous appelons “fonction objective” la fonction qui quantifie la qualité de l’ajustement de sorte que plus le nombre renvoyé par cette fonction est petit, meilleur est l’ajustement. Cette fonction objective peut être définie librement, mais dans de nombreux cas, il s’agit du même critère que pour la régression linéaire par les moindres carrés, à savoir (considérant que la fonction \\(f\\) que nous souhaitons ajuster a \\(k\\) paramètres notés \\(p_1\\), \\(p_2\\), …, \\(p_k\\)) : \\[f_{obj}(p_{1},p_{2},...,p_{k})=\\sum_{i=1}^{n}(y_{i}-f(x_{i,}p_{1},p_{2},...,p_{k}))^{2}=\\sum_{i}(y_{i}-\\hat{{y_{i}}})^{2}\\] 4.2.2 Calcul itératif L’ajustement de notre courbe selon le modèle \\(y = f(x, p_1, p_2, ... p_k) + \\epsilon\\) avec les résidus \\(\\epsilon \\approx N(0, \\sigma)\\) peut se faire de manière itérative, en testant différentes valeurs des paramètres de la fonction, et en retenant au final la combinaison qui minimise le plus la fonction objective. Par exemple, si notre courbe à ajuster est : \\(y = a x^b\\), nous devrons estimer conjointement la valeur des deux paramètres \\(a\\) et \\(b\\) de la courbe. Bien qu’il s’agisse effectivement de la technique employée pour ajuster une courbe dans un nuage de point, nls() utilise ici des algorithmes d’optimisation efficaces pour trouver la solution plus rapidement. Une recherche en aveugle serait très peu efficace évidemment. La description détaillée et les développements mathématiques de ces algorithmes d’optimisation sortent du cadre de ce cours. Nous renvoyons le lecteur intéressé à l’annexe C de (Sen and Srivastava 1990). Les algorithmes utilisés dans la fonction nls() sont : Gauss-Newton, (par défaut), un algorithme utilisant la différentiation de la courbe et une expansion en série de Taylor pour approximer cette courbe par une série de termes additifs dont la solution peut être trouvée par régression linéaire multiple. plinear de Golub-Pereyra, bien que peu répandu, est également implémenté. Il est utile en ce sens qu’il sépare les paramètres en deux sous-groupes : ceux qui sont linéaires dans la fonction (coefficients multiplicateurs de termes additifs) et ceux qui ne le sont pas. La recherche itérative ne se fait que sur les seconds. Les premiers étant estimés par régression linéaire. Ainsi, lorsque la fonction ne comporte que peu de paramètres non linéaires, l’algorithme converge beaucoup plus rapidement. Port est également disponible. Il a la particularité, contrairement aux deux précédents, de permettre de définir des limites supérieures et inférieures acceptables pour chaque paramètres. Cela limite la recherche dans un domaine de validité, lorsque celui-ci peut être défini. Reprenons le calcul de notre RLC pour P. oceanica. L’argument trace = TRUE peut être ajouté à l’appel de nls() pour visionner les différentes étapes du calcul itératif. Voici ce que cela donne : rlc_nls &lt;- nls(data = rlc, etr ~ pgh_model(par, etr_max, alpha, beta), start = list(etr_max = 9, alpha = 1, beta = 0), trace = TRUE) # 24.34024 : 9 1 0 # 4.236559 : 8.4785637209 0.5324097242 -0.0004037391 # 1.141967 : 8.772552037 0.289806296 0.001855493 # 0.1807006 : 9.361592818 0.325563316 0.003954183 # 0.1793698 : 9.353620948 0.327230129 0.003990887 # 0.1793659 : 9.351296953 0.327369993 0.003982181 # 0.1793658 : 9.35108479 0.32738331 0.00398142 # 0.1793658 : 9.351064462 0.327384565 0.003981347 A gauche, nous avons la valeur de la fonction objective \\(f_{obj}\\), et à droite des deux points, la valeur actuelle des trois paramètres dans l’ordre etr_max, alpha et beta. La première ligne indique l’étape 0. \\(f_{obj}\\) vaut 24,3 avec les valeurs par défaut. Dès l’étape suivante (itération #1), cette valeur est divisée par 6 et vaut 4,2. A ce stade, etr_max = 8,5, alpha = 0,29 et beta = 0,0019. Les étapes #2 et #3 font encore significativement diminuer \\(f_{obj}\\), et puis nous grapillons des décimales. Voici une animation de ce processus : La convergence est considérée comme obtenue lorsque la différence de valeur de \\(f_{obj}\\) d’une étape à l’autre tombe en dessous d’un seuil de tolérance que nous nous sommes fixés. C’est l’argument control = de nls() qui en reprend les détails, voir ?nls.control. Les options sont : maxiter = le nombre maximum d’itérations permises. Pour éviter de calculer à l’infini, nous arrêtons à cette valeur et décrétons que le calcul n’a pas convergé. La valeur pas défaut est de 50 itérations. tol = le niveau de tolérance pour décider que l’on a convergé. C’est lui notre seuil de tolérance ci-dessus. Par défaut, il vaut 1e-05, soit 0,00001. C’est une valeur relative. Cela siginifie que la variation de la fonction objective ne doit pas être plus grande que cela en valeur relative, soit \\(|\\frac{fobj_i - fobj_{i + 1}}{fobj_i}| &lt; tol\\). … d’autres arguments moins importants ici. Comme nous pouvons le constater, le seuil de tolérance par défaut est très bas, de sorte que nous n’ayons besoin de le changer que très rarement. 4.2.3 Pièges et difficultés L’ajustement d’un modèle de manière itérative en utilisant un algorithme d’optimisation est une tâche délicate. Dans certains cas, la convergence est lente (il faut beaucoup d’étapes pour arriver au résultat). Dans d’autre cas, le processus s’interrompt à cause d’une singularité de la fonction à ajuster, d’une discontinuité, ou d’une fonction qui n’est pas définie sur une partie du domaine, … Nous pouvons aussi rencontrer des divisions par zéro ou des paramètres qui tendent vers l’infini lors des calculs. Dans ce cas, il faut tenter d’autres valeurs de départ, voire changer le paramétrage de la fonction. Valeurs initiales La recherche de la solution optimale nécessite de partir de valeurs de départ plausibles pour les paramètres du modèle (c’est-à-dire, un ensemble de valeurs pour les paramètres telle que la courbe initiale est proche de la solution recherchée). Définir les valeurs initiales n’est pas toujours chose aisée, et de plus, le résultat final peut dépendre fortement de ces valeurs initiales, à savoir que, si l’on choisi des valeurs initiales trop éloignées de la solution recherchée, on peut très bien se trouver enfermé dans une fausse solution (un minimum local de la fonction objective qui est moins bas que le minimum absolu que l’on recherche). Considérons un cas fictif simple où la fonction à ajuster n’aurait qu’un seul paramètre \\(p\\). Dans ce cas, nous pouvons visualiser la valeur de la fonction objective en fonction de la valeur choisie pour le paramètre \\(p\\). Admettons que nous obtenons le graphique suivant pour cette représentation : Nous observons que la fonction objective a un minimum global pour \\(p\\) valant 0,72. Par contre, un minimum local est également présent pour \\(p\\) valant -0,26. Dans ce cas, si nous prenons une valeur de départ pour \\(p\\) supérieure à 0,25, nous atterrirons dans le minimum global. Mais si nous prenons une valeur plus faible, par exemple 0 ou -0.5 comme valeur de départ, nous nous ferons piéger dans le minimum local. Dans ce cas, nls() déclarera qu’elle a convergé et que la valeur de \\(p\\) vaut -0,26. Si ce n’est toujours pas clair, imaginez que la courbe en rouge représente la forme d’un terrain sur lequel vous placer une balle à la position de départ. En fonction de l’endroit où vous la placer, la balle va glisser le long de la pente et finir par s’immobiliser dans une des deux cuvettes… mais celle de droite (minimum global) est plus profonde que celle de gauche (minimum local). Il vaut toujours mieux tester différentes combinaisons de valeurs de départ pour vérifier si la convergence se fait correctement et pour démasquer les cas de minima locaux. Complexité du modèle Lorsque la convergence a du mal à se faire, ceci est éventuellement lié à la complexité de la fonction que l’on cherche à ajuster. Les propriétés mathématique de la courbe choisie peuvent très bien faire que cette courbe ne soit pas définie pour certaines valeurs des paramètres, ou qu’elle ait un comportement spécial dans un certain domaine (par exemple, courbe tendant vers l’infini). Ces cas sont difficilement traités par la plupart des algorithmes de minimisation de la fonction objective, et des erreurs de calcul de type “division par zéro”, ou “résultat non défini” peuvent apparaître. Dans les meilleures implémentations (nls() en est une), des gardes-fous ont été ajoutés dans le code de la fonction pour éviter les erreurs dans pareils cas. Toutefois, il n’est pas possible de traiter tous les cas possibles. Ainsi, il arrive parfois que le modèle choisi ne soit pas ajustable sur les données. Au final, l’ajustement d’un modèle non linéaire par les moindres carrés est une opération beaucoup plus délicate que l’ajustement par les moindres carrés d’un modèle linéaire. Dans le cas linéaire, la solution est trouvée de manière immédiate grâce à un calcul matriciel simple. Dans le cas non linéaire, il n’existe souvent pas de solution miracle et le meilleur ajustement doit être recherché de manière itérative. Nous verrons ci-dessous que R propose, pour certains modèles appelés ‘SelfStart’ une solution élégante pour calculer automatiquement les valeurs initiales et pour converger très rapidement vers la solution recherchée, mais la plupart du temps il faut tâtonner pour ajuster sa courbe. A vous de jouer ! Réalisez un rapport scientifique sur la vitesse d’une réaction chimique, en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/pa48JVSl Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt speed-reaction. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. 4.2.4 Modèles ‘selfStart’ dans R Dans certains cas, il existe des petites astuces de calcul pour converger directement vers la solution, ou du moins, pour calculer des valeurs de départ très proches de la solution recherchée de sorte que l’algorithme de recherche pourra converger très rapidement. Par exemple, lorsqu’il est possible de linéariser le modèle en transformant les données. Dans ce cas, une bonne estimation des valeurs de départ des paramètres peut être obtenue en linéarisant la relation et en calculant les paramètres par régression linéaire. Ensuite, les paramètres obtenus sont retransformés dans leur forme initiale, et ils sont utilisés comme valeurs de départ. Par exemple, si nous décidons d’ajuster un modèle allométrique de Huxley, de type \\(y = a x^b\\)11, nous pouvons linéariser la relation en transformant les deux variables en logarithmes. En effet, \\(\\log(y) = \\log(a x^b)\\), ce qui est équivalent à une droite en développant: \\(\\log(y) = b \\log(x) + \\log(a)\\). Le paramètre \\(b\\) devient donc la pente de la droite, et \\(\\log(a)\\) devient l’ordonnée à l’origine dans le modèle linéaire transformé. Une fois la droite ajustée, on a directement la valeur de \\(b\\), et il suffit de calculer l’exponentielle de l’ordonnée à l’origine pour obtenir \\(a\\). Toutefois, comme les résidus sont différents dans la relation non transformée initiale, il ne s’agit pas de la solution recherchée mais d’une bon point de départ très proche de cette solution. Ainsi, on prendra les valeurs de \\(a\\) et de \\(b\\) ainsi calculées par la droite en double log comme point de départ, et on laissera l’algorithme de recherche du minimum terminer le travail. En fait, c’est exactement de cette façon que les modèles dits ‘selfStart’ fonctionnent dans R. Plus qu’une fonction, il s’agit en réalité d’un programme complet qui contient : la fonction elle-même, la résolution analytique de la dérivée première de la fonction en chaque point (utilisée par l’algorithme de convergence pour déterminer l’écart à apporter aux paramètres à l’itération suivante), du code optimisé pour choisir les valeurs initiales idéales (proches du minimum global pour la \\(f_{obj}\\)) automatiquement, du code pour optimiser la convergence, éventuellement. Toutes les fonctions ‘SelfStart’ ont un nom commençant par SS. nous pouvons donc les lister à l’aide de l’instruction suivante : apropos(&quot;^SS&quot;) # [1] &quot;SSasymp&quot; &quot;SSasympOff&quot; &quot;SSasympOrig&quot; &quot;SSbiexp&quot; &quot;SSD&quot; # [6] &quot;SSfol&quot; &quot;SSfpl&quot; &quot;SSgompertz&quot; &quot;SSlogis&quot; &quot;SSmicmen&quot; # [11] &quot;SSweibull&quot; Ensuite, nous recherchons l’aide relative à ces fonctions, par exemple via ?SSasymp. Références "],
["modeles-courants-en-biologie.html", "4.3 Modèles courants en biologie", " 4.3 Modèles courants en biologie Les domaines les plus courants où des modèles non linéaires sont utilisés en biologie concernent les cinétiques de réactions (chimiques, biochimiques), les courbes de type dose-réponse, les courbes de survie et les modèles de croissance. Nous verrons principalement divers modèles de croissance dans la section suivante. Certains de ces modèles, comme le modèle exponentiel, celui de Gompertz ou de Weibull sont aussi utilisés comme courbes de survie. De plus, la courbe logistique est un modèle classique de type dose-réponse. Ainsi, les différentes courbes de croissances recouvrent également une majorité des modèles utilisés dans d’autres domaines. 4.3.1 Modèle de Michaelis-Menten La corube de Michaelis-Menten est bien connue pour modéliser des cinétiques chimiques simples, enzymatiques en particulier. Son équation est : \\[V = \\frac{V_{max} \\cdot conc}{K + conc}\\] où \\(conc\\) est la concentration des réactifs au début de la réaction, c’est-à-dire, en absence de produits de cette réaction en mol/L, \\(V\\) est la vitesse de réaction en mol/min. Le modèle a deux paramètres \\(V_{max}\\) la vitesse maximale asymptotique en mol/min et \\(K\\) en mol/L correspondant à la concentration telle que la vitesse est la moitié de \\(V_{max}\\). Dans R, il existe un modèle ‘selfStart’ facile à utiliser pour ajuster une courbe de type Michaelis-Menten. Il s’agit de la fonction SSmicmen(). Voici le graphique d’un modèle Michaelis-Menten avec \\(V_{max} = 1\\) et \\(K = 0,4\\). Le trait horizontal en Vm = 1 représente la vitesse maximale possible (asymptote horizontale du modèle). Nous voyons que cette vitesse maximale n’est atteinte que très lentement ici, et il faudrait que l’axe des X s’étende beaucoup plus sur la droite pour l’observer. micmen_data &lt;- tibble( conc = seq(0, 10, by = 0.1), v = SSmicmen(conc, Vm = 1, K = 0.5) ) chart(data = micmen_data, v ~ conc) + geom_line() + xlab(&quot;Concentration [mol/L]&quot;) + ylab(&quot;Vitesse [mol/min]&quot;) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.5, 1), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0.4, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Vm&quot;, x = -0.4, y = 1) + annotate(&quot;text&quot;, label = &quot;Vm/2&quot;, x = -0.5, y = 0.5) + annotate(&quot;text&quot;, label = &quot;K&quot;, x = 0.5, y = 0.03) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver le meilleur modèle de Michaelis-Menten dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04a_michaelis_menten&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04a_michaelis_menten&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04a_michaelis_menten Concernant les modèles utiles en chimie et biochimie, le modèle à compartiment de premier ordre permet de décrire la cinétique de transformation d’une substance au cours du temps. Voyez le modèle ?Ssfol. Il peut servir par exemple pour déterminer la cinétique d’élimination d’une substance du sang après injection. 4.3.2 Modèles de croissance Parmi les phénomènes biologiques courants qui sont essentiellement non linéaires, les modèles de croissance occupent une part importante. Il s’agit de phénomènes complexes, résultat d’un ensemble de processus, eux-même très complexes : l’anabolisme, ou élaboration de matière organique et le catabolisme qui la détruit pour la transformer en énergie, en ce qui concerne la croissance somatique individuelle. Un modèle typique de croissance individuelle est le modèle de von Bertalanffy (von Bertalanffy, 1938, 1957). Il existe un autre type de modèle de croissance : la croissance des populations. Ce type de modèle décrit l’évolution du nombre d’individus dans une population au cours du temps. Dans ce cas particulier, il s’agit également du résultat de plusieurs processus : la natalité, la mortalité et, éventuellement, les migrations. Une courbe type de modèle de croissance de population est la courbe logistique. Un autre modèle couramment utilisé est celui de Weibull pour décrire un nombre décroissant d’individus suite à une mortalité prédominante dans celle-ci. De nombreux modèles de croissance différents sont disponibles. Certains sont exclusivement des modèles de croissance individuelle, d’autres sont exclusivement des modèles de croissance de populations, mais beaucoup peuvent être utilisés indifféremment dans les deux cas. Tous ont comme particularité d’être l’une ou l’autre forme de modèle exponentiel, exprimant ainsi le fait que la croissance est fondamentalement un processus exponentiel (comprenez que l’augmentation de masse ou du nombre d’individus est proportionnelle à la masse ou au nombre préexistant à chaque incrément temporel). Nous allons décrire ci-dessous quelques un des modèles de croissance principaux. Ensuite, nous les utiliserons pour ajuster une courbe de croissance dans un jeu de donnée réel. 4.3.3 Courbe exponentielle En 1798, Thomas Malthus a décrit un modèle de croissance applicable pour décrire la croissance de la population humaine. Cependant, d’après Murray (1993), ce modèle a été suggéré en premier lieu par Euler. Quoi qu’il en soit, ce modèle n’est plus guère utilisé actuellement, mais son importance historique ne doit pas être négligée. Il s’agit, en effet, de la première modélisation mathématique d’une des caractéristiques les plus fondamentales de la croissance : son caractère exponentiel (positive ou négative). Malthus a observé que la population des Etat-Unis double tous les 25 ans. Il suggère alors que les populations humaines augmentent d’une proportion fixe \\(r\\) sur un intervalle de temps donné, lorsqu’elles ne sont pas affectées de contraintes environnementales ou sociales. Cette proportion \\(r\\) est par ailleurs indépendante de la taille initiale de la population : \\[y_{t+1} = (1+r) \\ y_t = k \\ y_t\\] Une forme continue du modèle précédent (intervalle de temps infinitésimal) donne une équation différentielle : \\[\\frac{d y(t)}{dt} = y&#39;(t) = k \\ y(t)\\] Cette équation différentielle admet la solution suivante : \\[y(t) = y_0 \\ e^{k \\ t}\\] avec \\(y_0\\), la taille initiale de la population au temps \\(t = 0\\). Ce modèle à deux paramètres est également intéressant parce qu’il montre une bonne manière de construire un modèle de croissance. Il suffit de décrire la croissance pour un accroissement infinitésimal de temps par le biais d’une équation différentielle, et ensuite de la résoudre (modélisation dynamique). Presque tous les modèles de croissance existants ont été élaborés de cette manière. Ainsi, la quasi-totalité des modèles de croissance correspondent en fait à une équation différentielle relativement simple. Dans R, nous pourrons utiliser la fonction suivante : exponent &lt;- function(x, y0, k) y0 * exp(k * x) Voici un exemple d’un modèle de croissance exponentiel avec \\(y_0 = 1,5\\) et \\(k = 0,9\\). Le paramètre \\(y_0\\) est indiqué sur le graphique. # Graphique avec y0 = 1.5 et k = 0.9 exponent_data &lt;- tibble( t = seq(0, 3, by = 0.1), y = exponent(t, y0 = 1.5, k = 0.9) ) chart(data = exponent_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.05, y = 1.5) Il existe aussi un modèle bi-exponentiel qui combine deux signaux exponentiels différents. Pour plus de détails, voyez ?SSbiexp. A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04b_exponent&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04b_exponent&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04b_exponent 4.3.4 Courbe logistique Le modèle exponentiel décrit une croissance infinie sans aucunes contraintes. Ce n’est pas une hypothèse réaliste. En pratique, la croissance est limitée par les ressources disponibles. Verhulst (1838), en travaillant aussi sur la croissance de populations, propose un modèle qui contient un terme d’auto-limitation \\([y_\\infty – y (t)] / y_\\infty\\) qui représente une quelconque limite théorique des ressources disponibles : \\[\\frac{dy(t)}{dt} = k \\ \\frac{y_\\infty - y(t)}{y_\\infty} \\ y(t) = - \\frac{k}{y_\\infty} \\ y(t)^2 + k \\ y(t)\\] Lorsque l’on résout et simplifie cette équation différentielle, on obtient : \\[y(t) = \\frac{y_\\infty}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est une des formes de la courbe logistique. Cette fonction a deux asymptotes horizontales en \\(y(t) = 0\\) et \\(y(t) = y_\\infty\\) (voir schéma ci-dessous) et c’est une sigmoïde symétrique autour du point d’inflexion (les deux courbes du S sont identiques). Le modèle ‘selfStart’ correspondant dans R s’appelle SSlogis(). Ses paramètres sont Asym (= \\(y_\\infty\\)), xmid (= \\(t_0\\)), et scal (= \\(k\\)). Voici le graphique d’une courbe logistique avec \\(y_\\infty = 0,95\\), \\(t_0 = 5\\) et \\(k = 1\\). logis_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSlogis(t, Asym = 0.95, xmid = 5, scal = 1) ) chart(data = logis_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/2, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/2&quot;, x = -0.5, y = 0.95/2) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.03) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6, y = 0.45) Cette courbe sigmoïdale est asymptotique en 0 et \\(y_\\infty\\), et elle est également symétrique autour de son point d’inflexion situé à \\({t_0, y_\\infty / 2}\\). A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04c_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04c_logistique&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04c_logistique Il est possible de généraliser ce modèle en définissant une courbe logistique dont l’asymptote basse peut se situer n’importe où ailleurs qu’en 0. Si cette asymptote se situe en \\(y_0\\), nous obtenons l’équation : \\[y(t) = y_0 + \\frac{y_\\infty - y_0}{1 + e^{-k \\ (t - t_0)}}\\] Ceci est le modèle logistique généralisé à quatre paramètres (modèle ‘selfSart’ SSfpl() dans R, pour four-parameters logistic). Les arguments sont A, la première asymptote horizontale (= \\(y_0\\)), B, la seconde asymptote horizontale (= \\(y_\\infty\\)), xmid (= \\(t_0\\)) et scal (= \\(k\\)). Le graphique ressemble très fort à celui de la fonction logistique, mais la première asymptote n’est plus nécessairement à 0 (ici, \\(y_0\\) = 0,15). fpl_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSfpl(t, A = 0.15, B = 0.95, xmid = 5, scal = 1) ) chart(data = fpl_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.15, 0.8/2 + 0.15, 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 5, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;A&quot;, x = -0.4, y = 0.15) + annotate(&quot;text&quot;, label = &quot;B&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;xmid&quot;, x = 5.4, y = 0.13) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 6.1, y = 0.53) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04d_logistique&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04d_logistique_gen&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04d_logistique_gen 4.3.5 Modèle de Gompertz Gompertz (1825) a observé de manière empirique que le taux de survie décroît souvent de manière proportionnelle au logarithme du nombre d’animaux qui survivent. Bien que ce modèle reste utilisé pour décrire des courbes de survie, elle trouve de nombreuses applications pour décrire également des données de croissance. L’équation différentielle du modèle de Gompertz est : \\[\\frac{dy(t)}{dt} = k \\ [ \\ln y_\\infty - \\ln y(t)] \\ y(t)\\] qui se résout et se simplifie en : \\[y(t) = y_\\infty \\ e^{-k \\ (t - t_0)} = y_\\infty \\ e^{-a \\cdot b^t} = y_\\infty \\ a^{e^{-k \\ t}} = y_\\infty \\ a^{b^t}\\] La dernière forme apparaît plus simple et est le plus souvent utilisée. La première forme est directement dérivée de l’équation différentielle et donne une meilleure comparaison avec la courbe logistique, puisque \\(t_0\\) correspond aussi à l’abscisse du point d’inflexion, qui n’est plus en position symétrique ici (voir figure ci-dessous). Le modèle ‘selfStart’ correspondant dans R s’appelle SSgompertz(). Sa paramétriqation correspond à la seconde forme, mais avec \\(a\\) appelé b2 et \\(b\\) appelé b3 (\\(y(t) = Asym \\ e^{-b2 \\cdot b3^t}\\)). Voici le graphique d’une courbe de Gompertz avec \\(y_\\infty = 0,95\\), \\(a = 5\\) et \\(b = 0,5\\). gomp_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSgompertz(t, Asym = 0.95, b2 = 5, b3 = 0.5) ) chart(data = gomp_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0, 0.95/exp(1), 0.95), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 2.3, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;Asym/e&quot;, x = -0.5, y = 0.95/exp(1)) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion&quot;, x = 3.5, y = 0.32) A vous de jouer ! Ajuste les pramètres du modèles afin de trouver la meilleure courbe exponentielle dans une application interactive “shiny”. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir l’application : BioDataScience2::app(&quot;04e_gompertz&quot;) # TODO Méthode alternative : shiny::runApp(system.file(&quot;shiny/04e_gompertz&quot;, package = &quot;BioDataScience2&quot;)) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R lorsque vous aurez fini avec l’application shiny. 04e_gompertz 4.3.6 Modèles de von Bertalanffy Le modèle de von Bertalanffy, parfois appelé Brody-Bertalanffy (d’après les travaux de von Bertalanffy et Brody) ou modèle de Pütter (dans Ricker, 1979 notamment), est la première courbe de croissance qui a été élaborée spécifiquement pour décrire la croissance somatique individuelle. Il est basé sur une analyse bioénergétique simple. Un individu est vu comme un simple réacteur biochimique dynamique où les entrées (anabolisme) sont en compétition avec les sorties (catabolisme). Le résultat de ces deux flux étant la croissance. L’anabolisme est plus ou moins proportionnel à la respiration et la respiration est proportionnelle à une surface pour beaucoup d’animaux (la surface développée des poumons ou des branchies), soit encore, les 2/3 de la masse. Le catabolisme est toujours proportionnel à la masse. Ces relations mécanistiques sont rassemblées dans l’équation différentielle suivante où \\(y(t)\\) mesure l’évolution d’un volume ou d’un poids au cours du temps : \\[\\frac{dy(t)}{dt} = a \\ y(t)^{2/3} - b \\ y(t)\\] En résolvant cette équation, nous obtenons le modèle de croissance pondérale de von Bertalanffy : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^3\\] La forme la plus simple de ce modèle est obtenue lorsque nous mesurons des dimensions linéaires pour quantifier la taille de l’organisme, car une dimension linéaire est, en première approximation, la racine cubique d’une masse, proportionnelle au volume à densité constante (sans prendre en compte une possible allométrie). Le modèle de von Bertalanffy pour des mesures linéaires est alors simplement  : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})\\] Un graphique des deux modèles est présenté ci-dessous. Le modèle de von Bertalanffy pour mesures linéaire n’a pas de point d’inflexion. La croissance est la plus rapide à la naissance et ne fait que diminuer avec le temps pour finalement atteindre zéro lorsque la taille maximale asymptotique est atteinte. Avec ce modèle, la croissance est donc déterminée et elle ne peut dépasser cette asymptote horizontale située en \\(y(t)= y_\\infty\\). A cause de la puissance cubique de la forme pondérale du modèle von Bertalanffy, cette dernière est une sigmoïde asymétrique, comme l’est le modèle de Gompertz également. Trois modèles ‘selfStart’ existent dans R : SSasympOff(), SSasymp() et SSasympOrig(). SSAsympOff() est définie comme \\(y(t) = y_\\infty \\ (1 - e^{-e^{lrc} \\ (t - t_0)})\\). Ici \\(k\\) est remplacé par \\(e^{lrc}\\). Cette astuce permet d’avoir un paramètre \\(k\\) qui ne prend pas de valeur négatives, puisque l’exponentielle d’une valeur négative est un nombre compris entre 0 et 1. Donc, un \\(lrc\\) négatif donne un \\(k\\) compris entre 0 et 1. Cela permet de contraindre un paramètre du modèle sans nécessité de passer obligatoirement par l’algorithme “Port”. A noter finalement que \\(y_\\infty\\) s’appelle Asym dans tous les modèles ‘selfStart’ dans R, y compris ici, et que \\(t_0\\) s’appelle ici c0. SSAsymp() est définie comme \\(y(t) = y_\\infty + (R_0 - y_\\infty) \\ e^{-e^{lrc} \\cdot t}\\). Par rapport à la forme habituelle, outre l’astuce de \\(e^{lrc}\\) à la place de \\(k\\), \\(t_0\\) est également reparamétré en \\(R_0\\) qui représente la taille initiale au temps \\(t = 0\\). Cela mets l’accent sur cette “taille à la naissance”. Reparamétriser un modèle consiste à exprimer la fonction mathématique qui le représente d’une façon différente. Etant donné que l’interprétation biologique des paramètres fait partie des objectifs de la méthode. On parle d’approche mécanistique, qui vise à décrypter le mécanisme sous-jacent versus une approche purement empirique basée sur les données uniquement avec un modèle polynomial tout venant, par exemple. Ainsi, L’équation du modèle de von Bertalanffy présentée au début montre une paramétrisation “classique”, implémentée dans SSasympOff() alors que SSasymp() mets la taille initiale en évidence via le paramètre \\(R_0\\). SSasympOrig() force le modèle à passer par l’origine des axes {0, 0}. Il n’y a donc plus que deux paramètres Asym = \\(y_\\infty\\) et lrc tel que \\(k = e^{lrc}\\). Ce modèle simplifié est souvent utile en pratique. Il a l’avantage d’être simple, avec seulement deux paramètres. Il est l’équivalent d’une droite forcée à zéro pour le modèle von Bertalanffy. Si R0 dans SSasymp() ou c0 dans SSasympOff() ne sont pas significativement différents de zéro (test t de Student dans le tableau des paramètres), vous pouvez envisager de simplifier le modèle vers SSasympOrig(). Le modèle von Bertalanffy en poids n’est pas implémenté, nous devons utiliser une fonction personnalisée dans ce cas (avec une paramétrisation similaire à SSasympOff() : asympOff3 &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^3 Voici les deux modèles de von Bertalanffy présentés sur le même graphique avec \\(y_\\infty\\) (alias Asym) = 0,95, lrc = 0,1 et \\(t_0\\) (alias c0) = 0. vb_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSasympOff(t, Asym = 0.95, lrc = 0.1, c0 = 0), y3 = asympOff3(t, Asym = 0.95, lrc = 0.1, c0 = 0), ) chart(data = vb_data, y ~ t %col=% &quot;VB en taille&quot;) + geom_line() + geom_line(f_aes(y3 ~ t %col=% &quot;VB en poids&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Modèle&quot;) Dans les deux cas l’asymptote représentant la taille maximale possible vaut 0,95. Pour le modèle pondéral, c’est le cube de la valeur obtenue avec SSasympOff() appliquée sur la racine cubique des masses. Dans les deux cas, lrc = 0,1 donc \\(k = e^{0,1} = 1,1\\) et c0 = 0, la droite passe par l’origine (donc, nous aurions également pu utiliser SSasympOrig() pour générer ces données. 4.3.7 Modèle de Richards La forme généralisée du modèle de von Bertalanffy est : \\[y(t) = y_\\infty \\ (1 - e^{-k \\ (t - t_0)})^m\\] Von Bertalanffy (1938, 1957) a fixé \\(m\\) à 1 ou à 3. Richards (1959) permet à \\(m\\) de varier librement, et donc son modèle a un paramètre de plus. Cette dernière courbe est très flexible (voir schéma ci-dessous) et il est possible de démontrer que plusieurs autres modèles de croissance ne sont que des cas particuliers de ce modèle généraliste avec différentes valeurs de \\(m\\). Nous avons déjà observé que le modèle de Richards se réduit aux deux modèles de von Bertalanffy quand \\(m = 1\\) et \\(m = 3\\). Il se réduit aussi à une courbe logistique lorsque \\(m = -1\\) et il est possible de montrer qu’il converge vers le modèle de Gompertz lorsque \\(|m| \\rightarrow \\infty\\). Il n’existe aucun modèle ‘selfStart’ pour la courbe de Richards dans R. Par ailleurs, il s’agit d’un modèle particulièrement délicat à ajuster, comme nous le verrons dans un exemple concret plus loin dans la section “choix du modèle”. Avec une paramétrisation proche de celle de SSasympOff(), la fonction de Rcichards peut s’écrire comme ceci : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Voici l’allure de différentes courbes de Richards en fonction de la valeur de \\(m\\) (0,5, 1, 3, 6 et 9) avec \\(lrc = 0,1\\), \\(y_\\infty = 0,95\\) et \\(t_0 = 0\\) pour toutes les courbes. rich_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 1), y05 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 0.5), y3 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 3), y6 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 6), y9 = richards(t, Asym = 0.95, lrc = 0.1, c0 = 0, m = 9) ) chart(data = rich_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y3 ~ t %col=% &quot;m = 3&quot;)) + geom_line(f_aes(y6 ~ t %col=% &quot;m = 6&quot;)) + geom_line(f_aes(y9 ~ t %col=% &quot;m = 9&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + labs(color = &quot;Richards avec :&quot;) 4.3.8 Modèle de Weibull Depuis son introduction en 1951 par Weibull, ce modèle est présenté comme polyvalent. Il a été décrit à l’origine comme une distribution statistique. Il trouve de nombreuses applications en croissance de population (éventuellement négative), et il est utilisé également pour décrire la courbe de survie en cas de maladie ou dans des études de dynamique de populations. Il a parfois été utilisé comme un modèle de croissance. La forme la plus générale de ce modèle est  : \\[y(t) = y_\\infty - d \\ e^{-k \\ t^m}\\] avec \\(d = y_\\infty - y_0\\). Un modèle à trois paramètres est également utilisé où \\(y_0 = 0\\). La fonction est sigmoïdale lorsque \\(m &gt; 1\\), sinon elle ne possède pas de point d’inflexion (voir graphique ci-dessous). Dans R, le modèle ‘selfStart’ s’appelle SSweibull(). Comme pour le modèle von Bertalanffy, le paramètre \\(k\\) est contraint à une valeur positive via l’astuce \\(k = e^{lrc}\\). Comme d’habitude, \\(y_\\infty\\) se nomme Asym. \\(d\\) est ici appelé Drop et \\(m\\) est appelé pwr. Voici l’allure de différentes courbes de Weibull pour respectivement \\(m\\) (alias pwr) = 5, 2, 1 et 0,5, avec \\(lrc = -0,5\\) (donc, \\(k = e^{-0,5}\\) = 0,61), \\(y_\\infty\\) (alias Asym) = 0,95 et \\(y_0= 0,05\\), ce qui donne \\(d\\) (alias Drop) = 0,95 - 0,05 = 0,9. La courbe avec \\(m = 1\\) est équivalente à un modèle de von Bertalanffy linéaire. Toutes les courbes démarrent en \\(y_0\\) et passent par \\(y_\\infty - d \\ e^{-k}\\) qui est également le point d’inflexion pour les sigmoïdes lorsque \\(m &gt; 1\\). weib_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 1), y05 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 0.5), y2 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 2), y5 = SSweibull(t, Asym = 0.95, Drop = 0.9, lrc = -0.5, pwr = 5) ) chart(data = weib_data, y ~ t %col=% &quot;m = 1&quot;) + geom_line() + geom_line(f_aes(y05 ~ t %col=% &quot;m = 0.5&quot;)) + geom_line(f_aes(y2 ~ t %col=% &quot;m = 2&quot;)) + geom_line(f_aes(y5 ~ t %col=% &quot;m = 5&quot;)) + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = c(0.05, 0.95, 0.95 - 0.9 * exp(-exp(-0.5))), col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) + annotate(&quot;text&quot;, label = &quot;y0&quot;, x = -0.4, y = 0.05) + annotate(&quot;text&quot;, label = &quot;Asym-d*e^-k&quot;, x = 0, y = 0.95 - 0.9 * exp(-exp(-0.5))) + annotate(&quot;text&quot;, label = &quot;point d&#39;inflexion si m &gt; 1&quot;, x = 3, y = 0.43) + labs(color = &quot;Weibull avec :&quot;) 4.3.9 Modèle Preece-Baines 1 Preece et Baines (1978) ont décrit plusieurs modèles spécifiques à la croissance humaine. Ces modèles combinent deux phases de croissance exponentielle pour représenter la croissance graduelle d’enfants suivie par une courte phase de croissance accélérée à l’adolescence, mais qui atteint rapidement un plateau correspondant à la taille adulte définitive (voir graphique ci-dessous). Ce type de modèle est naturellement très utile pour tous les mammifères, mais certains, comme le modèle 1 présenté ici, ont aussi été utilisés dans d’autres circonstances, profitant de sa grande flexibilité. Son équation est : \\[y(t) = y_\\infty - \\frac{2 (y_\\infty - d)}{e^{k1 \\ (t - t_0)} + e^{k2 \\ (t - t_0)}}\\] Dans R, la fonction à utiliser (avec une paramétrisation similaire à celle des autres modèles ‘selfStart’) est : preece_baines1 &lt;- function(x, Asym, Drop, lrc1, lrc2, c0) Asym - (2 * (Asym - Drop)) / (exp(exp(lrc1) * (x - c0)) + exp(exp(lrc2) * (x - c0))) \\(k1\\) et \\(k2\\) sont forcés à des valeurs positives ou nulles grace à l’astuce de passer par lrc1 et lrc2. \\(d\\) est Drop et \\(t_0\\) est c0. Ci-dessous un exemple de courbe Preece-Baines 1 avec \\(y_\\infty\\) (alias Asym) = 0,95, \\(d\\) (alias Drop) = 0,8, \\(k1 = 0,19\\) (alias lrc1 = log(0,19) = -1,7), \\(k2 = 2,5\\) (alias lrc2= log(2,5) = 0,92) et \\(t_0\\) (alias c0) = 6. pb1_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = preece_baines1(t, Asym = 0.95, Drop = 0.8, lrc1 = -1.7, lrc2 = 0.92, c0 = 6) ) chart(data = pb1_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) + geom_hline(yintercept = 0.95, col = &quot;gray&quot;, linetype = &quot;dashed&quot;) + annotate(&quot;text&quot;, label = &quot;Asym&quot;, x = -0.4, y = 0.95) 4.3.10 Modèle de Tanaka Tous les modèles précédents sont asymptotiques, à l’exception de la courbe exponentielle (mais cette dernière ne modélise valablement que la phase de croissance initiale). Tous ces modèles décrivent donc une croissance déterminée qui n’excédera jamais une taille maximale représentée par une asymptote horizontale en \\(y(t) = y_\\infty\\). Knight (1968) s’est demandé s’il s’agit d’une réalité biologique ou simplement d’un artefact mathématique. Dans le second cas, la croissance n’apparaîtrait déterminée que parce que les modèles choisis pour la représenter sont asymptotiques. Pour s’affranchir d’une telle contrainte, Tanaka (1982, 1988) a élaboré un nouveau modèle de croissance qui décrit une croissance non déterminée : \\[y(t) = \\frac{1}{\\sqrt{b}} \\ \\ln |2 \\ b \\ (t - t_0) + 2 \\ \\sqrt{b^2 \\ (t - t_0)^2 + a \\ b}| + d\\] Ce modèle complexe à quatre paramètres a une période initiale de croissance lente, suivie d’une période de croissance exponentielle qui se poursuit par une croissance continue mais plus faible tout au long de la vie de l’animal (voir graphique ci-dessous). Dans R, nous pouvons utiliser la fonction suivante pour ajuster un modèle de Tanaka : tanaka &lt;- function(x, a, b, c0, d) 1 / sqrt(b) * log(abs(2 * b * (x - c0) + 2 * sqrt(b^2 * (x - c0)^2 + a * b))) + d Ci-dessous, un exemple de courbe de Tanaka avec \\(a\\) = 3, \\(b\\) = 2,5, \\(d\\) = -0,2 et \\(t_0\\) (alias c0) = 2. tanaka_data &lt;- tibble( t = seq(0, 10, by = 0.1), y = tanaka(t, a = 3, b = 2.5, c0 = 2, d = -0.2) ) chart(data = tanaka_data, y ~ t) + geom_line() + geom_vline(xintercept = 0, col = &quot;darkgray&quot;) "],
["choix-du-modele.html", "4.4 Choix du modèle", " 4.4 Choix du modèle Le choix d’un modèle non linéaire fait intervenir des critères identiques à ceux d’un modèle linéaire (qualité d’ajustement évaluée par l’AIC, inspection visuelle de l’ajustement dans le nuage de points), mais il fait aussi intervenir une dimension supplémentaire : le choix de la fonction marthématique à ajuster. Comme nous venons de le voir au travers de quelques modèles courants en biologie, le nombre de fonctions mathématiques résultant en des formes similaire, par exemple de type sigmoïde, est grand. Ainsi, le choix de la meilleure fonction à utiliser dans un cas particulier est rendu plus difficile. Nous allons illustrer l’ajustement d’une courbe non linéaire par le choix et l’ajustement d’un modèle de croissance dans un jeu de données en modélisant la croissance somatique de l’oursin Paracentrotus lividus (Grosjean 2001). Ce jeu de données est disponible dans le package data.io, sous le nom urchin_growth. SciViews::R urchins &lt;- read(&quot;urchin_growth&quot;, package = &quot;data.io&quot;, lang = &quot;fr&quot;) chart(data = urchins, diameter ~ age) + geom_point() Comme vous pouvez le voir, différents oursins ont été mesurés via le diamètre à l’ambitus du test (zone la plus large) en mm à différents âges (en années). Les mesures ont été effectuées tous les 3 à 6 mois pendant plus de 10 ans, ce qui donne un bon aperçu de la croissance de cet animal y compris la taille maximale asymptotique qui est atteinte vers les 4 à 5 ans (pour ce genre de modèle, il est très important de continuer à mesurer les animaux afin de bien quantifier cette taille maximale asymptotique). Ainsi, l’examen du graphique nous permet d’emblée de choisir un modèle à croissance finie (pas le modèle de Tanaka, donc), et de forme sigmoïdale. Les modèles logistique, Weibull ou Gompertz pourraient convenir par exemple. Nous pouvons à ce stade, essayer différents modèles et choisir celui qui nous semble le plus adapté. Le choix du meilleur modèle se fait grâce à deux critères : Les connaissances théoriques et a priori du modèle que l’on ajuste. En effet, il n’existe qu’un seul modèle linéaire, mais une infinité de modèles curvilinéaires qui peuvent s’ajuster dans les données. Le choix du meilleur modèle se fait en fonction de considérations sur le phénomène sous-jacent qui doivent se refléter dans les propriétés mathématiques de la courbe choisie. Par exemple, si on sait que la croissance est asymptotique vers une taille maximale, nous devrons choisir une courbe mathématique qui présente une asymptote horizontale à son maximum pour représente au mieux le phénomène étudié. Le coefficient de détermination \\(R^2\\) n’est pas calculé par R pour une régression non linéaire car sa validité est sujette à discussion entre les statisticiens (d’autres logiciels statistiques le calculent). Nous n’avons donc pas d’estimation de la qualité de l’ajustement par ce biais, comme dans le cas de la régression linéaire. Par contre, il est possible de calculer un autre critère plus fiable que nous avons déjà utilisé : le critère d’Akaïke (fonction AIC() dans R). Ce critère tient compte à la fois de la qualité d’ajustement et de la complexité du modèle, exprimée par le nombre de paramètres qu’il faut estimer. Plus le modèle est complexe, plus on peut s’attendre à ce qu’il s’ajuste bien aux données car il est plus flexible. Cependant, en ce domaine, la complexité n’est pas forcément un gage de qualité. On recherche plutôt un compromis entre meilleur ajustement et simplicité. Le critère d’information d’Akaiké quantifie précisément ce compromis, c’est-à-dire que le modèle qui a un AIC le plus faible est considéré comme le meilleur. Appliquons donc ce concept pour sélectionner le meilleur modèle de croissance pour décrire la croissance somatique de nos oursins après avoir sélectionné les modèles candidats les plus judicieux (modèle sigmoïdal avec asymptote horizontale au maximum). Notons toutefois que, comme les animaux sont mesurés aux mêmes âges, tous les 3 à 6 mois, certains points se superposent. Afin d’afficher tous les points, il est utile d’utiliser la fonction geom_jitter() à la place de geom_point() qui décale les points d’une valeur aléatoire pour éviter ces superpositions (l’argument width = indique le décalage maximum à appliquer). Voici ce que cela donne (en ajoutant également un titre avec formattage correct du nom en latin)  : urchins_plot &lt;- chart(data = urchins, diameter ~ age) + geom_jitter(width = 0.1, alpha = 0.2, color = &quot;darkgrey&quot;) + ggtitle(expression(paste(&quot;Croissance de l&#39;oursin &quot;, italic(&quot;Paracentrotus lividus&quot;)))) urchins_plot Nous avons ici également représenté les points de manière semi-transparente avec alpha = 0.2(transparence de 20%) pour encore mieux mettre en évidence les points de mesures qui se superposent. Ajustons maintenant un modèle de Gompertz (modèle ‘SelfStart’) : urchins_gomp &lt;- nls(data = urchins, diameter ~ SSgompertz(age, Asym, b2, b3)) summary(urchins_gomp) # # Formula: diameter ~ SSgompertz(age, Asym, b2, b3) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 57.403687 0.257588 222.85 &lt;2e-16 *** # b2 3.901916 0.046354 84.18 &lt;2e-16 *** # b3 0.434744 0.003852 112.86 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.49 on 7021 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.655e-06 Utilisons maintenant notre fonction as.function.nls() pour ajouter la courbe sur le graphique. as.function.nls &lt;- function(x, ...) { nls_model &lt;- x name_x &lt;- names(nls_model$dataClasses) stopifnot(length(name_x) == 1) function(x) predict(nls_model, newdata = structure(list(x), names = name_x)) } A présent, nous pouvons faire ceci : urchins_plot + stat_function(fun = as.function(urchins_gomp), color = &quot;red&quot;, size = 1) L’ajustement de cette fonction semble très bon, à l’oeil. Voyons ce qu’il en est d’autres modèles. Par exemple, une courbe logistique : urchins_logis &lt;- nls(data = urchins, diameter ~ SSlogis(age, Asym, xmid, scal)) summary(urchins_logis) # # Formula: diameter ~ SSlogis(age, Asym, xmid, scal) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 54.628069 0.202985 269.1 &lt;2e-16 *** # xmid 2.055285 0.009568 214.8 &lt;2e-16 *** # scal 0.764830 0.007355 104.0 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.6 on 7021 degrees of freedom # # Number of iterations to convergence: 4 # Achieved convergence tolerance: 1.079e-06 Et voici le graphique avec les deux modèles superposés : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + labs(color = &quot;Modèle&quot;) Notez que ici, la couleur a été incluse dans le “mapping” (argument mapping =) de stat_function() en l’incluant dans aes(). Cela change fondamentalement la façon dont la couleur est perçue par ggplot2. Dans ce cas-ci, la valeur est interprétée non comme une couleur à proprement parler, mais comme un niveau (une couche) à inclure dans le graphique et à reporter via une légende. Ensuite, à l’aide de labs() on change le titre de la légende relatif à la couleur par un nom plus explicite : “Modèle”. Nous pouvons comparer ces modèles à l’aide du critère d’Akaïke. AIC(urchins_gomp, urchins_logis) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 Comme on peut le voir clairement sur le graphe, la courbe logistique donne une autre solution, cette dernière est à peine moins bonne que le modèle de Gompertz, selon le critère d’Akaiké. Pourtant, la courbe est assez bien démarquée de celle de Gompertz. Essayons maintenant un modèle de Weibull. Ce modèle est plus complexe car il a quatre paramètres au lieu de trois pour les deux modèles précédents : urchins_weib &lt;- nls(data = urchins, diameter ~ SSweibull(age, Asym, Drop, lrc, pwr)) summary(urchins_weib) # # Formula: diameter ~ SSweibull(age, Asym, Drop, lrc, pwr) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 56.80491 0.32704 173.69 &lt;2e-16 *** # Drop 56.81320 0.59393 95.66 &lt;2e-16 *** # lrc -1.57392 0.02865 -54.94 &lt;2e-16 *** # pwr 1.67880 0.02960 56.72 &lt;2e-16 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.47 on 7020 degrees of freedom # # Number of iterations to convergence: 3 # Achieved convergence tolerance: 2.532e-06 Ajoutons ce nouveau modèle sur le graphique : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 1) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 1) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 1) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 Ce modèle fait presque jeu égal avec le modèle de Gompertz en terme de critère d’Akaiké ; juste un tout petit peu mieux. En fait, les deux courbes sont pratiquement superposées l’une à l’autre, mais le modèle de Weibull à un démarrage de croissance plus lent au début, ce qui se reflète dans les données. Par contre, il est pénalisé par le fait que c’est un modèle plus complexe qui possède un paramètre de plus. L’un dans l’autre, le critère d’information d’Akaiké considère donc les deux modèles sur pratiquement sur le même plan du point de vue de la qualité de leurs ajustements respectifs. A ce stade, nous voudrions également essayer un autre modèle flexible à quatre paramètres : le modèle de Richards. Malheureusement, il n’existe pas de fonction ‘SelfStart’ dans R pour ce modèle. Nous sommes donc réduit “à mettre les mains dans le cambouis”, à définir la fonction nous même, à trouver de bonnes valeurs de départ, etc. Voici comment définir la fonction : richards &lt;- function(x, Asym, lrc, c0, m) Asym*(1 - exp(-exp(lrc) * (x - c0)))^m Pour les valeurs de départ, là ce n’est pas facile. Asym est l’ asymptote horizontale à la taille maximum. On voit qu’elle se situe aux environ de 55 mm sur le graphique. Pour les autres paramètres, c’est plus difficile à évaluer. Prenons par exemple 1 comme valeur de départ pour les trois autres paramètres, ce qui donne (les valeurs de départ sont obligatoires ici puisque ce n’est pas un modèle ‘SelfStart’) : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = 0.1, c0 = 1, m = 1)) # Error in numericDeriv(form[[3L]], names(ind), env): Missing value or an infinity produced when evaluating the model … et voilà ! Un excellent exemple de plantage de l’algorithme de minimisation de la fonction objective suite à un comportement inadéquat de la fonction avec les valeurs testées. Ici, la fonction renvoie l’infini et l’algorithme ne peut donc effectuer la minimisation. La fonction de Richards est effectivement connue pour être difficile à ajuster pour cette raison. Il nous faut donc soit tester d’autres valeurs de départ, soit utiliser un autre algorithme de minimisation, soit les deux. Après différents essais il apparaît que le changement des valeurs de départ suffit dans le cas présent : urchins_rich &lt;- nls(data = urchins, diameter ~ richards(age, Asym, lrc, c0, m), start = c(Asym = 55, lrc = -0.7, c0 = 0, m = 1)) summary(urchins_rich) # # Formula: diameter ~ richards(age, Asym, lrc, c0, m) # # Parameters: # Estimate Std. Error t value Pr(&gt;|t|) # Asym 58.14348 0.37772 153.934 &lt; 2e-16 *** # lrc -0.27595 0.03152 -8.754 &lt; 2e-16 *** # c0 -0.87545 0.28464 -3.076 0.002109 ** # m 6.20711 1.82345 3.404 0.000668 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 5.487 on 7020 degrees of freedom # # Number of iterations to convergence: 11 # Achieved convergence tolerance: 7.11e-06 Ajoutons ce dernière modèle sur notre graphique (avec des traits un peu plus fins pour mieux distinguer les modèles les uns des autres) : urchins_plot + stat_function(fun = as.function(urchins_gomp), aes(color = &quot;Gompertz&quot;), size = 0.7) + stat_function(fun = as.function(urchins_logis), aes(color = &quot;logistique&quot;), size = 0.7) + stat_function(fun = as.function(urchins_weib), aes(color = &quot;Weibull&quot;), size = 0.7) + stat_function(fun = as.function(urchins_rich), aes(color = &quot;Richards&quot;), size = 0.7) + labs(color = &quot;Modèle&quot;) … et comparons à l’aide du critère d’Akaïke : AIC(urchins_gomp, urchins_logis, urchins_weib, urchins_rich) # df AIC # urchins_gomp 4 43861.73 # urchins_logis 4 44139.13 # urchins_weib 5 43810.72 # urchins_rich 5 43854.53 La courbe est très proche des modèles de Gompertz et Weibull aux jeunes âges, mais l’asymptote maximale est légèrement plus haute que pour les deux autres modèles (58 mm au lieu de 57 mm). Les trois courbes sont très, très proches l’une de l’autre. Le critère d’information d’Akaiké est marginalement moins bon pour le modèle de Richards que pour celui de Weibull, mais est tout juste meilleur que celui de Gompertz. En outre l’écart type pour le paramètre x0 est plus conséquent en comparaison de sa valeur, ce qui démontre une certaine instabilité de la fonction par rapport à ce paramètre, et par conséquent, une incertitude dans son estimation. Pour cette raison, det pour la difficulté à l’ajuster, le modèle de Richards sera écarté dans notre cas au benefice du modèle de Weibull, voire de celui de Gompertz plus simple. Le choix final entre Gompertz ou Weibull dépend de l’usage que l’on veut faire du modèle. Si la simplicité du modèle est primordiale, nous garderons Gompertz. Si la croissance des petits oursins est un aspect important de l’analyse, nous garderons Weibull qui semble mieux s’ajuster aux données à ce niveau. A vous de jouer ! Réalisez un cahier de laboratoire sur la croissance de bactérie en définissant un modèle non linéaire pertinent pour ces données. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/kSLMNWpw Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt bacterial_growth. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. Références "],
["hierarchique.html", "Module 5 Classification hiérarchique", " Module 5 Classification hiérarchique Objectifs Comprendre la notion de distance et la matrice de distance. Appréhender la classification hiérarchique et le dendrogramme. Être capable d’effectuer un regroupement pertinent des individus d’un jeu de données multivarié à l’aide de ces techniques. Prérequis Vous devez être à l’aise avec l’utilisation de R et Rstudio, en particulier pour l’importation, le remaniement et la visualisation de données multivariées. Ceci correspond au cours SDD I. Il n’est pas nécessaire d’avoir acquis toutes les notions vue dans la partie Cours II :modélisation pour pouvoir comprendre cette seconde partie du cours. Si vous ne vous sentez pas assez à l’aise avec R et RStudio, c’est peut-être le bon moment pour refaire le premier “learnr” du package BioDataScience2 : Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;01a_rappel&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. "],
["analyse-de-donnees.html", "5.1 Analyse de données", " 5.1 Analyse de données L’analyse de données (on dit aussi analyse exploratoire des données, EAD ou statistiques exploratoires) mets en œuvre des méthodes statistiques multivariées visant à découvrir de l’information pertinente dans un gros jeu de données via des approches multidimensionnelles et essentiellement descriptives. Ces méthodes se regroupent en deux grandes familles : Celles visant à réduire la dimensionnalité (travailler avec des tableaux ayant moins de colonnes). Elles permettent ensuite de présenter les données de manière synthétique pour observer des relations entre les variables ou les individus via des représentations graphiques. Nous aborderons ces techniques dans les modules suivants. Celles cherchant à classifier (ou regrouper) les individus. Il s’agit ici de synthétiser le gros tableau de données dans l’autre sens, selon les lignes. L’approche via la classification hiérarchique sera détaillée ici. La vidéo suivante introduit l’EAD (jusqu’à 2:11) : "],
["distance-entre-individus.html", "5.2 Distance entre individus", " 5.2 Distance entre individus Vous êtes en train d’analyser des données concernant les échantillons de plancton que vous avez prélevé sur votre lieu de recherche. Ce plancton a été numérisé (photo de chaque organisme) et les images ont été traitée avec un logiciel qui mesure automatiquement une vingtaine de variables telles que la surface de l’objet sur l’image, son périmètre, sa longueur, … Vous vous trouvez donc face à un jeu de données qui a une taille non négligeable : 20 colonnes par 1262 lignes, soit le nombre d’individus mesurés dans vos échantillons. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Vous voulez regrouper votre plancton en fonction de la ressemblance entre les organismes, c’est-à-dire, en fonction des écarts entre les mesures effectuées pour les 19 variables quantitatives, à l’exclusion de la vingtième colonne class qui est une variable factor). En raison de la taille du tableau, il est évident que cela ne pourra pas se faire de manière manuelle. Nous pouvons raisonnablement considérer que plus les mesures sont similaires entre deux individus, plus ils ont des chances d’être semblables, c’est-à-dire, d’appartenir au même groupe taxonomique. Mais comment faire pour synthétiser l’information de similarité ou différence contenue dans 19 paires de valeurs (une paire par variable)  ? Nous avons besoin d’une mesure de distance qui quantifie la similarité (ou à l’inverse la dissimilarité) en un seul nombre. Celle qui vient naturellement à l’esprit est la distance euclidienne. Prenons un cas simplifié. Quelle est la distance qui sépare deux individus A et B par rapport à trois variables x, y, z ? Ici, nous pouvons représenter l’information graphiquement dans un espace à trois dimensions. La distance qui nous intéresse est la distance linéaire entre les deux points dans l’espace. Autrement dit, c’est la longueur du segment de droite qui relie les deux points dans l’espace. Cette distance, nous pouvons la calculer à l’aide de la formule suivante (problème de niveau lycée, voir par exemple ici pour une résolution dans le plan) : \\[\\mathrm{D_{Euclidean}}_{A, B} = \\sqrt{(x_A - x_B)^2 + (y_A - y_B)^2 + (z_A - z_B)^2}\\] Notez que cette formule se généralise à n dimensions et s’écrit alors, pour n’importe quelle paire d’individus indicés j et k dans notre tableau et pour les différentes mesures de 1 à i notées yi : \\[\\mathrm{D_{Euclidean}}_{j, k} = \\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] C’est la racine carré de la somme dans les n dimensions des écarts entre les valeurs au carré pour toutes les variables yi. Plus sa valeur est grande, plus les individus sont éloignés (différents). Pour cette raison, nous appelerons cette distance, une mesure de dissimilarité. 5.2.1 Matrice de distances Nous avons maintenant la possibilité de quantifier la similitude entre nos organismes plactoniques… mais nous en avons un grand nombre. Cela va être impossible à gérer autant de mesures qu’il y a de paires possibles parmi 1262 individus12. La matrice de distance est une matrice ici 1262 par 1262 qui rassemble toutes les valeurs possibles. Notez que sur la diagonale, nous comparons chaque individu avec lui-même. La distance euclidienne vaut donc systématiquement zéro sur la diagonale. \\[\\mathrm{D_{Euclidean}}_{j, j} = 0\\] De plus, de part et d’autre de cette diagonale, nous trouvons les paires complémentaires (j versus k d’un côté et k versus j de l’autre). Or qu’elle soit mesurée dans un sens ou dans l’autre, la distance du segment de droite qui relie deux points dans l’espace est toujours la même. \\[\\mathrm{D_{Euclidean}}_{j, k} = \\mathrm{D_{Euclidean}}_{k, j}\\] Par conséquent, seulement une portion (soit le triangle inférieur, soit le triangle supérieur hors diagonale) est informative. La diagonale ne porte aucune informartion utile, et l’autre triangle est redondant. Nous avons donc pour habitude de ne calculer et représenter que le triangle inférieur de cette matrice. Maintenant que cela est clair, nous pouvons créer un objet dist qui contiendra notre matrice de distances enclidiennes. Il suffit d’utiliser la fonction dist(), ou mieux vegan::vegdist() qui offre plus de possibilités13. Comme cela prendrait trop de place d’imprimer la matrice complète, nous allons réaliser le travail sur seulement les six premiers individus de notre tableau (et nous devons aussi éliminer la colonne class qui ne contient pas de données numériques et qui ne nous intéresse pas pour le moment) : zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class head(., n = 6) -&gt; zoo6 # Récupération des 6 premiers individus zoo6_dist &lt;- vegan::vegdist(zoo6, method = &quot;euclidean&quot;) zoo6_dist # En pratique, on n&#39;imprime généralement ce genre d&#39;objet ! # 1 2 3 4 5 # 2 8.2185826 # 3 2.5649705 5.7320911 # 4 0.8582142 7.8813537 2.2220079 # 5 4.8478629 4.2950067 3.0119468 4.6148173 # 6 2.4269520 10.6197317 4.9228255 2.8477882 7.1766853 Nous voyons bien ici que R n’imprime que le triangle inférieur de notre matrice 6 par 6. Notez aussi que les objets dist de tailles plus réalistes que vous génèrerez dans vos analyses ne sont prévue pour être imprimées et visualisées telles quelles. Il s’agit seulement de la première étape vers une représentation utile qui sera réalisée à la page suivante, à l’aide de la classification hiérarchisée. Félicitations ! Vous venez de calculer votre première matrice de distances. Nous verrons à la page suivante comment nous pouvons utiliser l’information qu’elle contient pour regrouper les individus de manière pertinente. Mais avant cela, nous avons besoin d’un peu de théorie pour bien comprendre quelle métrique choisir pour calculer nos distances et pourquoi. On parle aussi d’indices de similarité ou dissimilarité. Attention : nous n’avons pas considéré ici les unités respectives de nos variables. Une surface (mm2) ou une longeur (mm) ne sont pas mesurées dans les mêmes unités. Nous risquons alors de donner plus de poids aux valeurs élevées. Nous aurions le même effet si nous décidions par exemple d’exprimer une mesure longitudinale en µm au leiu de l’exprimer en mm. Dans ce cas, il vaut mieux standardiser d’abord le tableau (moyenne de zéro et écart type de un) selon les colonnes avant d’effectuer le calcul. Ceci sera fait à la page suivante. 5.2.2 Indices de (dis)similarité Un indice de similarité (similarity index en anglais) est une descripteur statistique (nombre unique) de la similitude de deux échantillons ou individus représentés par plusieurs variables dans un échantillon multivarié. Un indice de similarité prend une valeur comprise entre 0 (différence totale) et 1 ou 100% (similitude totale). Un indice de dissimilarité} est le complément d’un indice de similarité (dis = 1 – sim); sa valeur est comprise entre 100% (différence totale) et 0 (similitude totale). Attention : dans certains cas, un indice de dissimilarité peut varier de 0 à +\\(\\infty\\)**. Il n’existe alors pas d’in,dice de similarité complémentaire. C’est le cas précisément de la distance euclidienne que nous avons exploré jusqu’ici. Tous les indices de similarité / dissimilarité peuvent servir à construire des matrices de distances. 5.2.2.1 Indice de Bray-Curtis L’indice de dissimilarité de Bary-Curtis, aussi appelé coefficient de Czecanowski est calculé comme suit : \\[\\mathrm{D_{Bray-Curtis}}_{j,k}=\\frac{\\sum_{i=1}^{n}\\left|y_{ij}-y_{ik}\\right|}{\\sum_{i=1}^{n}(y_{ij}+y_{ik})}\\] Dans R nous utiliserons vegan::vegdist(DF, method = &quot;bray&quot;).Il s’utilise pour mesurer la similitude entre échantillon sur base du dénombrement d’espèces. Si le nombre d’individus est très variable (espèces dominantes versus espèces rares), nous devons transformer les données pour éviter de donner trop de poids aux espèces les plus abondantes (ex: \\(log(x+1)\\), double racine carrée, …). Une caractéristique essentielle de cet indice (contrairement à la distance euclidienne) est que toute double absence n’est pas prise en compte dans le calcul. C’est souvent pertinent dans le cadre de son utilisation comme le dénombrement d’espèces. En effet, quelle information utile retire-t-on de doubles zéros dans un tableau répertoriant la faune belge pour le crocodile du Nil et le tigre de Sibérie par exemple ? Aucune ! Ils sont tous deux systématiquement absents des dénombrements, mais cette double absence n’apporte aucune information utile pour caractériser la faune belge par ailleurs. L’indices de similarité de Bray-Curtis (sim) est complémentaire à l’indices de dssimilarité correspondant (dis tel que calculé ci-dessus) : \\[sim = 1 – dis\\] 5.2.2.2 Indice de Canberra L’indice de dissimilarité de Canberra est similaire à l’indice de Bray-Curtis mais il pondère les espèces en fonction du nombre d’occurrences afin de donner le même poids à chacune dans le calcul. Il se calcule comme suit : \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{\\left|y_{i&#39;j}\\right|+\\left|y_{i&#39;k}\\right|}\\] où \\(nz\\) est le nombre de valeurs non nulles simultanément dans le tableau de départ. Toutes les espèces contribuent ici de manière égale. C’est un point positif, mais il faut faire attention à ce que cet indice a souvent tendnace à donner une surimportance aux espècces très rares observées une seule fois ou un petit nombre de fois ! Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;canberra&quot;). Toute double absence n’est pas prise en compte ici aussi. Seuls les indices ne dépendant pas des doubles zéros sont utilisables pour des dénombrements d’espèces ou des présence-absence. Ainsi pour ce type de données, notre choix se portera sur : Bray-Curtis si l’on souhaite que le résultat soit dominé par les espèces les plus abondantes. Canberra si notre souhait est de donner la même importance à toutes les espèces, mais avec un risque de domination des espèces rares. Bray-Curtis sur données transformées (\\(log(x+1)\\) ou double racine carrée) pour un compromis entre les deux avec prise en compte de toutes les espèces, mais domination partielle des espèces les plus abondantes. C’est souvent un bon compromis. Attention : Si les volumes échantillonnés entre stations ne sont pas comparables, il faut standardiser (moyenne nulle et écart type un) les données selon les échantillons avant de faire les calculs de distances. De même que pour Bray-Curtis, l’indice de similarité sim se calcule à partir de l’indice de dissimilarité dis tel que ci-dessus comme \\(sim = 1 - dis\\). 5.2.2.3 Distance Euclidienne Nous savons déjà que c’est la distance géométrique entre les points dans un espace à n dimensions : \\[\\mathrm{D_{Euclidean}}_{j,k}=\\sqrt{\\sum_{i=1}^{n}(y_{ij}-y_{ik})^2}\\] Dans R, cette distance peut être calculée avec dist(DF) ou vegan::vegdist(DF, method = &quot;euclidean&quot;). Cet indice de dissimilarité est utile pour des mesures quantitatives, pour des données environnmentales, etc. Il faut que les mesures soient toutes effectuées dans les mêmes unités. Si ce n’est pas le cas, penser alors à standardiser les mesures avant le calcul comme nous l’avons fait plus dans l’exemple sur le zooplancton. Il n’existe pas d’indice de similarité complémentaire. 5.2.2.4 Distance de Manhattan La distance de Manhattan, encore appelée “city-block distance” est un indice de dissimilarité qui, contrairement à la distance euclidienne ne mesure pas la distance géométrique entre les points en ligne droite, mais via un trajet qui suit les parallèmes aux axes. C’est comme si la distance euclidenne reliait les points à vol d’oiseau, alors qu’avec la distance de Manhattan, on devait contourner les blocs de maisons du quartier pour aller d’un point A à un point B (d’où le nom de cette métrique). Elle se calcule comme suit : \\[\\mathrm{D_{Manhattan}}_{j,k}=\\sum_{i=1}^{n}|y_{ij}-y_{ik}|\\] Dans R, nous utiliserons vegan::vegdist(DF, method = &quot;manhattan&quot;). Ici aussi, seul l’indice de dissimilarité est défini. L’indice de similarité complémentaire n’existe pas car la valeur de l’indice de dissimlarité n’est pas borné à droite et peut varier de zéro (dissimilarité nulle, les deux individus soint identiques) à l’infini pour une différence maximale. 5.2.3 Utilisation des indices Les distances euclidienne ou de Manhattan sont à préférer pour les mesures environnementales ou de manière générale pour les variables quantitatives continues. Les distances de Bray-Curtis ou Canberra sont meilleure pour les dénombrements d’espèces (nombreux double zéro), ou de manière générale, pour les variables quantitatives discrètes prenant des valeurs nulles ou positives. 5.2.4 Propriétés des indices Les indices varient en 0 et 1 (0 et 100%), mais les distances sont utilisées aussi comme indices de dissimilarité et varient entre 0 et \\(+\\infty\\). Un indice est dit métrique si : Minimum 0 : \\(I_{j, k} = 0\\) si \\(j = k\\) Positif : \\(I_{j, k}&gt;0\\) si \\(j \\neq k\\) Symétrique : \\(I_{j, k}=I_{k, j}\\) Inégalité triangulaire : \\(I_{j, k} + I_{k, l} &gt;= I_{j, l}\\) La dernière propriété d’inégalité triangulaire est la plus difficile à obtenir, et n’est pas toujours nécessaire. Nous pouvons montrer que certains indices qui ne respectent pas cette dernière propriété sont pourtant utiles dans le contexte. Nous dirons alors d’un indice que c’est une semi-métrique s’il répond à toutes les conditions sauf la quatrième. Enfin, un indice est dit non métrique dans tous les autres cas. Le tableau suivant reprend les métriques que nous avons vu jusqu’ici, et rajoute d’autres candidats potentiels (la distance Chi carré, l’indice de correlation ou de variance/covariance) en indiquant leur type : Distance Type Bray-Curtis semi-métrique Canberra métrique Euclidienne métrique Manhattan métrique Chi carré métrique (correlation) (non métrique) (variance/covariance) (non métrique) Pour en savoir plus Vous pouver aussi transposer le tableau pour calculer la distance entre les variables en utilisant la fonction t() dans R (dist(t(DF))). Par exemple, dans le cas d’un tableau “espèces - station” (dénombrement d’espèces en différentes stations), nous pouvons souhaiter comparer les stations du point de vue de la composition en espèces, mais nous pouvons aussi comparer les espèces du point de vue de leur répartition entre les stations. Pour passer d’un calcul à l’autre, nous transposerons donc le tableau (les colonnes deviennent les lignes et inversément) avant d’utiliser dist() ou vegan::vegdist(). Pour bien comprendre la logique derrière les indices, il est utile de comprendre les équations correspondantes. Si ces équations sont pour vous trop abstraites, une façon efficace de comprendre consiste à faire le calcul à la main. Par exemple dans le cas de l’indice de Canberra, la notion de nombre de données non nulles \\(nz\\) n’est pas évident. Effectuons un calcul à la main détaillé sur le tableau fictif suivant concernant trois espèces A, B, et C dénombrées en trois stations Sta1, Sta2 et Sta3 dans le tableau nommé ex1 : A B C Sta1 4 0 2 Sta2 3 0 10 Sta3 1 8 0 Pour rappel, la dissimilarité de Canberra se calcule comme suit: \\[\\mathrm{D_{Canberra}}_{j,k}=\\frac{1}{nz}\\sum_{i&#39;=1}^{nz}\\frac{\\left|y_{i&#39;j}-y_{i&#39;k}\\right|}{y_{i&#39;j}+y_{i&#39;k}}\\] où: nz est le nombre d’observations non nulles simultanément dans les deux vecteurs comparés (les doubles zéros ne sont pas pris en compte) i’ est l’itérateur sur toutes les valeurs non double zéros Voici le détail du calcul (notez bien comment le double zéro pour l’espèce B entre les stations 1 et 2 est pris en compte dans le calcul) : Sta1_2 &lt;- (1/2) * ((abs(4 - 3)) / (4 + 3) + (abs(2 - 10)) / (2 + 10)) round(Sta1_2, 2) # [1] 0.4 Sta1_3 &lt;- (1/3) * (abs(4 - 1) / (4 + 1) + abs(0 - 8) / (0 + 8) + abs(2 - 0) / (2 + 0)) round(Sta1_3, 2) # [1] 0.87 Sta2_3 &lt;- (1/3) * (abs(3 - 1) / (3 + 1) + abs(0 - 8) / (0 + 8) + abs(10 - 0) / (10 + 0)) round(Sta2_3, 2) # [1] 0.83 La matrice finale est la suivante : # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Vérifions en laissant R faire le calcul : vegan::vegdist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 0.40 # Sta3 0.87 0.83 Attention ! dist(, method = &quot;canberra&quot;) ne multiplie pas la somme par \\(1/nz\\), mais par \\(n/nz\\) (variante). Ainsi, nous obtenons des distances trois fois plus grandes ici avec dist() qu’avec vegan::vegdist(), mais les deux calculs restent valables car les écarts relatifs au sein de la matrice de distance sont les mêmes. dist(ex1, method = &quot;canberra&quot;) %&gt;.% round(., 2) # Sta1 Sta2 # Sta2 1.21 # Sta3 2.60 2.50 A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension des matrices de distance. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;05a_distance_matrix&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Réalisez un carnet de note par binôme sur le transect entre Nice et Calvi. Lisez attentivement le README Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/g/R45egn-p Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt spatial_distribution_zooplankton_ligurian_sea. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. Le nombre de paires uniques et distinctes (pas j, j ou k, k) possibles parmi n items est \\(n(n-1)/2\\), soit ici pour 1262 éléments nous avons 795.691 paires.↩ La fonction dist() ne propose par exemple pas la méthode de Bray-Curtis qui est fréquemment utilisée en biologie et en écologie, contrairement à vegan::vegdist(method = &quot;bray&quot;).↩ "],
["regroupement-avec-cah.html", "5.3 Regroupement avec CAH", " 5.3 Regroupement avec CAH A partir du moment où nous avons une matrice de distances entre toutes les paires d’individus dans notre jeu de données, nous pouvons les regrouper en fonction de leur ressemblance. Deux approches radicalement différentes existent. Soit nous pouvons diviser l’échantillon progressivement jusqu’à obtenir des groupes homogènes (méthodes divisives, telle que les K-moyennes que nous aborderons dans le module suivant), soit nous pouvons regrouper les items semblables petit à petit jusqu’à avoir traité tout l’échantillon (méthodes agglomératives, telle que la classification ascendante hiérarchique étudiée ici). 5.3.1 Dendrogramme Le dendrogramme est une manière très utile de représenter graphiquement un regroupement. Il s’agit de réaliser un arbre dichotomique ressemblant un peu à un arbre phylogénétique qui est familier aux biologistes (par exemple ici) : Dans l’arbre, nous représentons des divisions dichotomiques (division d’une branche en deux) matérialisant les divergences au cours de l’évolution. Dans l’arbre présenté ici, l’axe des ordonnées est utilisé pour représenter le temps et les branchements sont placées à une hauteur correspondante en face de l’axe. Le dendrogramme est une représentation similaire de la CAH avec l’axe des ordonnées indiquant à quelle distance le rassemblement se fait. Dans R, la réalisation du dendrogramme se fait en deux étapes : Son calcul par CAH à l’aide de la fonction hclust() Sa représentation sur un graphique en utilisant plot() (pour un graphique de base) ou ggdendro::ggdendrogram() (pour un graphique ggplot2). Partons, pour étayer notre raisonnement, d’une matrice de distances euclidiennes sur les données de zooplancton. Au passage, abordons quelques fonctions de R utiles dans le contexte pour préparer correctement nos données. A la section précédente, nous avons suggéré qu’il peut être utile de standardiser nos données préalablement si une distance de type euclidienne ou manhattan est ensuite calculée et si les données numériques sont mesurées dans des unités différentes, comme c’est le cas ici. La fonction scale() se charge de cette standardisation colonne par colonne dans un tableau. Comme elle renvoie une matrice, nous devons ensuite retransformer le résultat en data.frame ou tibble. Nous choisissons ici d’utiliser la fonction as_tibble(). Limitons, pour l’instant notre ambition à la comparaison de six individus. Afin d’observer tous les cas possibles dans le dendrogramme, nous ne prendrons pas les six premières lignes du tableau, mais les lignes 13 à 18. Cela peut se faire à l’aide de la fonction slice() que nous n’avons pas encore beaucoup utilisée jusqu’ici. Cette fonction permet de spécifier explicitement les numéros de lignes à conserver, contrairement à filter() qui applique un test de condition pour décider qulles ligne(s) converser. Voici donc notre matrice de distances euclidiennes sur les données ainsi traitées. Les individus initiaux 13 à 18 sont renumérotés 1 à 6. Nous n’imprimons plus ici la matrice de distance obtenue car ce n’est que la première étape du travail vers une représentation plus utile (le dendrogramme). zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Conversion de la matrice en data.frame +tibble slice(., 13:18) -&gt; zoo6 # Récupération des lignes 13 à 18 zoo6 %&gt;.% vegan::vegdist(., method = &quot;euclidean&quot;) -&gt; zoo6std_dist Notre objet zoo6std_dist est ensuite utilisé pour calculer notre CAH à l’aide de hclust(). Enfin, plot() (ou ggdendro::ggdendrogram()) se charge de tracer le dendrogramme. zoo6std_dist %&gt;.% hclust(.) -&gt; zoo6std_clust # Calcul du dendrogramme plot(zoo6std_clust) # ggdendro::ggdendrogram() peut aussi être utilisé ici Voici comment interpréter ce graphique. Les deux individus les plus semblables sont le 2 et le 5 (regroupepent effectué le plus bas, donc, avec la valeur de l’indice de dissimilarité le plus faible), également tous deux similaires au 3. Le 4 se rattache à ce groupe, mais bien plus haut sur l’axe, indiquant ainsi qu’il s’en différencie un peu plus, enfin, le 1 et le 6 sont rassemblés à un niveau à peu près équivalent. Finalement, le groupe de droite constitué des individus 2, 3, 4 et 5 et celui de gauche contenant le 1 et le 6 sont reliés encore plus haut suggérant ainsi la dissimilitude la plus forte entre ces deux groupes. Attention : La position des individus selon l’axe horizontal n’est pas importante. Il faut voir le dendrogramme comme un mobile qui peut tourner librement. C’est-à-dire que le groupe constitué de 2, 3, 4 et 5 aurait très bien pu être placé à la gauche de celui constitué de 1 et 6. De même, le sous-groupe 2, 3 et 5 aurait très bien pu être à la gauche du regroupement avec 4 à la droite, etc. Notez que nous n’avons pas utilisé l’information concernant les classes taxonomiques auxquelles les individus appartiennent (nous avons éliminé la variable class en tout début d’analyse). Pour cette raison, ce type de regroupement s’appelle une classification non supervisée parce que nous n’imposons pas les groupes que nous souhaitons réaliser14. Nous pouvons néanmoins révéler ces classes maintenant pour vérifier si le dendrogramme réalisé est logique (étape bien sûr facultative et souvent pas possible en pratique lorsque cette information n’est pas disponible) : zoo$class[13:18] # [1] Egg_round Poecilostomatoid Poecilostomatoid Decapod # [5] Calanoid Appendicularian # 17 Levels: Annelid Appendicularian Calanoid Chaetognath ... Protist Les individus 2, 3 et 5 (“Poecilostomatoid” ou “Calanoid”) sont des copépodes, des crustacés particulièrement abondants dans le zooplancton. Leur forme est similaire. Leur regroupement est logique. L’individu 4 est une larve de décapode, un autre crustacé. Ainsi le regroupement de 2, 3 et 5 avec 4 correspond aux crustacés ici. Enfin, les individus 1 et 6 sont très différents puisqu’il s’agit respectivement d’un œuf rond probablement de poisson et d’un appendiculaire. 5.3.2 Séparer les groupes Si notre dendrogramme est satisfaisant (nous le déterminons en l’étudiant et en vérifiant que le regroupement obtenu a un sens biologique par rapport à l’objectif de notre étude), nous concrétisons le regroupement en coupant l’arbre à une certaine hauteur à l’aide de la fonction cutree(). Nous pouvons matérialiser ce niveau de coupure en traçant un trait horizontal rouge avec abline(h = XXX, col = &quot;red&quot;) pour le graphe de base, ou en ajoutant + geom_hline(yintercept = XXX, col = &quot;red&quot;) au graphique ggplot2, avec ‘XXX’ la hauteur de coupure souhaitée. Par exemple, si nous voulons réaliser deux groupes, nous ferons : plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) Pour réaliser trois groupes, nous couperons plus bas (ici version ggplot2) : ggdendro::ggdendrogram(zoo6std_clust) + geom_hline(yintercept = 6.5, col = &quot;red&quot;) Et ainsi de suite. A mesure que la hauteur de coupure descend, nous réaliserons quatre, puis cinq groupes. Quel est le meilleur niveau de coupure ? Il n’y en a pas un seul forcément car les différents niveaux correspondent à un point de vue de plus en plus détaillé du regroupement. Néanmoins, lorsque le saut sur l’axe d’un regroupement à l’autre est fort, nous pouvons considérer une séparation des groupes d’autant meilleure. Cela crédibilise d’autant le regroupement choisi. Ainsi la séparation en deux groupes apparait forte (entre 10,2 et 7,4, sur un intervalle de 2,8 unités donc). De même, la séparation de l’individu 4 par rapport au groupe constitué de 2, 3 et 5 se fait sur un intervalle de 4,7 unités (entre 7,4 et 2,7). En comparaison, la séparation de l’individu 3 du groupe 2 et 5 est nettement moins nette puisqu’elle apparait aux hauteurs entre 2,7 à 2.4, soit seulement sur un intervalle de 0,3 unités. Ces mesures d’intervalles n’ont aucune valeur absolue. Il faut les considérer uniquement de manière relatives les unes par rapport aux autres, et seulement au sein d’un même dendrogramme. Ici, deux niveaux de coupure se détachent. Nous utilisons aussi la fonction rect.hclust() pour matérialiser ces groupes sur le dendrogramme, et cutree() pour obtenir une nouvelle variable que nous pourrons ajouter dans notre tableau de données qui concrétise le regroupement choisi : en deux groupes, nous avons les crustacés séparés des autres. plot(zoo6std_clust) abline(h = 8, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 8, border = c(&quot;blue&quot;, &quot;red&quot;)) (group2 &lt;- cutree(zoo6std_clust, h = 8)) # [1] 1 2 2 2 2 1 Les individus 1 et 6 sont dans le groupe 1, et les autres dans le groupe 2. en quatre groupes, nous avons les copépodes séparés des trois autres items chacun dans un groupe séparé. plot(zoo6std_clust, hang = -1) # Prolonger les tiges jusqu&#39;en bas abline(h = 5, col = &quot;red&quot;) rect.hclust(zoo6std_clust, h = 5, border = c(&quot;blue&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;)) Le groupe des individus 2, 3 et 5 est encadré en rouge. Les trois autres groupes des individus uniques 1, 6 et 4 respectivement sont ici encadrés en bleu. (group4 &lt;- cutree(zoo6std_clust, h = 5)) # [1] 1 2 2 3 2 4 L’individu 1 est dans le groupe 1, les individus 2, 3 et 5 sont dans le groupe 2, l’individu 4 est dans le groupe 3 et enfin l’individu 6 est dans le groupe 4. Donc, cutree() numérote ses groupes en fonction du premier item présent dedans dans l’ordre du tableau de données. Nous pouvons rajouter ces regroupements dans le tableau de données, et utiliser cette information pour en faire d’autres choses utiles. Par exemple, nous représentons un nuage de point des données et colorons nos points en fonction du premier regroupement effectué comme suit : zoo6$group2 &lt;- as.factor(group2) # Utiliser une variable facteur ici chart(zoo6, area ~ circularity %col=% group2) + geom_point() Nous voyons que les copépodes (groupe 2 en turquoise) sont plus grands (area) et moins circulaires (circularity) que les deux autres particules du groupe 1 en rouge. Nous pouvons ainsi réexplorer nos données en fonction du regroupement pour mieux le comprendre. 5.3.2.1 Méthodes de CAH Tant que l’on compare des individus isolés entre eux, il n’y a pas d’ambiguïté. Par contre, dès que nous comparons un groupe avec un individu isolé, ou deux groupes entre eux, nous avons plusieurs stratégies possible pour calculer leurs distances (argument method = de hclust()) : Liens simples (single linkages en anglais) hclust(DIST, method = &quot;single&quot;) : la distance entre les plus proches voisins au sein des groupes est utilisée. zoo6std_dist %&gt;.% hclust(., method = &quot;single&quot;) %&gt;.% plot(.) Les données tendent à être aggrégées de proche en proche en montrant une gradation d’une extrême à l’autre. Dans le cas d’une variation progressive, par exemple lors d’un passage graduel d’une situation A vers une situation B le long d’un transect, le dendrogramme obtenu à l’aide de cette méthode représentera la situation au mieux. Liens complets (complete linkages) hclust(DIST, method = &quot;complete&quot;), méthode utilisée par défault si non précisée : la distance entre les plus lointains voisins est considérée. C’est le dendrogramme que nous avons obtenu au début. Le dendrogramme a tendance à effectuer des groupes séparés plus nettement les uns des autres qu’avec les liens simples. Liens moyens (group-average) hclust(DIST, method = &quot;average&quot;) encore appelée méthode UPGMA : moyenne des liens entre toutes les paires possibles intergroupes. Nous obtenons une situation intermédiaire entre liens simples et liens moyens. zoo6std_dist %&gt;.% hclust(., method = &quot;average&quot;) %&gt;.% plot(.) Dans ce cas-ci, le résultat est similaire aux liens simples, mais ce n’est pas forcément le cas à chaque fois. Méthode de Ward hclust(DIST, method = &quot;ward.D2&quot;). Considérant le partitionnement de la variance totale du nuage de points (on parle aussi de l’inertie du nuage de points) entre variance interclasse et variance intraclasse, la méthode vise à maximiser la variance interclasse et minimiser la variance intraclasse, ce qui revient d’ailleurs au même. Cette technique fonctionne souvent très bien pour obtenir des groupes bien individualisés. zoo6std_dist %&gt;.% hclust(., method = &quot;ward.D2&quot;) %&gt;.% plot(.) Ici nous obtenos un dendrogramme très proche des liens complets. Encore une fois, ce n’est pas forcément le cas dans toutes les situations. Liens médians, centroïdes, …, constituent encore d’autre méthodes possibles, mais moins utilisées. Elles ont l’inconvénient de produire parfois des inversions dans le dendrogramme, c’est-à-dire qu’un regroupement plus avant se fait parfois à une hauteur plus basse sur l’axe des ordonnées, ce qui rend le dendrogramme peu lisible et beaucoup moins esthétique. Dans notre exemple, la méthode centroïde crée une telle inversion à la hauteur de 5 environ sur l’axe des ordonnées entre l’individu 6 et l’individu 1. Alors, qui est regroupé en premier ? Pas facile à déterminer dans ce cas ! zoo6std_dist %&gt;.% hclust(., method = &quot;centroid&quot;) %&gt;.% plot(.) 5.3.3 Étude complète Voici ce que cela donne si nous effectuons une CAH sur le jeu zooplancton complet avec ses 1262 lignes. zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class scale(.) %&gt;.% # Standardisation des 19 colonnes as_tibble(.) %&gt;.% # Transformation en data.frame + tibble vegan::vegdist(., method = &quot;euclidean&quot;) %&gt;.% # Matrice de distances hclust(., method = &quot;ward.D2&quot;) -&gt; zoo_clust # CAH avec Ward plot(zoo_clust, labels = FALSE) # Dendrogramme sans labels des individus abline(h = 70, col = &quot;red&quot;) # Séparation en 3 groupes Naturellement, avec 1262 branches, notre dendrogramme est très encombré ! Cependant, il reste analysable tant que nous nous intéressons aux regroupements de plus haut niveau (vers le haut du dendrogramme). Notez comme les vlaeurs sur l’axe des ordonnées ont changé par rapport à nos cas simple à six items (ne jamais comparer les hauteurs entre dendrogrammes différents). Notre CAH est terminée, mais nous pouvons matérialiser le regroupement sous forme d’une variable supplémentaire dans le tableau et l’utiliser ensuite. zoo_clust %&gt;.% cutree(., h = 70) %&gt;.% # Matérialisation des groupes as.factor(.) -&gt; # Conversion en variable facteur zoo$Groupes # Ajout au tableau comme variable Groupes chart(zoo, compactness ~ ecd %col=% Groupes) + geom_point() + # Exploration visuelle des groupes (exemple) coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) # Axes en log10 Et de manière optionnelle, si un classement est disponible par ailleurs (c’est le cas ici avec la variable class), nous pouvons réaliser un tableau de contingence à double entrées entre le regroupement obtenu et le classement pour comparaison. table(zoo$class, zoo$Groupes) # # 1 2 3 # Annelid 31 13 6 # Appendicularian 0 36 0 # Calanoid 9 237 42 # Chaetognath 0 17 34 # Cirriped 1 21 0 # Cladoceran 47 3 0 # Cnidarian 3 5 14 # Cyclopoid 0 50 0 # Decapod 121 5 0 # Egg_elongated 2 48 0 # Egg_round 44 0 5 # Fish 4 46 0 # Gastropod 48 2 0 # Harpacticoid 0 39 0 # Malacostracan 27 66 28 # Poecilostomatoid 20 136 2 # Protist 0 50 0 Ainsi, nous pouvons constater que le groupe 1 contient une fraction importante des annélides, des cladocères,des décapodes, des œufs ronds et des gastéropodes. Le groupe 2 contient un maximum des copépodes représentés par les calanoïdes, les cyclopoïdes, les harpacticoïdes et les poecilostomatoïdes. Il contient aussi tous les appendiculaires, tous les protistes, presque tous les œufs allongés, les poissons, et une majorité des malacostracés. Enfin, le groupe 3 contient une majorité des chaetognathes et des cnidaires. Le regroupement a été réalisé uniquement en fonction de mesures effectuées sur les images. Il n’est pas parfait, mais des tendances se dégagent tout de même. Pour en savoir plus La vidéo suivante présente la matière que nous venons d’étudier de manière légèrement différente. Plus d’explications sont également apportées concernant la méthode de Ward. Une autre explication encore ici. Pour bien comprendre la façon dont une CAH est réalisée, il est utile de détailler le calcul étape par étape sur un exemple simple. Voici une matrice de distances euclidiennes fictive entre 6 stations  : A B C D E B 15 C 6.4 10.86 D 5.2 13.04 5.48 E 5.1 12.37 7.28 7.81 F 10.39 7.42 5.57 9.64 9.49 Effectuons une CAH manuellement par liens complets et traçons le dendrogramme correspondant. Etape 1 : Nous repérons dans la matrice de distance la paire qui a l’indice de dissimilarité le plus petit et effectuons un premier regroupement. A_E = groupe I à la distance 5,1. La matrice de distances est simplifiées par rapport à ce groupe I en considérant la règle utilisée (ici, liens complets, donc on garde la plus grande distance entre toutes les paires possibles lorsqu’il y a des groupes). * Distance entre B et I = B_A = 15 * Distance entre C et I = C_E = 7,28 * Distance entre D et I = D_E = 7,81 * Distance entre F et I = F_A = 10,39 Matrice de distance recalculée : I B C D B 15.00 C 7.28 10.86 D 7.81 13.04 5.48 F 10.39 7.42 5.57 9.64 Etape 2 : on répète le processus. C_D = groupe II à la distance 5,48. * Distance entre I et II = I_D = 7,81 * Distance entre B et II = B_D = 13,04 * Distance entre F et II = F_D = 9,64 Matrice de distance recalculée : I B II B 15.00 II 7.81 13.04 F 10.39 7.42 9.64 Etape 3 : B_F = groupe III à la distance 7,42 * Distance entre I et III = I_B = 15 * Distance entre II et III = II_B = 13,04 Matrice de distance recalculée  : I III III 15.00 II 7.81 13.04 Etape 4 : I_II = groupe IV à la distance 7,81 * Distance entre III et IV = III_I = 15 Matrice de distance recalculée : III IV 15 Etape 5 : III - IV = groupe V à la distance 15. fini ! Voici le dendrogramme résultant : Les techniques complémentaires de classification supervisées, recommandées dans le cas du jeu de données zooplancton seront abordées dans le cours de Science des Données Biologiques III l’an prochain.↩ "],
["k-moyenne-mds-som.html", "Module 6 K-moyenne, MDS &amp; SOM", " Module 6 K-moyenne, MDS &amp; SOM Objectifs Maîtriser la technique de classification par les k-moyennes comme alternative à la CAH pour les gros jeux de données. Comprendre la représentation d’une matrice de distances sur un carte (ordination) et la réduction de dimensions via le positionnement multidimensionnel MDS. Être capable de créer des cartes auto-adaptatives ou SOM, de les interpréter et de les utiliser comme autre technique de classification. Prérequis Ces techniques étant basées sur des matrices de distances et complémentaires à la classification ascendante hiérarchique, le module 5 doit être assimilé avant de s’attaquer au présent module. "],
["k-moyennes.html", "6.1 K-moyennes", " 6.1 K-moyennes Les k-moyennes (ou “k-means” en anglais) représentent une autre façon de regrouper les individus d’un tableau multivarié. Par rapport à la CAH, cette technique est généralement moins efficace, mais elle a l’avantage de permettre le regroupement d’un très grand nombre d’individus (gros jeu de données), là où la CAH nécessiterait trop de temps de calcul et de mémoire vive. Il est donc utile de connaitre cette seconde technique à utiliser comme solution de secours lorsque le dendrogramme de la CAH devient illisible sur de très gros jeux de données. Le principe des k-moyennes est très simple15 : L’utilisateur choisi le nombre de groupes k qu’il veut obtenir à l’avance. La position des k centres est choisie au hasard au début. Les individus sont attribués aux k groupes en fonction de leurs distances aux centres (attribution au groupe de centre le plus proche). Les k centres sont replacés au centre de gravité des groupes ainsi obtenus. Les individus sont réaffectés en fonction de leurs distances à ces nouveaux centres. Si au moins un individu a changé de groupe, le calcul est réitéré. Sinon, nous considérons avoir atteint la configuration finale. La technique est superbement expliquée et illustrée dans la vidéo suivante : Essayez par vous même via l’application ci-dessous qui utilise le célèbre jeu de données iris. Notez que vous devez utiliser des variables numériques. Par exemple, Species étant une variable qualitative, vous verrez que cela ne fonctionne pas dans ce cas. 6.1.1 Exemple simple Afin de comparer la classification par k-moyennes à celle par CAH, nous reprendrons ici le même jeu de données zooplankton. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Commençons par l’exemple simplissime de la réalisation de deux groupes à partir de six individus issus de ce jeu de données, comme nous l’avons fait avec la CAH : zoo %&gt;.% select(., -class) %&gt;.% # Elimination de la colonne class slice(., 13:18) -&gt; zoo6 # Récupération des lignes 13 à 18 zoo6_kmeans &lt;- kmeans(zoo6, centers = 2) zoo6_kmeans # K-means clustering with 2 clusters of sizes 3, 3 # # Cluster means: # ecd area perimeter feret major minor mean # 1 1.1926500 1.1279667 10.346667 2.201133 1.677067 0.8596333 0.3217333 # 2 0.6292647 0.3188667 3.224133 1.159200 1.096433 0.4023333 0.1871667 # mode min max std_dev range size aspect # 1 0.3533333 0.00400000 0.8986667 0.2620000 0.8946667 1.2683500 0.5149422 # 2 0.1026667 0.01066667 0.5400000 0.1166667 0.5293333 0.7493833 0.4753843 # elongation compactness transparency circularity density # 1 23.046713 7.987806 0.06173831 0.1357000 0.37630000 # 2 6.333315 2.727708 0.14732060 0.4900333 0.06943333 # # Clustering vector: # [1] 2 1 2 1 1 2 # # Within cluster sum of squares by cluster: # [1] 200.03837 54.18647 # (between_SS / total_SS = 68.1 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Nous voyons que la fonction kmeans() effectue notre classification. Nous lui fournissons le tableau de départ et spécifions le nombre k de groupes souhaités via l’argument centers =. Ne pas oublier d’assigner le résultat du calcul à une nouvelle variable, ici zoo6_kmeans, pour pouvoir l’inspecter et l’utiliser par la suite. L’impression du contenu de l’objet nous donne plein d’information dont : le nombre d’individus dans chaque groupe (ici 3 et 3), la position des centres pour les k groupes dans Cluster means, l’appartenance aux groupes dans Cluster vectors (dans le même ordre que les lignes du tableau de départ), la sommes des carrés des distances entre les individus et la moyenne au sein de chaque groupe dans Within cluster sum of squares ; le calcul between_SS / total_SS est à mettre en parallèle avec le \\(R^2\\) de la régression linéaire : c’est une mesure de la qualité de regroupement des données (plus la valeur est proche de 100% mieux c’est, mais attention que cette valeur augmente d’office en même temps que k), et enfin, la liste des composants accessibles via l’opérateur $ ; par exemple, pour obtenir les groupes (opération similaire à cutree() pour la CAH), nous ferons : zoo6_kmeans$cluster # [1] 2 1 2 1 1 2 Le package broom contient trois fonctions complémentaires qui nous seront utiles : tidy(), augment() et glance(). broom::glance() retourne un data.frame avec les statistiques permettant d’évaluer la qualité de la classification obtenue : broom::glance(zoo6_kmeans) # # A tibble: 1 x 4 # totss tot.withinss betweenss iter # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; # 1 796. 254. 542. 1 De plus, le package factoextra propose une fonction fviz_nbclust() qui réalise un graphique pour aider au choix optimal de k : factoextra::fviz_nbclust(zoo6, kmeans, method = &quot;wss&quot;, k.max = 5) Le graphique obtenu montre la décroissance de la somme des carrés des distances intra-groupes en fonction de k. Avec k = 1, nous considérons toutes les données dans leur ensemble et nous avons simplement la somme des carrés des distances euclidiennes entre tous les individus et le centre de gravité du nuage de points dont les coordonnées sont les moyennes de chaque variable. C’est le point de départ qui nous indique de combien les données sont dispersées (la valeur absolue de ce nombre n’est pas importante). Ensuite, avec k croissant, notre objectif est de faire des regroupement qui diminuent la variance intra-groupe autant que possible, ce que nous notons par la diminution de la somme des carrés intra-groupes (la variance du groupe est, en effet, la somme des carrés des distances enclidiennes entre les points et le centre du groupe, divisée par les degrés de liberté). Nous recherchons ici des sauts importants dans la décroissance de la somme des carrés, tout comme dans le dendrogramme obtenu par la CAH nous recherchions des sauts importants dans les regroupements (hauteur des barres verticales du dendrogramme). Nous observons ici un saut important pour k = 2, puis une diminution moins forte de k = 3 à k = 5. Ceci suggère que nous pourrions considérer deux groupes. Le nombre de groupes proposé par factoextra::fviz_nbclust() n’est qu’indicatif ! Si vous avez par ailleurs d’autres informations qui vous suggèrent un regroupement différent, ou si vous voulez essayer un regroupement plus ou moins détaillé par rapport à ce qui est proposé, c’est tout aussi correct. La fonction factoextra::fviz_nbclust() propose d’ailleurs deux autres méthodes pour déterminer le nombre optimal de groupes k, avec method = &quot;silhouette&quot; ou method = &quot;gap_stat&quot;. Voyez l’aide en ligne de cette fonction ?factoextra::fviz_nbclust. Ces différentes méthodes peuvent d’ailleurs suggérer des regroupements différents pour les mêmes données… preuve qu’il n’y a pas une et une seule solution optimale ! A ce stade, nous pouvons collecter les groupes et les ajouter à notre tableau de données. Pour la CAH, vous avez déjà remarqué que rajouter ces groupes dans le tableau de départ peut mener à des effets surprenants si nous relançons ensuite l’analyse sur le tableau ainsi complété16. Donc, nous prendrons soin de placer les données ainsi complétées de la colonne cluster dans un tableau différent nommé zoo6b. Pour se faire, nous pouvons utiliser broom::augment(). broom::augment(zoo6_kmeans, zoo6) %&gt;.% rename(., cluster = .cluster) -&gt; zoo6b names(zoo6b) # [1] &quot;ecd&quot; &quot;area&quot; &quot;perimeter&quot; &quot;feret&quot; # [5] &quot;major&quot; &quot;minor&quot; &quot;mean&quot; &quot;mode&quot; # [9] &quot;min&quot; &quot;max&quot; &quot;std_dev&quot; &quot;range&quot; # [13] &quot;size&quot; &quot;aspect&quot; &quot;elongation&quot; &quot;compactness&quot; # [17] &quot;transparency&quot; &quot;circularity&quot; &quot;density&quot; &quot;cluster&quot; Comme vous pouvez le constater, une nouvelle colonne nommée .cluster a été ajoutée au tableau en dernière position, que nous avons renommée immédiatement en cluster ensuite (c’est important pour le graphique plus loin). Elle contient ceci : zoo6b$cluster # [1] 2 1 2 1 1 2 # Levels: 1 2 C’est le contenu de zoo6_kmeans$cluster, mais transformé en variable factor. class(zoo6b$cluster) # [1] &quot;factor&quot; Nous pouvons enfin utiliser broom::tidy() pour obtenir un tableau avec les coordonnées des k centres. Nous l’enregistrerons dans la variable zoo6_centers, en ayant bien pris soin de nommer les variables du même nom que dans le tableau original zoo6 (argument col.names = names(zoo6), cela sera important pour le graphique ci-dessous) : zoo6_centers &lt;- broom::tidy(zoo6_kmeans, col.names = names(zoo6)) zoo6_centers # # A tibble: 2 x 21 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 1.19 1.13 10.3 2.20 1.68 0.860 0.322 0.353 0.004 0.899 0.262 # 2 0.629 0.319 3.22 1.16 1.10 0.402 0.187 0.103 0.0107 0.540 0.117 # # … with 10 more variables: range &lt;dbl&gt;, size &lt;int&gt;, aspect &lt;dbl&gt;, # # elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, withinss &lt;dbl&gt;, cluster &lt;fct&gt; La dernière colonne de ce tableau est également nommée cluster. C’est le lien entre le tableau zoo6b augmenté et zoo6_centers. Nous avons maintenant tout ce qu’il faut pour représenter graphiquement les regroupements effectués par les k-moyennes en colorant les points en fonction de la nouvelle variable cluster. chart(data = zoo6b, area ~ circularity %col=% cluster) + geom_point() + # Affiche les points représentant les individus geom_point(data = zoo6_centers, size = 5, shape = 17) # Ajoute les centres Comparez avec le graphique équivalent au module précédent consacré à la CAH. Outre que l’ordre des groupes est inversé et que les données n’ont pas été standardisées ici, un point est classé dans un groupe différent par les deux méthodes. Il s’agit du point ayant environ 0.25 de circularité et 0.5 de surface. Comme nous connaissons par ailleurs la classe à laquelle appartient chaque individu, nous pouvons la récupérer comme colonne supplémentaire du tableau zoo6b et ajouter cette information sur notre graphique. zoo6b$class &lt;- zoo$class[13:18] zoo6_centers$class &lt;- &quot;&quot; # Ceci est nécessaire pour éviter le label des centres chart(data = zoo6b, area ~ circularity %col=% cluster %label=% class) + geom_point() + ggrepel::geom_text_repel() + # Ajoute les labels intelligemment geom_point(data = zoo6_centers, size = 5, shape = 17) Nous constatons que le point classé différemment est un “Poecilostomatoïd”. Or, l’autre groupe des k-moyennes contient aussi un individu de la même classe. Donc, CAH a mieux classé notre plancton que les k-moyennes dans le cas présent. Ce n’est pas forcément toujours le cas, mais souvent. Un dernier point est important à mentionner. Comme les k-moyennes partent d’une position aléatoire des k centres, le résultat final peut varier et n’est pas forcément optimal. Pour éviter cela, nous pouvons indiquer à kmeans() d’essayer différentes situations de départ via l’argument nstart =. Par défaut, nous prenons une seule situation aléatoire de départ nstart = 1, mais en indiquant une valeur plus élevée pour cet argument, il est possible d’essayer plusieurs situations de départ et ne garder que le meilleur résultat final. Cela donne une analyse plus robuste et plus reproductible… mais le calcul est naturellement plus long. kmeans(zoo6, centers = 2, nstart = 50) # 50 positions de départ différentes # K-means clustering with 2 clusters of sizes 3, 3 # # Cluster means: # ecd area perimeter feret major minor mean # 1 0.6292647 0.3188667 3.224133 1.159200 1.096433 0.4023333 0.1871667 # 2 1.1926500 1.1279667 10.346667 2.201133 1.677067 0.8596333 0.3217333 # mode min max std_dev range size aspect # 1 0.1026667 0.01066667 0.5400000 0.1166667 0.5293333 0.7493833 0.4753843 # 2 0.3533333 0.00400000 0.8986667 0.2620000 0.8946667 1.2683500 0.5149422 # elongation compactness transparency circularity density # 1 6.333315 2.727708 0.14732060 0.4900333 0.06943333 # 2 23.046713 7.987806 0.06173831 0.1357000 0.37630000 # # Clustering vector: # [1] 1 2 1 2 2 1 # # Within cluster sum of squares by cluster: # [1] 54.18647 200.03837 # (between_SS / total_SS = 68.1 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Dans ce cas simple, cela ne change pas grand chose. Mais avec un plus gros jeu de données plus complexe, cela peut être important. 6.1.2 Classification du zooplancton Maintenant que nous savons utiliser kmeans() et les fonctions annexes, nous pouvons classer le jeu de données zoo tout entier. zoo %&gt;.% select(., -class) %&gt;.% factoextra::fviz_nbclust(., kmeans, method = &quot;wss&quot;, k.max = 10) Nous observons un saut maximal pour k = 2, mais le saut pour k = 3 est encore conséquent. Afin de comparer avec ce que nous avons fait par CAH, nous utiliserons donc k = 3. Enfin, comme un facteur aléatoire intervient, qui définira au final le numéro des groupes, nous utilisons set.seed() pour rendre l’analyse reproductible. Pensez à donner une valeur différente à cette fonction pour chaque utilisation ! Et pensez aussi à éliminer les colonnes non numériques à l’aide de select(). set.seed(562) zoo_kmeans &lt;- kmeans(select(zoo, -class), centers = 3, nstart = 50) zoo_kmeans # K-means clustering with 3 clusters of sizes 786, 91, 385 # # Cluster means: # ecd area perimeter feret major minor mean # 1 0.6664955 0.431915 3.575374 1.134705 0.9744768 0.4780780 0.2388065 # 2 1.3774670 1.998097 19.653860 4.063837 2.1465758 0.9602846 0.1488495 # 3 0.9715857 1.009902 9.197299 2.668022 1.8468984 0.6194652 0.1723774 # mode min max std_dev range size aspect # 1 0.09256997 0.007094148 0.7269109 0.1842660 0.7198168 0.7262774 0.5372808 # 2 0.02470330 0.004000000 0.7013187 0.1472286 0.6973187 1.5534302 0.5362249 # 3 0.04455065 0.004207792 0.6315844 0.1512922 0.6273766 1.2331818 0.5349924 # elongation compactness transparency circularity density # 1 7.184451 3.002093 0.07385014 0.42917214 0.09349338 # 2 61.837019 20.325398 0.09737903 0.05186813 0.31140879 # 3 27.898079 9.529156 0.11719954 0.11197351 0.16938468 # # Clustering vector: # [1] 1 1 1 1 1 1 2 1 2 1 1 3 1 3 1 3 3 1 2 1 1 1 1 3 3 1 1 3 1 1 1 1 1 1 # [35] 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 3 1 1 3 1 3 3 1 # [69] 3 1 1 1 1 2 1 1 1 1 1 3 1 1 1 1 1 2 1 1 3 1 1 3 1 2 1 1 1 1 1 1 1 1 # [103] 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 # [137] 1 1 1 1 1 3 1 1 2 3 2 3 1 1 1 1 1 3 3 3 3 3 1 2 1 1 1 1 1 1 1 3 1 1 # [171] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 # [205] 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 # [239] 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 # [273] 3 2 1 1 3 1 3 3 1 3 1 1 2 1 1 2 3 2 3 3 1 1 1 1 3 3 2 1 3 1 3 3 1 3 # [307] 3 3 2 1 3 3 2 1 3 2 2 1 2 3 3 3 1 1 3 1 1 3 1 1 1 1 1 3 1 2 1 1 1 3 # [341] 1 1 1 1 1 1 3 3 3 1 3 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 # [375] 3 1 1 1 1 1 1 3 3 1 1 1 1 1 3 3 1 1 3 1 3 3 1 3 3 1 1 1 1 1 1 1 1 3 # [409] 1 1 1 3 3 1 1 1 1 1 1 1 3 3 3 2 1 3 3 1 1 1 3 3 3 1 1 1 1 2 1 1 3 3 # [443] 2 1 3 1 3 1 3 1 3 1 1 3 1 3 1 3 3 1 1 1 1 1 3 1 1 3 1 1 1 3 1 1 3 1 # [477] 1 1 3 1 1 1 1 1 1 1 1 3 1 3 3 3 1 3 1 3 1 3 3 3 1 1 1 1 1 1 1 1 3 3 # [511] 1 3 2 3 1 2 1 1 3 3 3 1 3 1 1 1 2 1 2 2 3 1 3 1 1 1 3 1 1 1 1 3 3 1 # [545] 1 1 3 3 1 1 3 1 3 1 1 3 2 3 1 1 1 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 # [579] 1 2 3 3 3 2 1 2 3 3 2 1 3 3 3 1 1 3 3 2 1 1 1 1 1 1 1 1 1 3 1 3 3 1 # [613] 1 1 1 1 1 1 2 1 1 1 1 1 3 3 2 3 1 3 2 3 1 3 1 3 3 1 1 1 1 3 3 3 3 3 # [647] 3 3 1 1 3 1 1 1 1 1 1 1 3 3 3 3 3 3 1 2 3 3 3 3 1 1 3 3 1 1 1 1 3 3 # [681] 3 1 1 1 1 1 1 3 1 1 1 3 1 1 3 3 3 3 3 1 1 2 1 1 2 3 1 3 1 1 3 2 2 3 # [715] 1 1 2 3 1 2 3 3 3 2 3 1 3 2 1 3 1 1 2 3 1 2 1 2 3 3 3 1 2 1 3 3 2 3 # [749] 3 1 1 3 1 3 3 1 1 1 1 1 3 3 3 1 1 1 1 3 3 3 1 1 1 1 3 1 1 1 3 1 1 3 # [783] 1 3 1 1 1 3 1 1 3 1 1 3 1 1 3 1 1 1 3 1 3 2 3 1 1 3 3 2 1 3 1 1 3 3 # [817] 3 1 1 1 3 1 1 1 1 1 1 1 1 1 3 1 3 3 1 1 1 3 1 3 3 1 3 1 1 1 1 1 1 1 # [851] 3 3 3 1 1 3 3 3 1 1 1 1 1 1 3 1 3 2 3 1 3 1 3 1 1 2 1 2 1 3 3 3 3 3 # [885] 3 3 1 1 3 1 1 1 3 2 1 3 1 1 1 1 3 1 3 1 1 1 3 3 1 3 1 1 3 1 1 3 1 2 # [919] 1 3 3 1 1 3 3 1 1 1 2 3 2 3 2 3 3 3 2 1 3 2 3 3 3 1 3 3 3 3 3 3 2 3 # [953] 3 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 2 1 2 1 1 1 1 3 3 3 3 1 1 1 2 # [987] 2 1 3 3 1 2 1 3 3 2 3 3 3 3 1 3 2 3 2 3 2 1 3 1 1 1 1 3 3 1 3 1 2 3 # [1021] 1 1 3 3 3 3 3 1 3 3 3 1 1 3 3 1 2 3 3 3 2 1 3 2 1 3 3 2 1 1 3 1 1 1 # [1055] 3 1 1 1 1 1 1 1 1 3 3 3 3 3 2 3 3 3 3 1 3 3 3 1 3 2 1 3 3 3 2 3 1 1 # [1089] 1 1 1 2 1 3 3 1 2 2 3 1 1 3 3 1 1 2 3 3 3 3 3 1 1 1 2 1 1 1 1 1 1 1 # [1123] 3 1 1 1 3 1 3 3 1 1 3 1 1 1 1 1 1 3 3 1 3 3 3 3 2 1 1 1 1 3 1 1 1 1 # [1157] 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 1 1 3 3 1 1 1 3 1 1 1 1 3 1 1 1 1 1 # [1191] 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 # [1225] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 # [1259] 3 3 3 3 # # Within cluster sum of squares by cluster: # [1] 24356.96 43792.25 39813.31 # (between_SS / total_SS = 77.0 %) # # Available components: # # [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; # [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; # [9] &quot;ifault&quot; Récupérons les clusters dans zoob broom::augment(zoo_kmeans, zoo) %&gt;.% rename(., cluster = .cluster) -&gt; zoob Et enfin, effectuons un graphique similaire à celui réalisé pour la CAH au module précédent. À noter que nous pouvons ici choisir n’importe quelle paire de variables quantitatives pour représenter le nuage de points. Nous ajoutons des ellipses pour matérialiser les groupes à l’aide de stat_ellipse(). Elles contiennent 95% des points du groupe à l’exclusion des extrêmes. Enfin, comme il y a beaucoup de points, nous choisissons de les rendre semi-transparents avec l’argument alpha = 0.2 pour plus de lisibilité du graphique. chart(data = zoob, compactness ~ ecd %col=% cluster) + geom_point(alpha = 0.2) + stat_ellipse() + geom_point(data = broom::tidy(zoo_kmeans, col.names = names(zoo6)), size = 5, shape = 17) Nous observons ici un regroupement beaucoup plus simple qu’avec la CAH, essentiellement stratifié de bas en haut en fonction de la compacité des points (Compactness). La tabulation des clusters en fonction des classes connues par ailleurs montre aussi que les k-moyennes les séparent moins bien que ce qu’a pu faire la CAH : table(zoob$class, zoob$cluster) # # 1 2 3 # Annelid 38 6 6 # Appendicularian 21 0 15 # Calanoid 82 41 165 # Chaetognath 6 0 45 # Cirriped 14 0 8 # Cladoceran 50 0 0 # Cnidarian 13 3 6 # Cyclopoid 5 5 40 # Decapod 117 0 9 # Egg_elongated 50 0 0 # Egg_round 49 0 0 # Fish 50 0 0 # Gastropod 50 0 0 # Harpacticoid 1 9 29 # Malacostracan 54 26 41 # Poecilostomatoid 143 1 14 # Protist 43 0 7 Le cluster numéro 2 n’est pas vraiment défini en terme des classes de plancton car aucune classe ne s’y trouve de manière majoritaire. Le groupe numéro 1 contient la majorité des items de diverses classes, alors que le groupe 3 a une majorité de calanoïdes et d’harpacticoïdes (différents copépodes). Globalement, le classement a un sens, mais est moins bien corrélé avec les classes de plancton que ce que la CAH nous a fourni. Notez que, si nous avions standardisé les données avant d’effectuer les k-moyennes comme nous l’avons fait pour la CAH, nous aurions obtenu d’autres résultats. La transformation des variables préalablement à l’analyse reste une approche intéressante pour moduler l’importance des différentes variables entre elles dans leur impact sur le calcul des distances, et donc, des regroupements réalisés. Nous vous laissons réaliser les k-moyennes sur les données zoo standardisées à l'aide de la fonctionscale()` comme pour la CAH comme exercice. A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la méthode des k-moyennes. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06a_kmeans&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Pour en savoir plus Il existe une approche mixte qui mèle la CAH et les k-moyennes. Cette approche est intéressante pour les gros jeux de données. Le problématique est expliquée ici, et l’implémentation dans la fonction factoextra::hkmeans() est détaillée ici (en anglais). Cet article explique dans le détail kmeans() et hclust() dans R, et montre aussi comment on peut calculer les k-moyennes à la main pour bien en comprendre la logique (en anglais). En pratique, différents algorithmes avec diverses optimisations existent. Le plus récent et le plus sophistiqué est celui de Hartigan-Wong. Il est utilisé par défaut par la fonction kmeans(). En pratique, il y a peu de raison d’en changer.↩ Nous vous avons proposé exprès de rajouter les groupes dans le tableau de départ pour que vous soyez confronté à ce problème. Ici, nous proposons donc une autre façon de travailler qui l’évite en assignant le résultat dans une autre variable.↩ "],
["positionnement-multidimensionnel-mds.html", "6.2 Positionnement multidimensionnel (MDS)", " 6.2 Positionnement multidimensionnel (MDS) Le positionnement multidimensionnel, ou “multidimensional scaling” en anglais, d’où son acronyme fréquemment utilisé en français également : le MDS, est une autre façon de représenter clairement l’information contenue dans une matrice de distances. Ici, l’objectif n’est pas de regrouper ou de classifier les individus du tableau, mais de les ordonner sur un graphique en nuage de points en deux ou trois dimensions. Ce graphique s’appelle une “carte”, et la technique qui la réalise est une méthode d’ordination. Au départ, nous avons p colonnes et n lignes dans le tableau cas par variables, c’est-à-dire, p variables quantitatives mesurées sur n individus distincts. Nous voulons déterminer les similitudes ou différences de ces n individus en les visualisant sur une carte où la distance d’un individu à l’autre représente cette similitude. Plus deux individus sont proches, plus ils sont semblables. Plus les individus sont éloignés, plus ils diffèrent. Ces distances entre paires d’individus, nous les avons déjà calculées dans la matrice de distances. Mais comment les représenter ? En effet, une représentation exacte ne peut se faire que dans un espace à p dimensions (même nombre de dimensions que de variables initiales). Donc, afin de réduire les dimensions à seulement 2 ou 3, nous allons devoir “tordre” les données et accepter de perdre un peu d’information. Ce que nous allons faire avec la MDS correspond exactement à cela : nous allons littéralement “écraser” les données dans un plan (deux dimensions) ou dans un espace à trois dimensions. C’est donc ce qu’on appelle une technique de réduction de dimensions. Il existe, en réalité, plusieurs techniques de MDS. Elle répondent toutes au schéma suivant : A partir d’un tableau multivarié de n lignes et p colonnes, nous calculons une matrice de distances (le choix de la transformation initiale éventuelle et de la métrique de distance utilisée sont totalement libres ici17). Nous souhaitons représenter une carte (nuage de points) à k dimensions (k = 2, éventuellement k = 3) où les n individus seront placés de telle façon que les proximités exprimées par des valeurs faibles dans la matrice de dissimilarité soient respectées autant que possible entre tous les points. Pour y arriver les points sont placés successivement sur la carte et réajustés afin de minimiser une fonction de coût, encore appelée fonction de stress qui quantifie de combien nous avons dû “tordre” le réseau à p dimensions initial représentant les distances entre toutes les paires. C’est en adoptant différentes fonctions de stress que nous aboutissons aux différentes variantes de MDS. La fonction de stress est représentée graphiquement (voir ci-dessous) pour diagnostiquer le traitement réaliser et décider si la représentation est utilisable (pas trop tordue) ou non. Le positionnement des points faisant intervenir un facteur aléatoire (choix des points à placer en premier, réorganisation ensuite pour minimiser la fonction de stress), le résutat final peut varier d’une fois à l’autre sur les mêmes données, voir ne pas converger vers une solution stable. Il faut en être conscient. Nous vous épargnons ici les développements mathématiques qui mènent à la définition de la fonction de stress. Nous nous concentrerons sur les principales techniques et sur leurs propriétés utiles en pratique. 6.2.1 MDS simplifiée sous SciViews::R Dans R, il existe plus d’une dizaine de fonctions différentes pour réaliser le MDS. Afin de vous simplifier le travail et de pouvoir traiter votre MDS comme d’autres analyses similaires nous vous propoons les fonctions supplémentaites suivante. Ces fonctions sont à copier-coller en haut de vos scripts R, ou dans un chunk de “setup” à l’intérieur de vos documents R Markdown/Notebook. SciViews::R() library(broom) # function mds for several multidimensionnal scaling functions ------ mds &lt;- function(dist, k = 2, type = c(&quot;metric&quot;, &quot;nonmetric&quot;, &quot;cmdscale&quot;, &quot;wcmdscale&quot;, &quot;sammon&quot;, &quot;isoMDS&quot;, &quot;monoMDS&quot;, &quot;metaMDS&quot;), p = 2, ...) { type &lt;- match.arg(type) res &lt;- switch(type, metric = , wcmdscale = structure(vegan::wcmdscale(d = dist, k = k, eig = TRUE, ...), class = c(&quot;wcmdscale&quot;, &quot;mds&quot;, &quot;list&quot;)), cmdscale = structure(stats::cmdscale(d = dist, k = k, eig = TRUE, ...), class = c(&quot;cmdscale&quot;, &quot;mds&quot;, &quot;list&quot;)), nonmetric = , metaMDS = structure(vegan::metaMDS(comm = dist, k = k, ...), class = c(&quot;metaMDS&quot;, &quot;monoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), isoMDS = structure(MASS::isoMDS(d = dist, k = k, ...), class = c(&quot;isoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), monoMDS = structure(vegan::monoMDS(dist = dist, k = k, ...), class = c(&quot;monoMDS&quot;, &quot;mds&quot;, &quot;list&quot;)), sammon = structure(MASS::sammon(d = dist, k = k, ...), class = c(&quot;sammon&quot;, &quot;mds&quot;, &quot;list&quot;)), stop(&quot;Unknown &#39;mds&#39; type &quot;, type) ) # For non-metric MDS, we add also data required for the Shepard plot if (type %in% c(&quot;nonmetric&quot;, &quot;sammon&quot;, &quot;isoMDS&quot;, &quot;monoMDS&quot;, &quot;metaMDS&quot;)) res$Shepard &lt;- MASS::Shepard(d = dist, x = res$points, p = p) res } class(mds) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) # plot.mds : MDS2 ~ MDS1 -------------------------------- plot.mds &lt;- function(x, y, ...) { points &lt;- tibble::as_tibble(x$points, .name_repair = &quot;minimal&quot;) colnames(points) &lt;- paste0(&quot;mds&quot;, 1:ncol(points)) plot(data = points, mds2 ~ mds1,...) } autoplot.mds &lt;- function(object, labels, ...) { points &lt;- tibble::as_tibble(object$points, .name_repair = &quot;minimal&quot;) colnames(points) &lt;- paste0(&quot;mds&quot;, 1:ncol(points)) if (!missing(labels)) { if (length(labels) != nrow(points)) stop(&quot;You must provide a character vector of length &quot;, nrow(points), &quot; for &#39;labels&#39;&quot;) points$.labels &lt;- labels chart::chart(points, mds2 ~ mds1 %label=% .labels, ...) + geom_point() + ggrepel::geom_text_repel() + coord_fixed(ratio = 1) } else {# Plot without labels chart::chart(points, mds2 ~ mds1, ...) + geom_point() + coord_fixed(ratio = 1) } } shepard &lt;- function(dist, mds, p = 2) structure(MASS::Shepard(d = dist, x = mds$points, p = p), class = c(&quot;shepard&quot;, &quot;list&quot;)) plot.shepard &lt;- function(x, y, l.col = &quot;red&quot;, l.lwd = 1, xlab = &quot;Observed Dissimilarity&quot;, ylab = &quot;Ordination Distance&quot;, ...) { she &lt;- tibble::as_tibble(x, .name_repair = &quot;minimal&quot;) plot(data = she, y ~ x, xlab = xlab, ylab = ylab, ...) lines(data = she, yf ~ x, type = &quot;S&quot;, col = l.col, lwd = l.lwd) } autoplot.shepard &lt;- function(object, alpha = 0.5, l.col = &quot;red&quot;, l.lwd = 1, xlab = &quot;Observed Dissimilarity&quot;, ylab = &quot;Ordination Distance&quot;, ...) { she &lt;- tibble::as_tibble(object) chart(data = she, y ~ x) + geom_point(alpha = alpha) + geom_step(chart::f_aes(yf ~ x), direction = &quot;vh&quot;, col = l.col, lwd = l.lwd) + labs(x = xlab, y = ylab) } # augment.mds ------------------------------------------- augment.mds &lt;- function(x, data, ...){ points &lt;- as_tibble(x$points) colnames(points) &lt;- paste0(&quot;.mds&quot;, 1:ncol(points)) bind_cols(data, points) } # glance.mds ------------------------------------------- glance.mds &lt;- function(x, ...){ if (&quot;GOF&quot; %in% names(x)) {# Probably cmdscale() or wcmdscale() =&gt; metric MDS tibble::tibble(GOF1 = x$GOF[1], GOF2 = x$GOF[2]) } else {# Non metric MDS # Calculate linear and non linear R^2 from the Shepard (stress) plot tibble::tibble( linear_R2 = cor(x$Shepard$y, x$Shepard$yf)^2, nonmetric_R2 = 1 - sum(vegan::goodness(x)^2) ) } } 6.2.2 MDS métrique ou PCoA La forme classique, aussi appelée MDS métrique ou analyse en coordonnées principales (Principal Coordinates Analysis en anglais ou PCoA), va projetter le nuage de points à p dimensions dans un espace réduit à k = 2 dimensions (voire éventuellement à 3 dimensions). Cette projection se fait de manière similaire à une ombre chinoise projettée d’un objet tridimensionnel sur une surface plane en deux dimensions. Ombre chinoise : un placement astucieux des mains dans le faisceau lumineux permet de projetter l’ombre d’un animal ou d’un objet sur une surface plane. La PCoA fait de même avec vos données. Considérons un relevé de couverture végétale en 24 stations concernant 44 plantes répertoriées sur le site de l’étude, par exemple, Callvulg est Calluna vulgaris, Empenigr est Empetrum nigrum, etc. Les valeurs sont les couvertures végétales observées pour chaque plante sur le site, expérimées en pourcents. La première colonne nommée rownames à l’importation contient les identifiants des stations (chaînes de caractères). Nous la renommons donc pour un intitulé plus explicite : station. read(&quot;varespec&quot;, package = &quot;vegan&quot;) %&gt;.% rename(., station = rownames) -&gt; veg veg # # A tibble: 24 x 45 # station Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex # &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 18 0.55 11.1 0 0 17.8 0.07 0 # 2 15 0.67 0.17 0 0.35 12.1 0.12 0 # 3 24 0.1 1.55 0 0 13.5 0.25 0 # 4 27 0 15.1 2.42 5.92 16.0 0 3.7 # 5 23 0 12.7 0 0 23.7 0.03 0 # 6 19 0 8.92 0 2.42 10.3 0.12 0.02 # 7 22 4.73 5.12 1.55 6.05 12.4 0.1 0.78 # 8 16 4.47 7.33 0 2.15 4.33 0.1 0 # 9 28 0 1.63 0.35 18.3 7.13 0.05 0.4 # 10 13 24.1 1.9 0.07 0.22 5.3 0.12 0 # # … with 14 more rows, and 37 more variables: Betupube &lt;dbl&gt;, # # Vacculig &lt;dbl&gt;, Diphcomp &lt;dbl&gt;, Dicrsp &lt;dbl&gt;, Dicrfusc &lt;dbl&gt;, # # Dicrpoly &lt;dbl&gt;, Hylosple &lt;dbl&gt;, Pleuschr &lt;dbl&gt;, Polypili &lt;dbl&gt;, # # Polyjuni &lt;dbl&gt;, Polycomm &lt;dbl&gt;, Pohlnuta &lt;dbl&gt;, Ptilcili &lt;dbl&gt;, # # Barbhatc &lt;dbl&gt;, Cladarbu &lt;dbl&gt;, Cladrang &lt;dbl&gt;, Cladstel &lt;dbl&gt;, # # Cladunci &lt;dbl&gt;, Cladcocc &lt;dbl&gt;, Cladcorn &lt;dbl&gt;, Cladgrac &lt;dbl&gt;, # # Cladfimb &lt;dbl&gt;, Cladcris &lt;dbl&gt;, Cladchlo &lt;dbl&gt;, Cladbotr &lt;dbl&gt;, # # Cladamau &lt;dbl&gt;, Cladsp &lt;dbl&gt;, Cetreric &lt;dbl&gt;, Cetrisla &lt;dbl&gt;, # # Flavniva &lt;dbl&gt;, Nepharct &lt;dbl&gt;, Stersp &lt;dbl&gt;, Peltapht &lt;dbl&gt;, # # Icmaeric &lt;dbl&gt;, Cladcerv &lt;dbl&gt;, Claddefo &lt;dbl&gt;, Cladphyl &lt;dbl&gt; Typiquement ce genre de données ne contient pas d’information constructive lorsqu’une plante est simultanément absente de deux stations (double zéros). Donc, les métriques de type euclidienne ou Manhattan ne conviennent pas ici. Nous devons choisir entre distance de Bray-Curtis ou Canberra en fonction de l’importance que nous souhaitons donner aux plantes les plus rares (avec couverture végétale faible et/ou absentes de la majorité des stations). Afin de décider quelle métrique utiliser, visualisons à présent l’abondance ou la rareté des différentes plantes : veg %&gt;.% select(., -station) %&gt;.% # Colonne &#39;station&#39; pas utile ici gather(., key = &quot;espèce&quot;, value = &quot;couverture&quot;) %&gt;.% # Tableau en format long chart(., couverture ~ espèce) + geom_boxplot() + # Boites de dispersion labs(x = &quot;Espèce&quot;, y = &quot;Couverture [%]&quot;) + coord_flip() # Labels plus lisibles si sur l&#39;axe Y Comme nous pouvions nous y attendre, sept ou huit espèces dominent la couverture végétales et les autres données sont complètement écrasées à zéro sur l’axe pour la majorité des stations. Si nous utilisons la distance de Bray-Curtis, l’analyse sera pratiquement réalisée sur seulement ces quelques espèces dominantes. Avec Canberra, nous risquons par contre de donner beaucoup trop d’importance aux espèces extrêmement rares (toutes les espèces ont une importance égale avec cette métrique). Une solution intermédiaire est de transformer les données pour réduire l’écart d’importance entre les espèces abondantes et les rares, soit avec \\(log(x + 1)\\), soit avec \\(\\sqrt{\\sqrt{x}}\\). Voyons ce que donne la transformation logarithmique ici en utilisant la fonction log1p() dans R. veg %&gt;.% select(., -station) %&gt;.% gather(., key = &quot;espèce&quot;, value = &quot;couverture&quot;) %&gt;.% chart(., log1p(couverture) ~ espèce) + # Transformation log(couverture + 1) geom_boxplot() + labs(x = &quot;Espèce&quot;, y = &quot;Couverture [%]&quot;) + coord_flip() C’est nettement mieux car les données concernant les espèces rares ne sont plus totalement écrasées vers zéro sur l’axe horizontal ! La matrice de distances de Bray-Curtis sur nos données transformées log est la première étape de l’analyse : veg %&gt;.% select(., -station) %&gt;.% log1p(.) %&gt;.% vegan::vegdist(., method = &quot;bray&quot;) -&gt; veg_dist N’imprimez pas le contenu de veg_dist ! C’est de toutes façons illisible. La PCoA va visualiser son contenu de manière bien plus utile. La seconde étape consiste à calculer notre MDS métrique en utilisant mds$metric() veg_mds &lt;- mds$metric(veg_dist) Ensuite, troisième étape, le but étant de visualiser les distances nous effectons immédiatement un graphique comme suit : autoplot(veg_mds, labels = veg$station) Ce graphique s’interprète comme suit : Des stations proches l’une de l’autre sur la carte ont des indices de dissimilarité faibles. Ces stations sont semblables du point de vue de la couverture végétale. Plus les stations sont éloignées les unes des autres, plus elles sont dissemblables. Si des regroupements apparaissent sur la carte, il se peut que ce soit des biotopes semblables, et qui diffèrent des autres regroupements. Par exemple ici, les stations 14–15, 20 et 22–25 forment un groupe relativement homogène en haut à gauche du graphique qui s’individualise du reste. Au contraire, les stations 5, 21, ou encore 27 ou 28 sont relativement isolées et constituent donc des assemblages végétaux uniques. Les stations aux extrémités sont des configurations extrêmes ; celles au centre sont des configurations plus courantes. Par contre, ni l’orientation des axes, ni les valeurs absolues sur ces axes n’ont de significations particulières ici. N’en tenez pas compte. Attention : rien ne garantit que notre MDS métrique projettée en deux dimensions soit suffisamment représentative des données dans leur ensemble. Si la méthode n’a pas réussi à représenter fidèlement les données, c’est que ces dernières sont trop complexes et ne s’y prêtent pas. Contrôlez donc toujours les indicateurs que sont les valeurs de “Goodness-of-fit” (GOF, qualité d’ajustement). Les indicateurs “GOF” sont obtenus via la fonction glance() : glance(veg_mds) # # A tibble: 1 x 2 # GOF1 GOF2 # &lt;dbl&gt; &lt;dbl&gt; # 1 0.527 0.554 Ici GOF1 est la somme des valeurs propres obtenues lors du calcul (ces valeurs propres vous seront expliquées dans le module suivant consacré à l’ACP). Retenez simplement que c’est une mesure de la part de variance du jeu de données initial qui a pu être représentée sur la carte. Plus la valeur se rapproche de 1, mieux c’est, avec des valeurs &gt; 0.7 ou 0.8 qui restent acceptables. Le second indicateur, GOF2 est la somme uniquement des valeurs propres positives. Certains préfèrent ce dernier indicateur. En principe, les deux sont proches ou égaux. Donc, le choix de l’un ou de l’autre ne devrait pas fondamentalement modifier vos conclusions. Ici, avec des valeurs de goodness-of-fit à peine supérieures à 50% nous pouvons considérer que la carte n’est pas suffisamment représentative. Soit nous tentons de la représenter en trois dimensions (mais c’est rarement plus lisible car il faut quand même se résigner à présenter ce graphique 3D dans un plan à deux dimensions -l’écran de l’ordinateur, ou une feuille de papier- au final). Une autre solution lorsque la MDS métrique ne donne pas satisfaction est de se tourner vers la MDs non métrique. Ce que nous allons faire ci-dessous. A noter que la PCoA sur matrice euclidienne après standardisation ou non des données est équivalente à une Analyse en Composantes Principales (ACP) que nous étudierons dans le module suivant, … mais avec un calcul nettement moins efficace. Dans ce contexte, la PCoA n’a donc pas grand intérêt. Elle est surtout utile lorsque vous voulez représenter des métriques de distances différentes de la distance euclidienne comme c’est le cas ici avec un choix de distances de Bray-Curtis. 6.2.3 MDS non métrique La version non métrique de la MDS vise à réaliser une carte sur base de la matrice de distances, mais en autorisant des écarts plus flexibles entre les individus… pour autant que des individus similaires restent plus proches les uns des autres que des individus plus différents, et ce, partout sur la carte. Donc, une dissimilarité donnée pourra être “compressée” ou “dilatée”, pour autant que la distortion garde l’ordre des points intacts. Cela signifie que la distortion se fera via une fonction monotone croissante (une dissimilarité plus grande ne pouvant pas être représentée par une distance plus petite sur la carte). La distortion ainsi introduite est appelée un stress. C’est un peu comme si vous écrasiez par la force un objet 3D sur une surface plane, au lieu de juste en projeter l’ombre. Comme il existe différentes fonctions de stress, il existe donc différentes versions de MDS non métriques. Ici, nous nous attacherons à maitriser une version implémentée dans mds$nonmetric(). Il s’agit de l’une des premières formes de MDS non métriques qui a été proposée par le statisticien Joseph Kruskal (on parle aussi du positionnement multidimensionnel de Kruskal). La logique est la même que pour la MDS métrique : étape 1 : construction d’une matrice de distances, étape 2 : calcul du positionnement des points, étape 3 : réalisation de la carte et vérification de sa validité. Repartons de la même matrice de distances déjà réalisée pour la MDS métrique qui se nomme veg_dist. Le calcul est itératif. Comme il n’est pas garanti de converger, ni de donner la meilleure réponse, nous utilisons ici une fonction “intelligente” qui va effectuer une recherche plus poussée de la solution optimale, notamment en partant de différentes configurations au départ. Pour les détails et les paramètres de cet algorithme, voyez l’aide en ligne de la fonction ?vegan::metaMDS. Dans le cadre de ce cours, nous ferons confiance au travail réalisé et vérifierons juste qu’une solution est trouvée (indication *** Solution reached à la fin). Notez toutefois que le stress est quantifié. Il tourne ici autour de 0,126. Plus la valeur de stress est basse, mieux c’est naturellement. veg_nmds &lt;- mds$nonmetric(veg_dist) # Calcul # Run 0 stress 0.1256617 # Run 1 stress 0.1262346 # Run 2 stress 0.1262346 # Run 3 stress 0.1262346 # Run 4 stress 0.1256617 # ... Procrustes: rmse 1.056302e-05 max resid 3.244521e-05 # ... Similar to previous best # Run 5 stress 0.1256617 # ... Procrustes: rmse 6.34123e-06 max resid 1.783762e-05 # ... Similar to previous best # Run 6 stress 0.1256617 # ... Procrustes: rmse 1.768348e-05 max resid 4.243365e-05 # ... Similar to previous best # Run 7 stress 0.1262346 # Run 8 stress 0.1262346 # Run 9 stress 0.1256617 # ... Procrustes: rmse 1.668964e-05 max resid 5.282751e-05 # ... Similar to previous best # Run 10 stress 0.1262347 # Run 11 stress 0.1912667 # Run 12 stress 0.1262346 # Run 13 stress 0.1256617 # ... Procrustes: rmse 1.797304e-05 max resid 5.269659e-05 # ... Similar to previous best # Run 14 stress 0.1256617 # ... Procrustes: rmse 1.209131e-05 max resid 3.718001e-05 # ... Similar to previous best # Run 15 stress 0.2004491 # Run 16 stress 0.1256617 # ... New best solution # ... Procrustes: rmse 9.079683e-06 max resid 3.527497e-05 # ... Similar to previous best # Run 17 stress 0.1262347 # Run 18 stress 0.1262346 # Run 19 stress 0.2250581 # Run 20 stress 0.2105936 # *** Solution reached A présent, nous pouvons représenter la carte. autoplot(veg_nmds, labels = veg$station) Nous avons une représentation assez différente de celle de la MDS métrique. Les stations 5, 21, 27 et 28 sont toujours isolées, mais le reste est regroupé de manière plus homogène. Comment savoir si cette représentation est meilleure que la version métrique qui avait une “goodness-of-fit” décevante ? En visualisant les indicateurs de qualité d’ajustement, ainsi que la fonction de stress sur un graphique dit graphique de Shepard. Comme d’habitude, glance() nous donne les statistiques voulues. glance(veg_nmds) # # A tibble: 1 x 2 # linear_R2 nonmetric_R2 # &lt;dbl&gt; &lt;dbl&gt; # 1 0.919 0.984 Le premier indicateur (R2 linéaire) est le coefficient de corrélation linéaire de Pearson entre les distances ajustées et les distances sur la carte au carré. Plus cette valeur est proche de un, moins les distances sont tordues. Le second indicateur, le R2 non métrique est calculé comme 1 - S2 où S est le stress (tel que quantifié plus haut lors de l’appel à la fonction mds$nonmetric()). Cette dernière statistique indique si l’ordre des points respecte l’ordre des distances partout sur le graphique. Avec 0,98, la valeur est excellente ici. Ensuite le R2 linéaire nous indique de combien les différentes distances sont éventuellement distordues. Avec une valeur de 0,92, la distortion n’est pas trop forte ici. Le diagramme de Shepard permet de visualiser dans le détail la distortion introduite pour parvenir à réaliser la carte en deux dimensions. veg_sh &lt;- shepard(veg_dist, veg_nmds) autoplot(veg_sh) Sur l’axe des abscisses, nous avons les valeurs de dissimilarité présentes dans la matrice de distances. Sur l’axe des ordonnées, le graphique représente les distances de l’ordination, c’est-à-dire, les distances entre les paires de points sur la carte. Chaque point correspond à la dissimilarité d’une paire d’individus sur X, et à la distance entre cette paire sur la carte en Y. Enfin, le trait en escalier rouge matérialise la fonction monotone croissante choisie pour distordre les distances. C’est la fonction de stress. Ce diagramme se lit comme suit : Plus les points sont proches de la fonction de stress, mieux c’est. Le R2 non métrique sera également d’autant plus élevé que les points sont proches de la fonction. Plus la fonction de stress est linéaire, plus les distances respectent les valeurs de dissimilarités. Le R2 linéaire est lié à la plus ou moins bonne linéarité de la fonction de stress. Vous pouvez très bien décider que seul l’ordre des individus sur la carte compte. Dans ce cas, la forme de la fonction de stress et la valeur du R2 linéaire importent peu. Seul compte la proximité la plus forte possible des points par rapport à la fonction de stress sur le diagramme de Shepard, ainsi donc que la valeur du R2 non métrique. Si par contre, vous voulez être plus contraignant, alors, les distances seront considérées également comme importantes. Vous rechercherez alors une fonction de stress pas trop éloignée d’une droite, ainsi qu’un R2 linéaire élevé. Dans ce cas, nous nous rapprochons des exigences de la MDS métrique. Ici, nous pouvons constater que les deux critères sont bons. Nous pouvons donc nous fier à la carte obtenue à l’aide de la MDs non métrique de Kruskal. Restez toujours attentif à la taille du jeu de données que vous utilisez pour réaliser une MDS, en particuliers une MDS non métrique. Quelques centaines de lignes, ça dois passer, plusieurs dizaines de milliers ou plus, ça ne passera pas ! La limite dépend bien sûr de la puissance de votre ordinateur et de la quantité de mémoire vive disponible. Retenez toutefois que la quantité de calculs augmente drastiquement avec la taille du jeu de données à traiter. Pour en savoir plus La fonction mds() donne accès à d’autres versions de MDS non métriques également. Ainsi, mds$isoMDS() ou mds$monoMDS() correspondent toutes deux à la version de Kruskal mais en utilisant une seule configuration de départ (donc, moins robustes mais plus rapides à calculer). La mds$sammon() est une autre forme de MDS non métrique décrite dans l’aide en ligne de ?MASS::sammon. Des techniques existent pour déterminer la dimension k idéale de la carte. Le graphique des éboulis (screeplot en anglais) sera abordé au module suivante dans le cadre de l’ACP. Il en existe une version pour le MDS, voyez ici (en anglais). A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la mds. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06b_mds&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Chaque métrique de distance offre un éclairage différent sur les données. Elles agissent comme autant de filtres différents à votre disposition pour explorer vos données multivariées.↩ "],
["cartes-auto-adaptatives-som.html", "6.3 Cartes auto-adaptatives (SOM)", " 6.3 Cartes auto-adaptatives (SOM) Le positionnement multidimensionnel faisant appel à une matrice de distances entre tous les individus, les calculs deviennent vite pénalisants au fur et à mesure que le jeu de données augmente en taille. En général, les calculs sont assez lents. Nous verrons au module suivant que l’analyse en composantes principales apporte une réponse intéressante à ce problème, mais nous contraint à étudier des corrélations linéaires et des distances de type euclidiennes. Une approche radicalement différente, qui reste plus générale car non linéaire, est la méthode des cartes auto-adaptatives, ou encore, cartes de Kohonen du nom de son auteur se désigne par “self-organizing map” en anglais. L’acronyme SOM est fréquemment utilisé, même en français. Cette technique va encore une fois exploiter une matrice de distances dans le but de représenter les individus sur une carte. Cette fois-ci, la carte contient un certain nombre de cellules qui forment une grille, ou mieux, une disposition en nid d’abeille (nous verrons plus loin pourquoi cette disposition particulière est intéressante). De manière similaire au MDS, nous allons faire en sorte que des individus similaires soient proches sur la carte, et des individus différents soient éloignés. La division de la carte en différentes cellules permet de regrouper les individus. Ceci permet une classification comme pour la CAH ou les k-moyennes. Les SOM apparaissent donc comme une technique hybride entre ordination (représentation sur des cartes) et classification (regroupement des individus). La théorie et les calculs derrière les SOM sont très complexes. Elles font appel aux réseaux de neurones adaptatifs et leur fonctionnement est inspiré de celui du cerveau humain. Tout comme notre cerveau, les SOM vont utiliser l’information en entrée pour aller assigner une zone de traitement de l’information (pour notre cerveau) ou une cellule dans la carte (pour les SOM). Étant donné la complexité du calcul, les développement mathématiques n’ont pas leur place dans ce cours. Ce qui importe, c’est de comprendre le concept, et d’être ensuite capable d’utiliser les SOM à bon escient. Uniquement pour ceux d’entre vous qui désirent comprendre les détails du calcul, vous pouvez lire ici ou visionner la vidéo suivante (facultative et en anglais) : Plutôt que de détailler les calculs, nous vous montrons ici comment un ensemble de pixels de couleurs différentes est organisé sur une carte SOM de Kohonen en un arrangement infiniment plus cohérent… automatiquement (cet exemple est proposé par Frédéric De Lène Mirouze dans son blog). Image créée artificiellement avec disposition aléatoire des pixels. Carte SOM obtenue à partir de l’image précédente : les pixels sont automatiquement triés par couleur sur la carte. Ce qui est évident sur un exemple aussi visuel que celui-ci fonctionne aussi très bien pour ranger les individus dans un tableau multivarié a priori cahotique comme ceux que nous rencontrons régulièrement en statistiques multivariées en biologie. 6.3.1 SOM sur le zooplancton Reprenons notre exemple du zooplankton. zoo &lt;- read(&quot;zooplankton&quot;, package = &quot;data.io&quot;) zoo # # A tibble: 1,262 x 20 # ecd area perimeter feret major minor mean mode min max std_dev # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 0.770 0.465 4.45 1.32 1.16 0.509 0.363 0.036 0.004 0.908 0.231 # 2 0.700 0.385 2.32 0.728 0.713 0.688 0.361 0.492 0.024 0.676 0.183 # 3 0.815 0.521 4.15 1.33 1.11 0.598 0.308 0.032 0.008 0.696 0.204 # 4 0.785 0.484 4.44 1.78 1.56 0.394 0.332 0.036 0.004 0.728 0.218 # 5 0.361 0.103 1.71 0.739 0.694 0.188 0.153 0.016 0.008 0.452 0.110 # 6 0.832 0.544 5.27 1.66 1.36 0.511 0.371 0.02 0.004 0.844 0.268 # 7 1.23 1.20 15.7 3.92 1.37 1.11 0.217 0.012 0.004 0.784 0.214 # 8 0.620 0.302 3.98 1.19 1.04 0.370 0.316 0.012 0.004 0.756 0.246 # 9 1.19 1.12 15.3 3.85 1.34 1.06 0.176 0.012 0.004 0.728 0.172 # 10 1.04 0.856 7.60 1.89 1.66 0.656 0.404 0.044 0.004 0.88 0.264 # # … with 1,252 more rows, and 9 more variables: range &lt;dbl&gt;, size &lt;dbl&gt;, # # aspect &lt;dbl&gt;, elongation &lt;dbl&gt;, compactness &lt;dbl&gt;, transparency &lt;dbl&gt;, # # circularity &lt;dbl&gt;, density &lt;dbl&gt;, class &lt;fct&gt; Les 19 premières colonnes représentent des mesures réalisées sur notre plancton et la vingtième est la classe. Nous nous débarasserons de la colonne classe et transformons les données numériques en matrice après avoir standardisé les données (étapes obligatoires) pour stocker le résultat dans zoo_mat. zoo %&gt;.% select(., -class) %&gt;.% scale(.) %&gt;.% as.matrix(.) -&gt; zoo_mat Avant de pouvoir réaliser notre analyse, nous devons décider d’avance la topologie de la carte, c’est-à-dire, l’arrangement des cellules ainsi que le nombre de lignes et de colonnes. Le nombre de cellules totales choisies dépend à la fois du niveau de détails souhaité, et du nombre d’individus dans votre jeu de données (il faut naturellement plus de données que de cellules, disons, au moins 5 à 10 fois plus). Pour l’instant, considérons les deux topologies les plus fréquentes : la grille rectangulaire et la grille hexagonale. Plus le nombre de cellules est important, plus la carte sera détaillée, mais plus il nous faudra de données pour la calculer et la “peupler”. Considérons par exemple une grille 7 par 7 qui contient donc 49 cellules au total. Sachant que nous avons plus de 1200 particules de plancton mesurées dans zoo, le niveau de détail choisi est loin d’être trop ambitieux. La grille rectangulaire est celle qui vous vient probablement immédiatement à l’esprit. Il s’agit d’arranger les cellules en lignes horizontales et colonnes verticales. La fonction somgrid() du package kohonen permet de créer une telle grille. library(kohonen) # Charge le package kohonen # # Attaching package: &#39;kohonen&#39; # The following object is masked from &#39;package:purrr&#39;: # # map rect_grid_7_7 &lt;- somgrid(7, 7, topo = &quot;rectangular&quot;) # Crée la grille Il n’y a pas de graphique chart ou ggplot2 dans le package kohonen. Nous utiliserons ici les graphiques de base de R. Pour visualiser la grille, il faut la transformer en un objet kohonen. Nous pouvons ajouter plein d’information sur la grille. Ici, nous rajoutons une propriété calculée à l’aide de unit.distances() qui est la distance des cellules de la carte par référence à la cellule centrale. Les cellules sont numérotées de 1 à n en partant en bas à gauche, en progressant le long de la ligne du bas vers la droite, et en reprenant à gauche à la ligne au dessus. Donc, la ligne du bas contient de gauche à droite les cellules n°1 à 7. La ligne au dessus contient les cellules n°8 à 14, et ainsi de suite. La cellule du centre de la grille en en 4ème ligne en partant du bas et en position 4 sur cette ligne, soit trois lignes complètes plus quatre (\\(3*7+4=25\\)). C’est la cellule n°25. rect_grid_7_7 %&gt;.% # Transformation en un objet de classe kohonen qui est une liste structure(list(grid = .), class = &quot;kohonen&quot;) %&gt;.% # Objet de classe kohonen plot(., type = &quot;property&quot;, # Graphique de propriété property = unit.distances(rect_grid_7_7)[25, ], # distance à la cellule 25 main = &quot;Distance depuis la cellule centrale&quot;) # Titre du graphique Les cellules de la grille ne sont pas disposées au hasard dans la carte SOM. Des relations de voisinage sont utilisées pour placer les individus à représenter dans des cellules adjacentes s’ils se ressemblent. Avec une grille rectangulaire, nous avons donc deux modalités de variation : en horizontal et en vertival, ce qui donne deux gradients possibles qui, combinés donnent des extrêmes dans les coins opposés. Une cellule possède huits voisins directs. L’autre topologie possible est la grille hexagonale. Voyons ce que cela donne : hex_grid_7_7 &lt;- somgrid(7, 7, topo = &quot;hexagonal&quot;) hex_grid_7_7 %&gt;.% # Transformation en un objet de classe kohonen qui est une liste structure(list(grid = .), class = &quot;kohonen&quot;) %&gt;.% # Objet de classe kohonen plot(., type = &quot;property&quot;, # Graphique de propriété property = unit.distances(hex_grid_7_7)[25, ], # distance à la cellule 25 main = &quot;Distance depuis la cellule centrale&quot;) # Titre du graphique Ici, nous n’avons que six voisins directs, mais trois directions dans lesquelles les gradients peuvent varier : en horizontal, en diagonale vers la gauche et en diagonale vers la droite. Cela offre plus de possibilités pour l’agencement des individus. Nous voyons aussi plus de nuances dans les distances (il y a plus de couleurs différentes), pour une grille de même taille 7 par 7 que dans le cas de la grille rectangulaire. Nous utiliserons donc préférentiellement la grille hexagonale. Effectuons maintenant le calcul de notre SOM à l’aide de la fonction som() du package kohonen. Comme l’analyse fait intervenir le générateur pseudo-aléatoire, nous pouvons utiliser de manière optionnelle set.seed() avec un nombre choisi au hasard (et toujours différent à chaque utilisation) pour que cette analyse particulière-là soit reproductible. Sinon, à chaque exécution, nous obtiendrons un résultat légèrement différent. set.seed(8657) zoo_som &lt;- som(zoo_mat, grid = somgrid(7, 7, topo = &quot;hexagonal&quot;)) summary(zoo_som) # SOM of size 7x7 with a hexagonal topology and a bubble neighbourhood function. # The number of data layers is 1. # Distance measure(s) used: sumofsquares. # Training data included: 1262 objects. # Mean distance to the closest unit in the map: 2.519. Le résumé de l’objet ne nous donne pas beaucoup d’info. C’est normal. La technique étant visuelle, ce qui est important, c’est de représenter graphiquement la carte. Avec les graphiques R de base, la fonction utilisée est plot(). Nous avons plusieurs types disponibles et une large palette d’options. Voyez l’aide en ligne de?plot.kohonen. Le premier graphique (type = &quot;changes&quot;) montre l’évolution de l’apprentissage au fil des itérations. L’objectif est de descendre le plus possible sur l’axe des ordonnées pour réduire au maximum la distance des individus par rapport aux cellules (unit en anglais) où ils devraient se placer. Idéalement, nous souhaitons tendre vers zéro. En pratique, nous pourrons arrêter les itérations lorsque la courbe ne diminue plus de manière significative. plot(zoo_som, type = &quot;changes&quot;) Ici, il semble que nous ne diminuons plus vraiment à partir de la 85ème itération environ. Nous pouvons nous en convaincre en relançant l’analyse avec un plus grand nombre d’itérations (avec l’argument rlen = de som()). set.seed(954) zoo_som &lt;- som(zoo_mat, grid = somgrid(7, 7, topo = &quot;hexagonal&quot;), rlen = 200) plot(zoo_som, type = &quot;changes&quot;) Vous serez sans doute surpris de constater que la diminution de la courbe se fait plus lentement maintenant. En fait som() va adapter son taux d’apprentissage en fonction du nombre d’itérations qu’on lui donne et va alors “peaufiner le travail” d’autant plus. Au final, la valeur n’est pas plus basse pour autant. Donc, nous avons aboutit probablement à une solution. Le second graphique que nous pouvons réaliser consiste à placer les individus dans la carte, en utilisant éventuellement une couleur différente en fonction d’une caractéristique de ces individus (ici, leur classe). Ce graphique est obtenu avec type = &quot;mapping&quot;. Si vous ne voulez pas représenter la grille hexagonale à l’aide de cercles, vous pouvez spécifier shape = &quot;straight&quot;. Nous avons 17 classes de zooplancton et il est difficile de représenter plus de 10-12 couleurs distinctes, mais ce site propose une palette de 20 couleurs distinctes. Nous en utiliserons les 17 premières… colors17 &lt;- c(&quot;#e6194B&quot;, &quot;#3cb44b&quot;, &quot;#ffe119&quot;, &quot;#4363d8&quot;, &quot;#f58231&quot;, &quot;#911eb4&quot;, &quot;#42d4f4&quot;, &quot;#f032e6&quot;, &quot;#bfef45&quot;, &quot;#fabebe&quot;, &quot;#469990&quot;, &quot;#e6beff&quot;, &quot;#9A6324&quot;, &quot;#fffac8&quot;, &quot;#800000&quot;, &quot;#aaffc3&quot;, &quot;#808000&quot;, &quot;#ffd8b1&quot;) plot(zoo_som, type = &quot;mapping&quot;, shape = &quot;straight&quot;, col = colors17[zoo$class]) Nous n’avons pas ajouté de légende qui indique à quelle classe correspond quelle couleur. Ce que nous voulons voir, c’est si les cellules arrivent à séparer les classes. Nous voyons que la séparation est imparfaite, mais des tendances apparaissent avec certaines couleurs qui se retrouvent plutôt dans une région de la carte. Nous voyons donc ici que, malgré que l’information contenue dans class n’ait pas été utilisées. Les différents individus de zooplancton ne se répartissent pas au hasard en fonction de ce critère. Nous pouvons également voir les cellules qui contiennent plus ou moins d’individus, mais si l’objectif est de visionner uniquement le remplissage des cellules, le type = &quot;counts&quot; est plus adapté. plot(zoo_som, type = &quot;counts&quot;, shape = &quot;straight&quot;) Nous pouvons obtenir la cellule dans laquelle chaque individu est mappé comme suit : zoo_som$unit.classif # [1] 19 6 25 17 15 19 40 17 40 19 41 39 6 32 17 14 32 22 40 29 26 24 26 # [24] 20 38 19 20 39 19 5 25 2 19 19 43 20 29 23 36 10 23 5 30 16 17 18 # [47] 23 17 12 18 11 1 16 17 12 16 10 45 16 17 39 2 19 32 6 45 32 5 32 # [70] 16 6 2 9 46 4 14 14 6 17 32 19 4 6 4 6 40 9 12 45 16 6 45 # [93] 7 49 5 10 10 16 10 1 10 17 10 10 36 10 16 36 25 9 31 2 11 12 10 # [116] 23 15 10 10 25 1 1 10 16 13 16 17 17 38 10 16 25 10 2 10 16 25 11 # [139] 18 10 1 36 14 7 40 41 48 40 24 6 4 31 12 32 19 35 39 45 19 40 25 # [162] 24 1 30 4 7 18 43 18 12 30 30 17 17 19 31 36 30 36 30 11 11 16 26 # [185] 16 11 11 25 18 11 11 19 29 11 30 36 14 24 14 18 14 18 25 26 4 39 25 # [208] 10 19 11 18 10 15 19 11 32 20 30 6 36 6 12 14 18 10 1 11 18 12 18 # [231] 1 7 30 30 30 18 23 23 11 3 30 16 1 38 30 1 1 30 11 30 25 36 1 # [254] 30 11 11 36 1 30 1 36 30 30 1 5 18 30 30 34 34 47 20 20 47 25 38 # [277] 27 26 39 20 19 34 19 17 34 11 18 47 27 34 39 27 26 19 19 10 32 32 34 # [300] 31 31 19 33 33 20 36 27 31 47 20 32 39 46 11 33 34 47 19 34 27 30 38 # [323] 29 25 31 36 25 32 1 29 1 11 11 32 31 28 19 30 31 32 36 12 17 36 18 # [346] 17 39 38 38 25 36 36 19 10 37 30 18 11 19 18 19 19 30 17 18 31 6 33 # [369] 12 20 18 11 18 20 20 18 11 23 20 19 11 27 45 19 5 20 19 14 20 20 20 # [392] 5 29 11 26 20 18 39 20 18 4 23 18 25 10 11 11 38 11 18 17 38 43 18 # [415] 18 11 18 5 26 24 45 43 32 45 7 38 39 18 3 25 45 39 41 17 19 15 3 # [438] 46 10 26 45 33 28 22 39 8 30 3 43 20 33 7 41 39 16 39 22 30 38 7 # [461] 3 25 30 3 38 3 17 37 3 3 18 37 38 15 39 22 15 5 39 3 16 16 16 # [484] 30 23 3 3 22 31 39 38 45 15 28 3 28 15 43 39 38 3 29 23 3 23 29 # [507] 18 16 42 42 24 42 40 35 6 44 23 3 42 26 45 35 42 26 18 8 44 3 44 # [530] 44 49 15 28 16 3 5 43 10 29 10 8 26 43 16 23 14 42 33 3 12 35 41 # [553] 33 22 32 35 28 42 3 31 18 24 44 24 49 16 22 25 15 7 8 23 23 29 37 # [576] 1 23 15 3 34 44 44 37 40 29 46 43 43 44 41 20 42 43 24 4 28 35 49 # [599] 3 3 23 15 3 15 15 23 17 28 15 43 43 23 3 23 3 3 3 3 28 3 17 # [622] 3 17 15 28 42 28 39 3 28 44 32 33 28 9 33 39 41 22 22 9 38 28 28 # [645] 42 28 28 43 2 30 38 1 36 9 23 17 17 25 28 39 39 28 28 30 3 28 30 # [668] 32 26 37 30 22 39 28 22 14 30 30 46 35 28 3 3 3 22 27 30 43 3 3 # [691] 15 29 25 3 37 29 37 29 29 23 3 34 10 24 34 27 17 24 9 8 33 47 40 # [714] 32 2 2 34 33 20 34 33 38 33 47 26 9 33 34 9 39 2 32 34 27 8 47 # [737] 26 34 27 33 28 8 40 2 45 24 34 39 43 17 31 32 23 37 27 9 9 17 9 # [760] 15 45 37 37 31 17 8 17 45 28 28 19 29 25 7 39 19 9 9 43 41 24 40 # [783] 9 29 8 24 2 42 8 24 43 8 2 48 8 8 14 24 20 17 28 8 37 40 45 # [806] 7 7 37 32 46 21 37 7 41 45 40 39 9 17 23 37 7 10 16 16 17 23 30 # [829] 16 9 38 15 43 38 15 16 15 38 23 36 37 7 29 9 23 9 17 17 17 17 37 # [852] 39 24 19 32 35 35 44 20 19 23 20 19 17 44 42 45 40 20 24 44 33 45 19 # [875] 33 46 19 44 33 39 32 39 26 39 38 30 23 30 37 23 20 17 38 39 31 31 29 # [898] 19 12 23 37 30 38 25 30 16 38 37 12 45 16 23 38 31 7 39 25 46 26 44 # [921] 35 14 19 39 42 19 19 38 40 14 44 45 40 24 35 39 28 21 48 46 45 32 32 # [944] 16 44 22 39 43 38 39 46 32 32 25 38 7 23 23 12 23 30 43 22 30 29 23 # [967] 16 23 38 37 37 40 24 40 26 19 24 22 37 14 28 46 6 26 27 44 44 24 44 # [990] 45 24 46 26 32 24 45 44 37 39 32 24 42 40 30 40 40 46 23 33 15 5 23 # [1013] 23 37 44 12 43 23 44 42 16 26 44 35 38 42 45 24 35 43 26 20 23 42 43 # [1036] 33 40 44 45 45 44 24 43 46 25 32 42 46 4 24 32 7 23 25 37 17 7 22 # [1059] 23 29 23 15 10 29 38 37 37 35 40 42 39 45 42 24 42 42 44 26 35 46 35 # [1082] 39 42 20 46 42 26 26 14 5 19 46 24 42 35 26 40 40 33 26 24 42 35 24 # [1105] 12 46 42 45 42 42 42 19 24 11 46 5 13 8 13 12 10 17 32 10 15 7 28 # [1128] 11 39 20 10 7 28 32 18 4 11 18 12 45 28 18 45 33 26 28 28 5 11 7 # [1151] 18 28 7 5 5 7 7 18 18 18 18 7 16 18 5 5 16 28 43 32 45 27 5 # [1174] 22 29 29 7 36 6 29 5 5 7 5 29 11 16 5 7 11 7 7 7 7 31 2 # [1197] 8 4 9 8 28 6 2 30 9 8 4 10 8 8 4 9 31 20 11 4 45 2 4 # [1220] 8 1 2 1 31 1 11 10 17 5 8 8 25 9 8 1 1 10 1 1 1 1 23 # [1243] 36 25 10 1 1 1 10 10 1 36 1 25 6 36 2 36 37 43 45 38 Par conséquent, nous pouvons créer un tableau de contingence qui répertorie le nombre d’iundividus mappés dans chaque cellule à l’aide de table(). Nous l’enregistrons dans zoo_som_nb car nous la réutiliserons plus tard. zoo_som_nb &lt;- table(zoo_som$unit.classif) zoo_som_nb # # 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 33 17 40 15 24 17 32 24 23 38 38 18 3 16 24 35 42 43 44 31 2 17 46 33 30 # 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 # 28 13 37 26 45 20 35 23 17 18 24 29 33 42 26 9 30 27 26 34 20 8 3 4 6.3.2 Interprétation d’un SOM De nombreuses autres présentations graphiques sont possibles sur cette base. Nous allons explorer deux aspects complémentaires : (1) représentation des variables, et (2) réalisation et représentation de regroupements. 6.3.2.1 Représentation des variables La carte SOM est orientée. C’est-à-dire que les cellules représentent des formes différentes de plancton telles qu’exprimées à travers les 19 variables utilisées ici (quantification de la taille, de la forme, de la transparence, …). Le graphique type = &quot;codes&quot; permet de visualiser ces différences de manière générale : plot(zoo_som, type = &quot;codes&quot;, codeRendering = &quot;segments&quot;) Ce graphique est riche en informations. Nous voyons que : les très grands individus (ecd, area, perimeter, etc.), soit les segments verts sont en haut à droite de la carte et les petits sont à gauche, les individus opaques (variables mean, mode, max, etc.18), soit des segments dans les tons jaunes sont en bas à droite. Les organismes plus transparents sont en haut à gauche, au delà de ces deux principaux critères qui se dégagement prioritairement, les aspects de forme (segments rose-rouges) se retrouvent exprimés moins nettement le long de gradients. La circularity mesure la silhouette plus ou moins arrondie des items (sa valeur est d’autant plus élevée que la forme se rapproche d’un cercle). Les organismes circulaires se retrouvent dans le bas de la carte. L’elongation et l’aspect mesurent l’allongement de la particule et se retrouvent plutôt exprimés positivement vers le haut de la carte. Nous pouvons donc orienter notre carte SOM en indiquant l’information relative aux variables. Lorsque le nombre de variables est élevé ou relativement élevé comme ici, cela devient néanmoins difficile à lire. Il est aussi possible de colorer les cartes en fonction d’une et une seule variable pour en faciliter la lecture à l’aide de type = &quot;property&quot;. Voici quelques exemples (notez la façon de diviser une page graphique en lignes et colonnes à l’aide de par(mfrow = )) en graphiques R de base, ensuite une boucle for réalise les six graphiques l’un après l’autre) : par(mfrow = c(2, 3)) for (var in c(&quot;size&quot;, &quot;mode&quot;, &quot;range&quot;, &quot;aspect&quot;, &quot;elongation&quot;, &quot;circularity&quot;)) plot(zoo_som, type = &quot;property&quot;, property = zoo_som$codes[[1]][, var], main = var, palette.name = viridis::inferno) Nous pouvons plus facilement inspecter les zones d’influence de différentes variables ciblées. Ici, size est une mesure de la taille des particules, mode est le niveau d’opacité moyen, range est la variation d’opacité (un range important indique que la particule a des parties très transparentes et d’autres très opaques), aspect est le rapport longueur/largeur, elongation est une indication de la complexité du périmètre de la particule, et circularity est sa forme plus ou moins circulaire. Pour une explication détaillée des 19 variables, faites ?zooplankton. 6.3.2.2 Regroupements Lorsque nous avons réalisé une CAH sur le jeu de données zooplankton, nous étions obligés de choisir deux variables parmi les 19 pour visualiser le regroupement sur un graphique nuage de points. C’est peu, et cela ne permet pas d’avoir une vision synthétique sur l’ensemble de l’information. Les méthodes d’ordination permettent de visualiser plus d’information sur un petit nombre de dimensions grâce aux techniques de réduction des dimensions qu’elles implémentent. Les cartes SOM offrent encore un niveau supplémentaire de raffinement. Nous pouvons considérer que chaque cellule est un premier résumé des données et nous pouvons effectuer ensuite une CAH sur ces cellules afin de dégager un regroupement et le visualiser sur la carte SOM. L’intérêt est que l’on réduit un jeu de données potentiellement très volumineux à un nombre plus restreint de cellules (ici 7x7 = 49), ce qui est plus “digeste” pour la CAH. Voici comment ça fonctionne : zoo_som_dist &lt;- dist(zoo_som$codes[[1]]) # Distance euclidienne entre cellules zoo_som_cah &lt;- hclust(zoo_som_dist, method = &quot;ward.D2&quot;, members = zoo_som_nb) Notre CAH a été réalisée ici avec la méthode D2 de Ward. L’argument members = est important. Il permet de pondérer chaque cellule en fonction du nombre d’individus qui y sont mappés. Toutes les cellules n’ont pas un même nombre d’individus, et nous souhaitons mettre plus de poids dans l’analyse aux cellules les plus remplies. Voici le dendrogramme : plot(zoo_som_cah, hang = -1) abline(h = 11.5, col = &quot;red&quot;) # Niveau de coupure proposé Les V1 à V49 sont les numéros de cellules. Nous pouvons couper à différents endroits dans ce dendrogramme, mais si nous décidons de distringuer les cinq groupes correspondants au niveau de coupure à une hauteur de 11,5 (comme sur le graphique), voici ce que cela donne : groupes &lt;- cutree(zoo_som_cah, h = 11.5) groupes # V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 # 1 1 1 1 2 2 2 1 1 1 2 2 2 2 1 1 1 2 # V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 # 2 2 2 1 1 1 1 2 3 3 1 1 1 3 3 3 4 1 # V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 # 3 3 3 3 4 4 3 3 3 3 4 4 5 Visualisons ce découpage sur la carte SOM (l’argument bgcol = colorie le fond des cellules en fonction des groupes19, et add.cluster.boudaries() individualise des zones sur la carte en fonction du regroupement choisi). plot(zoo_som, type = &quot;mapping&quot;, pch = &quot;.&quot;, main = &quot;SOM zoo, 5 groupes&quot;, bgcol = RColorBrewer::brewer.pal(5, &quot;Set2&quot;)[groupes]) add.cluster.boundaries(zoo_som, clustering = groupes) Grâce à la topographie des variables que nous avons réalisée plus haut, nous savons que : le groupe vert bouteille en bas à gauche reprend les petites particules plutôt transparentes, le groupe orange en bas à droite est constituée de particules très contrastées avec des parties opaques et d’autres transparentes (range important), le groupe du dessus en bleu est constitué de particules petites à moyennes ayant une forme complexe (variable elongation), le groupe rose est constitué des particules moyennes à grandes, le groupe vert clair d’une seule cellule en haut à droite reprend les toutes grandes particules. Nous n’avons fait qu’effleurer les nombreuses possibilités des cartes auto-adaptatives SOM… Il est par exemple possible d’aller mapper des nouveaux individus dans cette carte (données supplémentaires), ou même de faire une classification sur base d’exemples (classification supervisée) que nous verrons au cours de Science des Données Biologiques III. Nous espérons que cela vous donnera l’envie et la curiosité de tester cette méthode sur vos données et d’explorer plus avant ses nombreuses possibilités. Pour en savoir plus Une explication très détaillée en français accompagnée de la résolution d’un exemple fictif dans R. Une autre explication détaillée en français avec exemple dans R. Si vous êtes aventureux, vous pouvez vous lancer dans la réimplémentation des graphiques du package kohonen en chartou ggplot2. Voici un bon point de départ (en anglais). A vous de jouer ! Réalisez le tutoriel afin de vérifier votre bonne compréhension de la som. Démarrez la SciViews Box et RStudio. Dans la fenêtre Console de RStudio, entrez l’instruction suivante suivie de la touche Entrée pour ouvrir le tutoriel concernant les bases de R : BioDataScience2::run(&quot;06c_som&quot;) N’oubliez pas d’appuyer sur la touche ESC pour reprendre la main dans R à la fin d’un tutoriel dans la console R. Complétez votre carnet de note par binôme sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Completez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea Attention : la variable transparency, contrairement à ce que son nom pourrait suggérer n’est pas une mesure de la transparence de l’objet, mais de l’aspect plus ou moins régulier et lisse de sa silhouette.↩ Nous avons choisi ici encore une autre palette de couleurs provenant du package RColorBrewer, voir ici.↩ "],
["acp-afc.html", "Module 7 ACP &amp; AFC", " Module 7 ACP &amp; AFC Objectifs Apprendre à réaliser une ordination de données quantitatives à l’aide de l’ACP. Savoir ordiner des variables qualitatives sous forme de tableaux cas par variables ou de tables de contingences à double entrée à l’aide de l’AFC. Appréhender l’accès aux bases de données depuis R et RStudio, en particulier via des requêtes SQL simples ou avec dbplyr. Prérequis Le module 6, et en particulier la partie sur le MDS doivent être assimilés avant d’attaquer le présent module. A vous de jouer ! En lien avec ce module vous avez une série d’exercices à réaliser. Vous avez à : Réaliser un projet spécifique et dédié uniquement au module 07. Ce module couvre l’ensemble de la matière du module 7. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/sWXOnhcQ Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd2_module07. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. Ce projet doit être terminé à la fin de ce module "],
["analyse-en-composantes-principales.html", "7.1 Analyse en composantes principales", " 7.1 Analyse en composantes principales Notre première approche d’ordination avec le MDS dans le précédent module nous a permis de comprendre l’intérêt de représenter des données multivariées sur des cartes. Malheureusement, les techniques itératives et basées sur les matrices de distances du MDS rendent cette technique peu propice pour analyser des gros jeux de données. En effet, le temps de calcul et le besoin en mémoire vive grandissent de manière exponentielle avec la taille des jeux de données. Heureusement, il existe aussi des techniques d’ordination qui se calculent plus facilement et plus rapidement sur de très gros jeux de données. L’Analyse en Composantes Principales ou ACP (“Principal Component Analysis” ou PCA en anglais) est une méthode de base qu’il est indispensable de connaitre et de comprendre. La plupart des autres techniques d’ordination plus sophistiquées sont des variante de l’ACP. Des relations linéaires sont suspectées entres les variables (si elles ne sont pas linéaires, penser à transformer les données auparavant pour les linéairser). Ces relations conduisent à une répartition des individus (le nuage de points) qui forme une structure que l’on cherchera à interpréter. Pour visualiser cette structure, les données sont simplifiées (réduites) de N variables à n (n &lt; N et n = 2 ou 3 généralement). La représentation sous forme d’un nuage de points s’appelle une carte. La réduction des dimensions se fait avec une perte minimale d’information au sens de la variance des données. 7.1.1 ACP dans SciViews::R L’ACP est facilitée dans SciViews::R, mais au stade actuel, tout le code nécessaire (en particulier pour réaliser les graphiques avec chart()) n’est pas encore complètement intégré dans les packages. Ainsi, vous pouvez copier-coller le code du chunk suivant au début de vos scripts ou dans un chunk de setup dans vos documenbts R Markdown/Notebook. SciViews::R() library(broom) # broom implements only methods for prcomp objects, not princomp, while pcomp # is compatible with princomp... but prcomp is simpler. So, conversion is easy as.prcomp &lt;- function(x, ...) UseMethod(&quot;as.prcomp&quot;) as.prcomp.default &lt;- function(x, ...) stop(&quot;No method to convert this object into a &#39;prcomp&#39;&quot;) as.prcomp.prcomp &lt;- function(x, ...) x as.prcomp.princomp &lt;- function(x, ...) structure(list(sdev = as.numeric(x$sdev), rotation = unclass(x$loadings), center = x$center, scale = x$scale, x = as.matrix(x$scores)), class = &quot;prcomp&quot;) # Comparison of pcomp() -&gt; as.prcomp() with prcomp() directly # Almost the same, only no rownames for x (is it important?) #iris_prcomp_pcomp &lt;- as.prcomp(pcomp(iris[, -5], scale = TRUE)) #iris_prcomp &lt;- prcomp(iris[, -5], scale = TRUE) # Now, broom methods can be defined simply by converting into prcomp objects augment.princomp &lt;- function(x, data = NULL, newdata, ...) if (missing(newdata)) { augment(as.prcomp(x), data = data, ...) } else { augment(as.prcomp(x), data = data, newdata = newdata, ...) } tidy.princomp &lt;- function(x, matrix = &quot;u&quot;, ...) tidy(as.prcomp(x), matrix = matrix, ...) # There is no glance.prcomp() method # There is a problem with pcomp() that returns a data.frame in scores, # while it is a matrix in the original princomp object. pca() corrects this pca &lt;- function(x, ...) { res &lt;- SciViews::pcomp(x, ...) # Change scores into a matrix res$scores &lt;- as.matrix(res$scores) res } scale_axes &lt;- function(data, aspect.ratio = 1) { range_x &lt;- range(data[, 1]) span_x &lt;- abs(max(range_x) - min(range_x)) range_y &lt;- range(data[, 2]) span_y &lt;- abs(max(range_y) - min(range_y)) if ((span_y / aspect.ratio) &gt; span_x) { # Adjust range_x span_x_2 &lt;- span_y / aspect.ratio / 2 range_x_mid &lt;- sum(range_x) / 2 range_x &lt;- c(range_x_mid - span_x_2, range_x_mid + span_x_2) } else { # Adjust range_y span_y_2 &lt;- span_x * aspect.ratio / 2 range_y_mid &lt;- sum(range_y) / 2 range_y &lt;- c(range_y_mid - span_y_2, range_y_mid + span_y_2) } list(x = range_x, y = range_y) } autoplot.pcomp &lt;- function(object, type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;loadings&quot;, &quot;correlations&quot;, &quot;scores&quot;, &quot;biplot&quot;), choices = 1L:2L, name = deparse(substitute(object)), ar.length = 0.1, circle.col = &quot;gray&quot;, col = &quot;black&quot;, fill = &quot;gray&quot;, scale = 1, aspect.ratio = 1, repel = FALSE, labels, title, xlab, ylab, ...) { type = match.arg(type) if (missing(title)) title &lt;- paste(name, type, sep = &quot; - &quot;) contribs &lt;- paste0(names(object$sdev), &quot; (&quot;, round((object$sdev^2/object$totdev^2) * 100, digits = 1), &quot;%)&quot;)[choices] scores &lt;- as.data.frame(object$scores[, choices]) names(scores) &lt;- c(&quot;x&quot;, &quot;y&quot;) if (!missing(labels)) { if (length(labels) != nrow(scores)) stop(&quot;You must provide a character vector of length &quot;, nrow(scores), &quot; for &#39;labels&#39;&quot;) scores$labels &lt;- labels } else {# Default labels are row numbers scores$labels &lt;- 1:nrow(scores) } lims &lt;- scale_axes(scores, aspect.ratio = aspect.ratio) if (!missing(col)) { if (length(col) != nrow(scores)) stop(&quot;You must provide a vector of length &quot;, nrow(scores), &quot; for &#39;col&#39;&quot;) scores$color &lt;- col scores_formula &lt;- y ~ x %col=% color %label=% labels } else { if (missing(labels)) { scores_formula &lt;- y ~ x %label=% labels } else { scores_formula &lt;- y ~ x %col=% labels %label=% labels } } res &lt;- switch(type, screeplot = object %&gt;.% # Classical screeplot tidy(., &quot;pcs&quot;) %&gt;.% chart(data = ., std.dev^2 ~ PC) + geom_col(col = col, fill = fill) + labs(y = &quot;Variances&quot;, title = title), altscreeplot = object %&gt;.% # screeplot represented by dots and lines tidy(., &quot;pcs&quot;) %&gt;.% chart(data = ., std.dev^2 ~ PC) + geom_line(col = col) + geom_point(col = &quot;white&quot;, fill = col, size = 2, shape = 21, stroke = 3) + labs(y = &quot;Variances&quot;, title = title), loadings = object %&gt;.% # Plots of the variables tidy(., &quot;variables&quot;) %&gt;.% spread(., key = PC, value = value) %&gt;.% #rename_if(., is.numeric, function(x) paste0(&quot;PC&quot;, x)) %&gt;.% select(., c(1, choices + 1)) %&gt;.% set_names(., c(&quot;labels&quot;, &quot;x&quot;, &quot;y&quot;)) %&gt;.% chart(data = ., y ~ x %xend=% 0 %yend=% 0 %label=% labels) + annotate(&quot;path&quot;, col = circle.col, x = cos(seq(0, 2*pi, length.out = 100)), y = sin(seq(0, 2*pi, length.out = 100))) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + geom_segment(arrow = arrow(length = unit(ar.length, &quot;inches&quot;), ends = &quot;first&quot;)) + ggrepel::geom_text_repel(hjust = &quot;outward&quot;, vjust = &quot;outward&quot;) + coord_fixed(ratio = 1) + labs(x = contribs[1], y = contribs[2], title = title), correlations = object %&gt;.% # Correlations plot Correlation(.) %&gt;.% as_tibble(., rownames = &quot;labels&quot;) %&gt;.% select(., c(1, choices + 1)) %&gt;.% set_names(., c(&quot;labels&quot;, &quot;x&quot;, &quot;y&quot;)) %&gt;.% chart(data = ., y ~ x %xend=% 0 %yend=% 0 %label=% labels) + annotate(&quot;path&quot;, col = circle.col, x = cos(seq(0, 2*pi, length.out = 100)), y = sin(seq(0, 2*pi, length.out = 100))) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + geom_segment(arrow = arrow(length = unit(ar.length, &quot;inches&quot;), ends = &quot;first&quot;)) + ggrepel::geom_text_repel(hjust = &quot;outward&quot;, vjust = &quot;outward&quot;) + coord_fixed(ratio = 1) + labs(x = contribs[1], y = contribs[2], title = title), scores = scores %&gt;.% # Plot of the individuals chart(data = ., scores_formula) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + coord_fixed(ratio = 1, xlim = lims$x, ylim = lims$y, expand = TRUE) + labs(x = contribs[1], y = contribs[2], title = title) + theme(legend.position = &quot;none&quot;), biplot = object %&gt;.% # Biplot using ggfortify function as.prcomp(.) %&gt;.% ggfortify:::autoplot.prcomp(., x = choices[1], y = choices[2], scale = scale, size = -1, label = TRUE, loadings = TRUE, loadings.label = TRUE) + geom_hline(yintercept = 0, col = circle.col) + geom_vline(xintercept = 0, col = circle.col) + theme_sciviews() + labs(x = contribs[1], y = contribs[2], title = title), stop(&quot;Unrecognized type, must be &#39;screeplot&#39;, &#39;altscreeplot&#39;, loadings&#39;, &#39;correlations&#39;, &#39;scores&#39; or &#39;biplot&#39;&quot;) ) if (type == &quot;scores&quot;) { if (isTRUE(repel)) { res &lt;- res + geom_point() + ggrepel::geom_text_repel() } else {# Use text res &lt;- res + geom_text() } } if (!missing(xlab)) res &lt;- res + xlab(xlab) if (!missing(ylab)) res &lt;- res + ylab(ylab) res } chart.pcomp &lt;- function(data, choices = 1L:2L, name = deparse(substitute(data)), ..., type = NULL, env = parent.frame()) autoplot.pcomp(data, choices = choices, name = name, ..., type = type, env = env) class(chart.pcomp) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) 7.1.2 Indiens diabétiques Les indiens Pimas sont des amérindiens originaires du nord du Mexique qui sont connus pour compter le plus haut pourcentage d’obèses et de diabétiques de toutes les éthnies. Ils ont fait l’objet de plusieurs études scientifiques d’autant plus que les Pimas en Arizona développent principalement cette obésité et ce diabète, alors que les Pimas mexicains les ont plus rarement. Il est supposé que leur mode de vie différent aux Etats_Units pourrait en être la raison. Voici un jeu de données qui permet d’explorer un peu ceci : pima &lt;- read(&quot;PimaIndiansDiabetes2&quot;, package = &quot;mlbench&quot;) pima # # A tibble: 768 x 9 # pregnant glucose pressure triceps insulin mass pedigree age diabetes # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 6 148 72 35 NA 33.6 0.627 50 pos # 2 1 85 66 29 NA 26.6 0.351 31 neg # 3 8 183 64 NA NA 23.3 0.672 32 pos # 4 1 89 66 23 94 28.1 0.167 21 neg # 5 0 137 40 35 168 43.1 2.29 33 pos # 6 5 116 74 NA NA 25.6 0.201 30 neg # 7 3 78 50 32 88 31 0.248 26 pos # 8 10 115 NA NA NA 35.3 0.134 29 neg # 9 2 197 70 45 543 30.5 0.158 53 pos # 10 8 125 96 NA NA NA 0.232 54 pos # # … with 758 more rows Ce jeu de données contient des vlaeurs manquantes. Le graphique suivant permet de visualiser l’importance des “dégâts” : naniar::vis_miss(pima) Moins de 10% des données sont manquantes, et c’est principalement dans les variables insulin et triceps. Si nous souhaitons un tableau sans variables manquantes, nous pouvons décider d’éliminer des lignes et ou des colonnes (variables), mais ici nous souhaitons garder toutes les variables et réduisons donc uniquement le nombre de lignes avec la fonction drop_na(). pima &lt;- drop_na(pima) pima # # A tibble: 392 x 9 # pregnant glucose pressure triceps insulin mass pedigree age diabetes # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; # 1 1 89 66 23 94 28.1 0.167 21 neg # 2 0 137 40 35 168 43.1 2.29 33 pos # 3 3 78 50 32 88 31 0.248 26 pos # 4 2 197 70 45 543 30.5 0.158 53 pos # 5 1 189 60 23 846 30.1 0.398 59 pos # 6 5 166 72 19 175 25.8 0.587 51 pos # 7 0 118 84 47 230 45.8 0.551 31 pos # 8 1 103 30 38 83 43.3 0.183 33 neg # 9 1 115 70 30 96 34.6 0.529 32 pos # 10 3 126 88 41 235 39.3 0.704 27 neg # # … with 382 more rows Notre tableau est presque amputé de la moitié, mais il nous reste tout de même encore 392 cas, soit assez pour notre analyse. Avant de nous lancer dans une ACP, nous devons décrire les données, repérer les variables quantitatives d’intérêt, et synthétiser les corrélations linéaires (coefficients de corrélation de Pearson) entre ces variables. skimr::skim(pima) # Skim summary statistics # n obs: 392 # n variables: 9 # # ── Variable type:factor ─────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # diabetes 0 392 392 2 neg: 262, pos: 130, NA: 0 FALSE # # ── Variable type:numeric ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 # age 0 392 392 30.86 10.2 21 23 27 36 # glucose 0 392 392 122.63 30.86 56 99 119 143 # insulin 0 392 392 156.06 118.84 14 76.75 125.5 190 # mass 0 392 392 33.09 7.03 18.2 28.4 33.2 37.1 # pedigree 0 392 392 0.52 0.35 0.085 0.27 0.45 0.69 # pregnant 0 392 392 3.3 3.21 0 1 2 5 # pressure 0 392 392 70.66 12.5 24 62 70 78 # triceps 0 392 392 29.15 10.52 7 21 29 37 # p100 hist # 81 ▇▂▂▁▁▁▁▁ # 198 ▁▅▇▆▅▃▂▂ # 846 ▇▆▂▁▁▁▁▁ # 67.1 ▂▆▇▅▂▁▁▁ # 2.42 ▇▆▃▁▁▁▁▁ # 17 ▇▃▂▁▁▁▁▁ # 110 ▁▁▂▆▇▆▁▁ # 63 ▃▆▇▇▆▃▁▁ Nous avons une variable facteur diabetes à exclure de l’analyse, mais la variable pregnant, est une variable numérique discrète (nombre d’enfants portés). Nous l’éliminerons aussi de l’analyse. La fonction correlation() du package SciViews nous permet d’inspecter les corrélations entre les variables choisies (donc toutes à l’exception de pregnant et diabetes qui ne sont pas quantitatives continues) : pima_cor &lt;- correlation(pima[, 2:8]) knitr::kable(pima_cor, digits = 2) glucose pressure triceps insulin mass pedigree age glucose 1.00 0.21 0.20 0.58 0.21 0.14 0.34 pressure 0.21 1.00 0.23 0.10 0.30 -0.02 0.30 triceps 0.20 0.23 1.00 0.18 0.66 0.16 0.17 insulin 0.58 0.10 0.18 1.00 0.23 0.14 0.22 mass 0.21 0.30 0.66 0.23 1.00 0.16 0.07 pedigree 0.14 -0.02 0.16 0.14 0.16 1.00 0.09 age 0.34 0.30 0.17 0.22 0.07 0.09 1.00 plot(pima_cor) Quelques corrélations positives d’intensités moyennes se dégagent ici, notamment entre mass et triceps (épaisseur du pli cutané au niveau du triceps), ainsi qu’entre glucose (taux de glucose dans le sang), insulin (taux d’insuline dans le sang) et age. Par contre, la pression artérielle (pressure) et le pedigree (variable qui quantifie la susceptibilité au diabète en fonction de la parenté) semblent peu corrélés avec les autres variables. L’ACP est en fait équivalente à une Analyse en Coordonnées Principales sur une matrice de distances euclidiennes (MDS métrique), mais en plus efficace en terme de calculs. Nous pouvons donc nous lancer dans l’analyse et en comprendre les résultats en gardant ceci à l’esprit. Nous utiliserons la fonction pca() qui prend un argument data = et une formule du type ~ var1 + var2 + .... + varn, ou plus simplement, directement un tableau contenant uniquement les variables à analyser comme argument unique. Comme les différentes variables sont mesurées dans des unités différentes, nous devons les standardiser (écart type ramené à un pour toutes). Ceci est réalisé par la fonction pca() en lui indiquant scale = TRUE. Donc : pima_pca &lt;- pca(data = pima, ~ glucose + pressure + triceps + insulin + mass + pedigree + age, scale = TRUE) Ou alors, nous sélectionnons les variables d’intérêt avec select() et appliquons pca() directement sur ce tableau, ce qui donnera le même résultat. pima %&gt;.% select(., glucose:age) %&gt;.% pca(., scale = TRUE) -&gt; pima_pca Le nuage de points dans l’espace initial à sept dimensions a été centré (origine ramenée au centre de gravité du nuage de points = moyenne des variables) par l’ACP. Ensuite une rotation des axes a été réalisée pour orienter son plus grand axe selon un premier axe principal 1 ou PC1 . Ensuite PC2 est construit orthogonal au premier et dans la seconde direction de plus grande variabilité du nuage de points, et ainsi de suite pour les autres axes. Ainsi les axes PC1, PC2, PC3, … représentent une part de variance de plus en plus faible par rapport à la variance totale du jeu de données. Ceci est présenté dans le résumé : summary(pima_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # Variance 2.412 1.288 1.074 0.878 0.6389 0.399 0.3098 # Proportion of Variance 0.345 0.184 0.153 0.126 0.0913 0.057 0.0443 # Cumulative Proportion 0.345 0.529 0.682 0.807 0.8988 0.956 1.0000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # glucose 0.441 -0.455 -0.198 0.736 # pressure 0.329 0.101 -0.613 0.206 0.654 -0.171 # triceps 0.439 0.488 -0.367 -0.644 # insulin 0.402 -0.418 0.263 -0.388 0.123 -0.642 -0.129 # mass 0.446 0.506 -0.181 0.711 # pedigree 0.198 0.625 0.711 0.251 # age 0.325 -0.337 -0.384 0.471 -0.592 -0.168 0.179 Le premier tableau Importance of components (eigenvalues): montre la part de variance présentée sur chacun des sept axes de l’ACP (PC1, PC2, …, PC7). Le fait qu’il s’agit de valeurs propres (eigenvalues en anglais) apparaitra plus clair lorsque vous aurez lu les explications détaillées plus bas. Ces parts de variance s’additionnent pour donner la variance totale du nuage de points dans les sept dimensions (propriété d’additivité des variances). Pour facilité la lecture, la Proportion de Variance en % est reprise également, ainsi que les proportions cumulées. Ainsi, les deux premiers axes de l’ACP capturent ici 53% de la variance totale. Et il faudrait considérer les cinq premiers axes pour capturer 90% de la variance totale. Cependant, les trois premiers axes cumulent tout de même plus des 2/3 de la variance. Nous pouvons restreindre notre analyse à ces trois axes-là. Le second tableau Loadings (eigenvectors, rotation matrix): est la matrice de transformation des coordonnées initiales sur les lignes en coordonnées PC1 à PC7 en colonnes. Nous pouvons y lire l’importante des variables initiales sur les axes de l’ACP. Par exemple, l’axe PC3 contraste essentiellement pressure et pedigree. Le graphique des éboulis sert à visualiser la “chute” de la variance d’un axe principal à l’autre, et aide à choisir le nombre d’axes à conserver (espace à dimensions réduites avec perte minimale d’information). Deux variantes en diagramme en barres versticales chart$screeplot() ou chart$scree() ou sous forme d’une ligne brisée chart$altscree() sont disponibles : chart$scree(pima_pca, fill = &quot;cornsilk&quot;) chart$altscree(pima_pca) La diminution est importante entre le premier et le second axe, mais plus progressive ensuite. Ceci traduit une structure plus complexe dans les données qui ne se réduit pas facilement à un très petit nombre d’axes. Nous pouvons visualiser le premier plan principal constitué par PC1 et PC2, tout en gardant à l’esprit que seulement 53% de la variance totale y est capturée. Donc, nous pouvons nous attendre à des déformations non négligeables des données dans ce plan, et d’autres aspects qui n’y sont pas (correctement) représentés. Nous verrons qu’il est porteur, toutefois, d’information utile. Deux types de représentations peuvent être réalisées à partir d’ici : la représentation dans l’espace des variables, et la représentation complémentaire dans l’espace des individus. Ces deux représentations sont complémentaires et s’analysent conjointement. L’espace des variables représente les axes initiaux projettés comme des ombres dans le plan choisi de l’ACP (rappelez-vous l’analogie avec les ombres chinoises). Il se réalise à l’aide de chart$loadings(). Par exemple pour PC1 et PC2 nous indiquons choices = c(1, 2) (ou rien du tout, puisque ce sont les valeurs par défaut)) : chart$loadings(pima_pca, choices = c(1, 2)) Ce graphique s’interpète comme suit : Plus la norme (longueur) du vecteur qui représente une variable est grande et se rapporche de un (matérialisé par le cer cle gris), plus la variable est bien représentée dans le plan choisi. On évitera d’interpréter ici les variables qui ont des normes petites, comme pedigree ou pressure. Des vecteurs qui pointent dans la même direction représentent des variables directement corrélés entre elles. C’est le cas de glucose, insulin et aged’une part, et par ailleurs aussi de mass et triceps. Des vecteurs qui pointent en directions opposées représentent des variables inversément proportionnelles. Il n’y en a pas ici. Des vecteurs orthogonaux représentent des variables non corrélées entre elles. ainsi le groupoe glucose/insulin/age n’est pas corrélé avec le groupe mass/triceps Les PCs sont orientés en fonction des variables initiales, ou à défaut, les zones du graphique sont orientés. Ici, les gros sont dans le haut à droite du graphique, alors que ceux qui sont agés, et ont beaucoup de sucre et d’insuline dans le sang sont en bas à droite. A l’opposé, on trouve les plus maigres en bas à gauche et les jeunes ayant moins de glucose et d’insuline dans le sang en haut à gauche du graphique. Cela donne déjà une vision synthétique des différentes corrélations entre la variables. Naturellement, on peut très bien choisir d’autres axes, pour peu qu’ils représentent une part de variance relativement importante. Par exemple, ici, nous pouvons représente le plan constitué par PC1 et PC3, puisque nous avons décidé de retenir les 3 premiers axes : chart$loadings(pima_pca, choices = c(1, 3)) Nous voyons que pedigree et pressure (inversément proportionnels) sont bien mieux représentés le long de PC3. Ici l’axe PC3 est plus facile à orienter : en haut les pédigrées élevés et les pressions qartérielles basses, et en bas le contraire. Nous avons déjà lu cette informatioin dans le tableau des vecteurs propres de summary(). Le graphice entre PC2 et PC3 complète l’analyse, mais n’apportant rien de plus, il peut être typiquement éliminé de votre rapport. chart$loadings(pima_pca, choices = c(2, 3)) La seconde représentation se fait dans l’espace des individus. Ici, nous allons projeter les points relatifs à chaque individu dans le plan de l’ACP choisi. Cela se réalise à l’aide de chart$scores() (l’aspect ratio est le rapport hauteur/largeur peut s’adapter) : chart$scores(pima_pca, choices = c(1, 2), aspect.ratio = 3/5) Ce graphique est peu lisible tel quel. Généralement, nous représentons d’autres informations utiles sous forme de labels et ou de couleurs différentes. Nous pouvons ainsi contraster les individus qui ont le diabète de ceux qui ne l’ont pas sur ce graphique et aussi ajouter des ellipses de confiance à 95% autour des deux groupes pour aider à la cerner à l’aide de stat_ellipse() : chart$scores(pima_pca, choices = c(1, 2), labels = pima$diabetes) + stat_ellipse() Ce graphique est nettement plus intéressant. Il s’interprète comme suit : Nous savons que les individus plus âgés et ayant plus de glucose et d’insuline dans le sang sont dans le bas à droite du graphique. Or le groupe des diabétique, s’il ne se détache pas complètement tend à s’étaler plus dans cette région. A l’inverse, le groupe des non diabétiques s’étale vers la gauche, c’est-à-dire dans une région reprenant les individus les plus jeunes et les moins gros. Le graphique entre PC1 et PC3 (analyse du troisième axe) donne ceci : chart$scores(pima_pca, choices = c(1, 3), labels = pima$diabetes) + stat_ellipse() Ici, la séparation se fait essentiellement sur l’axe horizontal (PC1). Donc, les différentes de pédigrée (élevé dans le haut du graphique) et de pression artérielle (élevée dans le bas du graphique) semblent être moins liés au diabète. Le graphique PC3 versus PC2 peut aussi être réalisé, mais il n’apporte rien de plus (et en pratique, nous l’éliminerions d’un rapport). chart$scores(pima_pca, choices = c(2, 3), labels = pima$diabetes) + stat_ellipse() Etant donné que les deux graphiques (variables et individus) s’interprètent conjointement, nous pourrions être tentés de les superposer, cela s’appelle un biplot. Mais se pose alors un problème : celui de mettre à l’échelle les deux représentations pour qu’elles soient cohérentes entre elles. Ceci n’est pas facile et différentes représentations coexistent. L’argument scale = de la fonction chart$biplot() permet d’utiliser différentes mises à l’échelle. Enfin, ce type de graphique tend à être souvent bien trop encombré. Il est donc plus difficile à lire que les deux graphiques des variables et individus séparés. Voici ce que cela donne pour notre jeu de données exemple : chart$biplot(pima_pca) Bien moins lisible, en effet ! 7.1.3 Biométrie d’oursin Analysons à présent un autre jeu de données qui nous montrera l’importance de la transformation (linéarisation), du choix de réduire ou non (argument scale =), et l’effet d’un effet saturant, et comment s’en débarrasser. Il s’agit de la biométrie effectuée sur deux populations de l’oursin violet Paracentrotus lividus, une en élevage et une oautre provenant du milieu naturel. Nous avons abondamment utilisé ce jeu de données en SDD I dans la section visualisation. Nous le connaissons bien, mais reprenons certains éléments essentiels ici… urchin &lt;- read(&quot;urchin_bio&quot;, package = &quot;data.io&quot;, lang = &quot;FR&quot;) urchin # # A tibble: 421 x 19 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 9.9 10.2 5 NA 0.522 0.478 # 2 Pêche… 10.5 10.6 5.7 NA 0.642 0.589 # 3 Pêche… 10.8 10.8 5.2 NA 0.734 0.677 # 4 Pêche… 9.6 9.3 4.6 NA 0.370 0.344 # 5 Pêche… 10.4 10.7 4.8 NA 0.610 0.559 # 6 Pêche… 10.5 11.1 5 NA 0.610 0.551 # 7 Pêche… 11 11 5.2 NA 0.672 0.605 # 8 Pêche… 11.1 11.2 5.7 NA 0.703 0.628 # 9 Pêche… 9.4 9.2 4.6 NA 0.413 0.375 # 10 Pêche… 10.1 9.5 4.7 NA 0.449 0.398 # # … with 411 more rows, and 12 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # skeleton &lt;dbl&gt;, lantern &lt;dbl&gt;, test &lt;dbl&gt;, spines &lt;dbl&gt;, # # maturity &lt;int&gt;, sex &lt;fct&gt; Ici aussi nous avons des valeurs manquantes : naniar::vis_miss(urchin) Ces valeurs manquantes sont rassemblées essentiellement dans les variables buoyant_weight, dry_integuments, les mesures relatives au squelette (skeleton, lantern, test et spines), et surtout au niveau de sex (impossible de déterminer le sexe des individus les plus jeunes). Si nous éliminons purement et simplement les lignes qui ont au moins une valeur manquante, nous perdons tous les individus jeunes, et c’est dommage. Nous allons donc d’abord éliminer les variables sex, ainsi que les quatres variables liées au squelette. Dans un second temps, nous appliquerons drop_na() sur ce qui reste : urchin %&gt;.% select(., -(skeleton:spines), -sex) %&gt;.% drop_na(.) -&gt; urchin2 urchin2 # # A tibble: 319 x 14 # origin diameter1 diameter2 height buoyant_weight weight solid_parts # &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; # 1 Pêche… 16.7 16.8 8.4 0.588 2.58 2.04 # 2 Pêche… 19.9 20 9.2 1.10 4.26 3.66 # 3 Pêche… 19.9 19.2 8.5 0.629 2.93 2.43 # 4 Pêche… 19.3 19.8 10.2 0.781 3.71 3.09 # 5 Pêche… 18.8 20 9.3 0.761 3.59 2.99 # 6 Pêche… 21.5 20.9 9.6 1.13 4.98 4.42 # 7 Pêche… 17.4 16.5 7.8 0.477 2.33 1.97 # 8 Pêche… 21 21.2 10.8 1.23 5.4 4.55 # 9 Pêche… 17.8 18.8 8.6 0.548 2.58 2.07 # 10 Pêche… 19.7 19.6 9.7 0.862 3.59 3.08 # # … with 309 more rows, and 7 more variables: integuments &lt;dbl&gt;, # # dry_integuments &lt;dbl&gt;, digestive_tract &lt;dbl&gt;, # # dry_digestive_tract &lt;dbl&gt;, gonads &lt;dbl&gt;, dry_gonads &lt;dbl&gt;, # # maturity &lt;int&gt; Il nous reste 319 lignes des 421 initiales. Nous n’avons perdu qu’un quart des données, tout en nous privant seulement de quatres variables quantitatives liées au squelette (sexétant une variable qualitative, elle ne peut de toutes façons pas être introduite dans l’analyse, mais elle aurait pu servir pour colorer les individus). skimr::skim(urchin2) # Skim summary statistics # n obs: 319 # n variables: 14 # # ── Variable type:factor ─────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n n_unique top_counts ordered # origin 0 319 319 2 Cul: 188, Pêc: 131, NA: 0 FALSE # # ── Variable type:integer ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # maturity 0 319 319 0.37 0.71 0 0 0 0 2 ▇▁▁▁▁▁▁▂ # # ── Variable type:numeric ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 # buoyant_weight 0 319 319 4.27 3.84 0.31 1.35 3.18 # diameter1 0 319 319 32.78 11.71 14.6 23.25 31.5 # diameter2 0 319 319 32.71 11.67 15 23.45 31.6 # digestive_tract 0 319 319 1.9 2.03 0.034 0.45 1.21 # dry_digestive_tract 0 319 319 0.23 0.21 0.015 0.075 0.17 # dry_gonads 0 319 319 0.51 0.82 0 0.029 0.17 # dry_integuments 0 319 319 7.16 6.3 0.58 2.22 5.42 # gonads 0 319 319 1.72 2.65 0 0.1 0.63 # height 0 319 319 16.78 6.25 7.3 11.1 16.2 # integuments 0 319 319 12.32 10.64 1.09 4 9.4 # solid_parts 0 319 319 16.52 15.27 1.46 4.96 11.73 # weight 0 319 319 21.8 21.37 1.61 6.08 15.25 # p75 p100 hist # 5.67 17.73 ▇▃▂▂▁▁▁▁ # 39.65 65.6 ▆▇▆▆▃▃▂▁ # 39.6 65.6 ▇▇▆▆▃▃▂▁ # 2.54 10.37 ▇▃▂▁▁▁▁▁ # 0.31 1.02 ▇▅▂▁▁▁▁▁ # 0.64 5 ▇▂▁▁▁▁▁▁ # 9.42 28.8 ▇▃▂▂▁▁▁▁ # 2.2 15.93 ▇▂▁▁▁▁▁▁ # 21.5 32.2 ▇▆▆▆▅▃▂▁ # 16.07 47.22 ▇▃▃▁▁▁▁▁ # 21.69 73.14 ▇▅▂▂▁▁▁▁ # 28.14 100.51 ▇▅▁▁▁▁▁▁ Nous avons 12 variables quatitatives continues. Notez la distribution très asymétrique et similaire (voir colonne hist) de toutes ces variables. les variables origin et maturity ne pourront pas être utilisées, mais seront éventuellement utiles pour colorer les points dans nos graphiques. Qu’en est-il des corrélations entre les 12 variables ? urchin2_cor &lt;- correlation(urchin2[, 2:13]) knitr::kable(urchin2_cor, digits = 2) diameter1 diameter2 height buoyant_weight weight solid_parts integuments dry_integuments digestive_tract dry_digestive_tract gonads dry_gonads diameter1 1.00 1.00 0.98 0.95 0.96 0.96 0.97 0.96 0.91 0.93 0.80 0.79 diameter2 1.00 1.00 0.97 0.95 0.96 0.96 0.97 0.96 0.91 0.93 0.80 0.79 height 0.98 0.97 1.00 0.93 0.92 0.93 0.94 0.93 0.88 0.91 0.76 0.75 buoyant_weight 0.95 0.95 0.93 1.00 0.99 0.99 0.99 1.00 0.92 0.94 0.88 0.87 weight 0.96 0.96 0.92 0.99 1.00 0.99 0.99 0.99 0.95 0.96 0.88 0.87 solid_parts 0.96 0.96 0.93 0.99 0.99 1.00 0.99 0.99 0.95 0.95 0.91 0.90 integuments 0.97 0.97 0.94 0.99 0.99 0.99 1.00 1.00 0.93 0.95 0.87 0.85 dry_integuments 0.96 0.96 0.93 1.00 0.99 0.99 1.00 1.00 0.92 0.94 0.87 0.86 digestive_tract 0.91 0.91 0.88 0.92 0.95 0.95 0.93 0.92 1.00 0.98 0.81 0.81 dry_digestive_tract 0.93 0.93 0.91 0.94 0.96 0.95 0.95 0.94 0.98 1.00 0.82 0.82 gonads 0.80 0.80 0.76 0.88 0.88 0.91 0.87 0.87 0.81 0.82 1.00 0.99 dry_gonads 0.79 0.79 0.75 0.87 0.87 0.90 0.85 0.86 0.81 0.82 0.99 1.00 plot(urchin2_cor) Toutes les corrélations sont positives, et certaines sont très élevées. Cela indique que plusieurs variables sont (pratiquement complètement) redondantes, par exemple, diameter1 et diameter2. Un effet principal semble dominer. Si nous refaisons quelques graphiques, nous nous rappelons que les relations ne sont pas linéaires, par exemple, entre diameter1 et weight : chart(data = urchin2, weight ~ diameter1) + geom_point() Ce type de relation, dite allométrique se linéarise très bien en effectuant une transformation double-log, comme nous pouvons le constater sur le graphique suivant : chart(data = urchin2, log(weight) ~ log(diameter1)) + geom_point() Il est crucial de bien nettoyer son jeu de données avant une ACP, et aussi, de vérifier que les relations sont linéaires. Sinon il faut transformer les données de manière appropriée. Rappelez-vous que l’ACP s’intéresse aux corrélations linéaires entre vos variables. Attention toutefois à la transformation logarithmique appliquée sur des données qui peuvent contenir des zéros (par exemple, gonads ou dry_gonads). Dans ce cas, la transformartion logarithme(x + 1) réalisée avec la fonction log1p() est plus indiquée. Nous allons ici trtansformer toutes les variables en log(x + 1). C’est assez fastidieux à faire avec mutate(), mais nous pouvons l’utiliser directemnt sur le tableau entier réduit aux variables quantitatives continues seules lors de l’appel à pca() comme suit : urchin2 %&gt;.% select(., -origin, -maturity) %&gt;.% # Elimine les variables non quantitatives log1p(.) %&gt;.% # Transforme toutes les autres en log(x + 1) pca(., scale = TRUE) -&gt; urchin2_pca # Effectue l&#39;ACP après standardisation Nous avons standardisé les données puisqu’elles sont mesurées dans des unités différentes (longueurs en mm, masses en g). Voici ce que donne notre ACP : summary(urchin2_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 # Variance 11.219 0.5010 0.1813 0.03862 0.02601 0.01657 # Proportion of Variance 0.935 0.0418 0.0151 0.00322 0.00217 0.00138 # Cumulative Proportion 0.935 0.9767 0.9918 0.99503 0.99720 0.99858 # PC7 PC8 PC9 PC10 PC11 PC12 # Variance 0.00931 0.00336 0.00210 0.00108 0.00082 0.00034 # Proportion of Variance 0.00078 0.00028 0.00017 0.00009 0.00007 0.00003 # Cumulative Proportion 0.99936 0.99964 0.99981 0.99990 0.99997 1.00000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # diameter1 0.295 -0.162 -0.441 0.237 -0.174 0.177 # diameter2 0.295 -0.166 -0.449 0.249 -0.178 0.157 # height 0.291 -0.218 0.154 -0.106 -0.902 # buoyant_weight 0.296 0.120 0.509 0.124 # weight 0.296 -0.149 # solid_parts 0.297 -0.106 0.127 0.234 # integuments 0.296 -0.159 0.157 0.153 0.125 # dry_integuments 0.296 -0.115 0.160 0.455 0.114 # digestive_tract 0.288 -0.571 -0.187 0.485 -0.519 # dry_digestive_tract 0.283 -0.702 0.217 -0.278 0.513 # gonads 0.271 0.568 0.226 0.575 0.430 # dry_gonads 0.259 0.697 -0.104 -0.465 -0.453 # PC8 PC9 PC10 PC11 PC12 # diameter1 0.242 -0.706 # diameter2 0.124 0.688 0.263 # height # buoyant_weight 0.530 0.265 -0.504 # weight -0.145 0.116 -0.912 # solid_parts -0.594 0.216 0.638 # integuments -0.396 0.148 -0.702 -0.371 # dry_integuments 0.105 0.128 -0.133 0.774 # digestive_tract 0.201 # dry_digestive_tract -0.161 # gonads 0.143 # dry_gonads -0.111 Whaaa ! Plus de 93% de la variance représentée sur le premier axe. Ça parait parfait ! Voici le graphique des éboulis : chart$scree(urchin2_pca) Ne vous réjousissez pas trop vite. Nous avons ici un effet saturant lié au fait que toutes les variables sont positivement corrélées entre elles. Cet effet est évident. Ici, c’est la taille. Nous allons conclure que plus un oursin est gros, plus ses dimensions et ses masses sont importante. C’est trivial et d’un intérêt très limité, avouons-le. Puisque l’ACP optimise la variance sur le premier axe, un effet saturant aura tendance à occulter d’autres effets intéressants. Nous pouvons nous en débarrasser en identifiant une des variables représentant le mieux cet effet, et en calculant les ratios entre toutes les autres variables et celle-là. Ainsi, nous passons de quantification de la taille sur toutes les variables à des ratios qui quantifient beaucoup mieux des effets de forme plus subtils. Notez aussi les valeurs relativement faibles, mais homogènes de toutes les variables sur l’axe PC1 dans le tableau des vecteurs propres, avec des valeurs comprises entre 0,26 et 0,30. Le graphique des variables est également très moche dans le premier plan de l’ACP, même si un effet différent relatif aux gonades apparait tout de même sur l’axe PC2, il ne compte que pour 4,2% de la variance totale : chart$loadings(urchin2_pca) Recommençons tout de suite l’analyse en éliminant l’effet saturant. Nous pourrons considérer comme référence de la taille, par exemple, la masse immergée (buoyant weight) connue comme étant une mesure pouvant être mesurée très précisément. Elle fait partie des variables les mieux corrélées sur l’axe PC1, représentant ainsi très bien cet effet saturant que nous voulons éliminer. Voici notre calcul : urchin2 %&gt;.% select(., -origin, -maturity, -buoyant_weight) %&gt;.% # Elimination des variables inutiles (. / urchin2$buoyant_weight) %&gt;.% # Division par buoyant_weight log1p(.) -&gt; urchin3 # Transformation log(x + 1) head(urchin3) # diameter1 diameter2 height weight solid_parts integuments # 1 3.380877 3.386644 2.726760 1.683990 1.497119 1.388714 # 2 2.953357 2.958109 2.240741 1.587131 1.468302 1.345385 # 3 3.485925 3.451231 2.675524 1.733496 1.582091 1.478861 # 4 3.247200 3.271795 2.643585 1.749467 1.600897 1.441601 # 5 3.247291 3.306831 2.582396 1.744070 1.595668 1.424509 # 6 3.000850 2.973973 2.254397 1.690963 1.594759 1.406850 # dry_integuments digestive_tract dry_digestive_tract gonads # 1 1.030481 0.1039141 0.02667720 0.009140213 # 2 1.022630 0.1806157 0.04368131 0.040178983 # 3 1.051165 0.1683868 0.03608357 0.000000000 # 4 1.049797 0.2061975 0.04764294 0.023167059 # 5 1.048737 0.3154008 0.06613980 0.028901124 # 6 1.034084 0.3464496 0.05538973 0.016565715 # dry_gonads # 1 0.001529182 # 2 0.007821777 # 3 0.000000000 # 4 0.002174887 # 5 0.004984271 # 6 0.003104904 Refaisons notre ACP sur urchin3 ainsi calculé : urchin3_pca &lt;- pca(urchin3, scale = TRUE) summary(urchin3_pca) # Importance of components (eigenvalues): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # Variance 4.687 3.353 1.273 0.9666 0.3668 0.1724 0.10547 # Proportion of Variance 0.426 0.305 0.116 0.0879 0.0333 0.0157 0.00959 # Cumulative Proportion 0.426 0.731 0.847 0.9345 0.9678 0.9835 0.99308 # PC8 PC9 PC10 PC11 # Variance 0.04761 0.01943 0.00834 0.00068 # Proportion of Variance 0.00433 0.00177 0.00076 0.00006 # Cumulative Proportion 0.99741 0.99918 0.99994 1.00000 # # Loadings (eigenvectors, rotation matrix): # PC1 PC2 PC3 PC4 PC5 PC6 PC7 # diameter1 -0.425 0.145 -0.297 -0.101 # diameter2 -0.425 -0.101 0.143 -0.296 -0.103 # height -0.427 0.131 -0.300 # weight 0.189 -0.428 0.216 0.497 -0.672 -0.145 # solid_parts -0.495 0.254 -0.117 0.335 -0.245 # integuments -0.142 -0.463 0.152 0.283 0.165 0.345 0.667 # dry_integuments -0.259 -0.242 0.173 0.533 -0.669 -0.161 -0.277 # digestive_tract 0.214 -0.370 -0.448 -0.102 0.388 -0.462 # dry_digestive_tract -0.360 -0.485 -0.401 -0.429 -0.338 0.382 # gonads 0.374 0.438 -0.257 -0.223 # dry_gonads 0.371 0.440 -0.269 -0.162 -0.122 # PC8 PC9 PC10 PC11 # diameter1 0.101 0.400 0.714 # diameter2 0.425 -0.700 # height 0.141 0.197 -0.790 # weight 0.109 # solid_parts -0.662 -0.223 # integuments 0.266 # dry_integuments # digestive_tract 0.468 0.148 # dry_digestive_tract -0.155 # gonads 0.725 0.130 # dry_gonads 0.446 -0.589 chart$scree(urchin3_pca) Maintenant que l’effet saturant est éliminé, la répartition des variances sur les axes se fait mieux. L’axe PC1 contraste les diamètres avec les gonades, l’axe PC2 représente les masses somatiques (dans l’ordre inverse), et l’axe PC3 contraste de manière intéressante les masses du tube digestif avec celles des gonades (le tout en ratios sur la masse immergée, ne l’oublions pas). Les deux premiers axes reprennent 73% de la variance, mais il semble qu’un effet intéressant se marque également sur PC3 avec 85% de la variance totale sur les trois premiers axes. Tout ceci est également visible sur les graphiques dans l’espace des variables (plans PC1 - PC2 et PC2 - PC3 représentés ici). chart$loadings(urchin3_pca, choices = c(1, 2)) chart$loadings(urchin3_pca, choices = c(2, 3)) Enfin, dans l’espace des individus, avec l’origine reprise en couleur, nous observons ceci dans le prmeier plan de l’ACP : chart$scores(urchin3_pca, choices = c(1, 2), col = urchin2$origin, labels = urchin2$maturity, aspect.ratio = 3/5) + theme(legend.position = &quot;right&quot;) + stat_ellipse() Et pour le plan PC2 - PC3 : chart$scores(urchin3_pca, choices = c(2, 3), col = urchin2$origin, labels = urchin2$maturity, aspect.ratio = 3/5) + theme(legend.position = &quot;right&quot;) + stat_ellipse() Vous devriez pouvoir interpréter ces résultats par vous-même maintenant. 7.1.4 Visualisation de données quantitatives 7.1.4.1 Deux dimensions Le nuage de points est le graphe idéal pour visualiser la distribution des données bivariées pour deux vaeriavbles quantitatives. Il permet de visualiser également une association entre deux variables. Il permet aussi de visualiser comment deux ou plusieurs groupes peuvent être séparés en fonction de ces deux variables. chart(data = pima, glucose ~ insulin %col=% diabetes) + geom_point() 7.1.4.2 Trois dimensions Le nuage de points en pseudo-3D est l’équivalent pour visualiser trois variables quantitatives simultanément. Il est nécessaire de rendre l’effet de la troisième dimension (perspective, variation de taille des objets, …). La possibilité de faire tourner l’objet 3D virtuel est indispensable pour concrétiser l’effet 3D et pour le visionner sous différents angles Le package rgl permet de réaliser ce genre de graphique 3D interactif (que vous pouvez faire tourner dans l’orientation que vous voulez à la souris) : rgl::plot3d(pima$insulin, pima$glucose, pima$triceps, col = as.integer(pima$diabetes)) 7.1.4.3 Plus de trois dimensions Déjà à trois dimensions la visualisation devient délicate, mais au delà, cela devient pratiquement mission impossible. La matrice de nuages de points peut rendre service ici, mais dans certaines limites (tous les angles de vue ne sont pas accessibles). GGally::ggscatmat(pima, 2:6, color = &quot;diabetes&quot;) Nous voyons qu’ici nous atteignons les limites des possibilités. C’est pour cela que, pour des données multivariées comportant beaucoup de variables quantitatives, les techniques de réduction des dimensions comme l’ACP sont indispensables. 7.1.5 ACP : mécanisme Nous allons partir d’un exemple presque trivial pour illuster le principe de l’ACP. Comment réduire un tableau bivarié en une représentation des individus en une seule dimension (classement sur une droite) avec perte minimale d’information ? Par exemple, en partant de ces données fictives : Voic une représentation graphique 2D de ces données : Si nous réduisons à une seule dimension en laissant tomber une des deux variables, voici ce que cela donne (ici on ne garde que Var1, donc, on projette les points sur l’axe des abscisses). Au final, nous avons ordonné nos individus en une dimension comme suit : C’est une mauvaise solution car il y a trop de perte d’information. Regardez l’écart entre 7 et 9 sur le graphqie en deux dimensions et dans celui à une dimension : les points sont trop près. Comparez sur les deux graphiques les distances 7 - 9 avec 9 - 8 et 1 - 2 versus 1 - 3. Tout cela est très mal représenté en une seule dimension. Une autre solution serait de projeter le long de la droite de “tendance générale”, c’est-à-dire le long de l’axe de plus grand allongement du nuage de points. Cela donne ceci en une seule dimension : C’est une bien meilleure solution car la perte d’information est ici minimale. Regardez à nouveau la distance entre 7 et 9 sur le graphique initial à deux dimensions et sur le nouveau graphique réduit à une dimension : c’est mieux qu’avant. Comparez aussi les distances respectives entre les paires 7 - 9 et 9 - 8, ainsi que 1 - 2 par rapport à 1 - 3. Tout cela est bien pieux représenté à présent. L’ACP effectue précisément la projection que nous venons d’imaginer. La droite de projection est appelée composante principale 1. La composante principale 1 présente la plus grande variabilité possible sur un seul axe. Ensuite on calcule la composante 2 comme étant orthogonale (i.e., perpendiculaire) à PC1 et présentant la plus grande variabilité non encore capturée par la composante 1. Le mécanisme revient à projeter les points sur des axes orientés différemment dans l’e plan’espace à N dimensions (pour N variables intiales). En effet, mathématiquement ce mécanisme se généralise facilement à trois, puis à N dimensions. 7.1.6 Calcul matriciel ACP La rotation optimale des axes vers les PC1 à PCN se résoud par un calcul matriciel. Nous allons maintenant le détailler. Mais auparavant, nous devons nous rafraîchir l’esprit concernant quelques notions. Multiplication matricielle : \\(\\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} 1\\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 11\\\\ 5 \\end{pmatrix}\\) Vecteurs propres et valeurs propres (il en existe autant qu’il y a de colonnes dans la matrice de départ) : \\[ \\begin{pmatrix} 2 &amp; 3\\\\ 2 &amp; 1 \\end{pmatrix} \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 12\\\\ 8 \\end{pmatrix} = 4 \\times \\begin{pmatrix} 3\\\\ 2 \\end{pmatrix} \\] La constante (4) est une valeur propre et la matrice multipliée (à droite) est la matrice des vecteurs propres. La rotation d’un système d’axes à deux dimensions d’un angle \\(\\alpha\\) peut se représenter sous forme d’un calcul matriciel : \\[ \\begin{pmatrix} \\cos \\alpha &amp; \\sin \\alpha\\\\ -\\sin \\alpha &amp; \\cos \\alpha \\end{pmatrix} \\times \\begin{pmatrix} x\\\\ y \\end{pmatrix} = \\begin{pmatrix} x&#39;\\\\ y&#39; \\end{pmatrix} \\] Dans le cas particulier de l’ACP, la matrice de transformation qui effectue la rotation voulue pour obtenir les axes principaux est la matrice rassemblant tous les vecteurs propres calculés après diagonalisation de la matrice de corrélation ou de variance/covariance (réduction ou non, respectivement). Le schéma suivant visualise la rotation depuis les axes initiaux X et Y (variables de départ) en bleu royal vers les PC1, PC2 en rouge. Un individu p est représenté par les coordonnées {x, y} dans le système d’axes initial XY. Les nouvelles coordonnées {x’, y’} sont recalculées par projection sur les nouveaux axes PC1-PC2. Les flèches bleues sont représentées dans l’espace des variables, tandis que les points reprojettés sur PC1-PC2 sont représentés dans l’espace des individus selon les coordonnées primes en rouge. 7.1.6.1 Résolution numérique simple Effectuons une ACP sur matrice var/covar sans réduction des données (mais calcul très similaire lorsque les données sont réduites) sur un exemple numérique simple. Étape 1 : centrage des données \\[ \\mathop{\\begin{pmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\\\ 5 &amp; 0 \\\\ 7 &amp; 6 \\\\ 9 &amp; 2 \\end{pmatrix}}_{\\text{Tableau brut}} \\xrightarrow{\\phantom{---}\\text{centrage}\\phantom{---}} \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\] Étape 2 : calcul de la matrice de variance/covariance \\[ \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\xrightarrow{\\phantom{---}\\text{var/covar}\\phantom{---}} \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice carrée (A)}} \\] Étape 3 : diagonalisation de la matrice var/covar \\[ \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice carrée (A)}} \\xrightarrow{\\phantom{---}\\text{diagonalisation}\\phantom{---}} \\mathop{\\begin{pmatrix} 9 &amp; 0 \\\\ 0 &amp; 5 \\end{pmatrix}}_{\\text{Matrice diagonalisée (B)}} \\] La trace des deux matrices A et B (somme des éléments sur la diagonale) est égale à : 8.2 + 5.8 = 14 = 9 + 5. 8.2 est la part de variance exprimée sur le premier axe initial (X) 5.8 est la part de variance exprimée sur le second axe initial (Y) 14 est la variance totale du jeu de données La matrice diagonale B est la solution exprimant la plus grande part de variance possible sur le premier axe de l’ACP : 9, soit 64,3% de la variance totale. Les éléments sur la diagonale sont les valeurs propres \\(\\lambda_i\\) ! Vous vous rappelez les fameuses “eigenvalues” dans la sortie de summary(pima_pca). Étape 4 : calcul de la matrice de rotation des axes (en utilisant la propriété des valeurs propres \\(\\text{A}.\\text{U} = \\text{B}.\\text{U}\\)). \\[ \\mathop{\\begin{pmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\end{pmatrix}}_{\\text{Matrice A}} \\times \\text{U} = \\mathop{\\begin{pmatrix} 9 &amp; 0 \\\\ 0 &amp; 5 \\end{pmatrix}}_{\\text{Matrice B}} \\times \\text{U} \\rightarrow \\text{U} = \\mathop{\\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix}}_{\\text{Matrice des vecteur propres (U)}} \\] La matrice des vecteurs propres (U) (“eigenvectors” en anglais) effectue la transformation (rotation des axes) pour obtenir les composantes principales. L’angle de rotation se déduit en considérant que cette matrice contient des sin et cos d’angles de rotation des axes : \\[ \\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix} = \\begin{pmatrix} \\phantom{-}\\cos(-26.6°) &amp; \\phantom{-}\\sin(-26.6°) \\\\ -\\sin(-26.6°) &amp; \\phantom{-}\\cos(-26.6°) \\end{pmatrix} \\] Étape 5 : représentation dans l’espace des variables. C’est une représentation dans un cercle de la matrice des vecteurs propres U sous forme de vecteurs. Étape 6 : représentation dans l’espace des individus. On recalcule les coordonnées des individus dans le système d’axe après rotation. \\[ \\mathop{\\begin{pmatrix} -3.2 &amp; -1.8 \\\\ -2.2 &amp; \\phantom{-}1.4 \\\\ -0.2 &amp; -2.6 \\\\ \\phantom{-}1.8 &amp; \\phantom{-}3.4 \\\\ \\phantom{-}3.8 &amp; -0.6 \\end{pmatrix}}_{\\text{Tableau centré (X)}} \\times \\mathop{\\begin{pmatrix} \\phantom{-}0.894 &amp; -0.447 \\\\ \\phantom{-}0.447 &amp; \\phantom{-}0.894 \\end{pmatrix}}_{\\text{Matrice des vecteur propres (U)}} \\xrightarrow{\\phantom{---}\\text{X}.\\text{U} = \\text{X&#39;}\\phantom{---}} \\mathop{\\begin{pmatrix} -3.58 &amp; \\phantom{-}0.00 \\\\ -1.34 &amp; \\phantom{-}2.24 \\\\ -1.34 &amp; -2.24 \\\\ \\phantom{-}3.13 &amp; \\phantom{-}2.24 \\\\ \\phantom{-}3.13 &amp; -2.24 \\end{pmatrix}}_{\\text{Tableau avec rotation (X&#39;)}} \\] Ensuite, on représente ces individus à l’aide d’un graphique en nuage de points. Tout ces calculs se généralisent facilement à trois, puis à N dimensions. Pour aller plus loin N’hésitez pas à combiner plusieurs techniques. Par exemple, vous pouvez représenter les groupes créés par classification ascendante hiérarchiques sur un graphique de l’ACP dans l’espace des individus en faisant varier les couleurs ou les labels des individus en fonction des groupes de la CAH. Une autre explication de l’ACP en utilsant quelques autres fonctions de R pour visualiser le résultat Une explication détaillée de la PCA en anglais. Une page qui reprend une série de vidéos qui présentent les différentes facettes de l’ACP (en franglais). A vous de jouer ! Réalisez le début du projet spécifique lié au module 7. Ce module couvre la matière entière du module 7. Le lien pour réaliser ce projet se trouve en début du module 7 Ce projet doit être terminé à la fin de ce module Complétez votre projet sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Complétez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea/blob/master/README.md Le README est un rappel des consignes, il ne s’agit aucunement du lien pour débuter le travail "],
["analyse-factorielle-des-correspondances.html", "7.2 Analyse factorielle des correspondances", " 7.2 Analyse factorielle des correspondances Comme l’ACP s’intéresse à des corrélations linéaires entre variables quantitatives, elle n’est absolument pas utilisable pour traiter des variables qualitatives. L’Analyse Factorielle des Correspondances sera utile dans ce dernier cas (AFC, ou en anglais “Correspondence Analysis” ou CA). 7.2.1 AFC dans SciViews::R L’AFC utilises la fonction ca() du package ca dans SciViews::R, mais au stade actuel, tout le code nécessaire (en particulier pour réaliser les graphiques avec chart()) n’est pas encore complètement intégré dans les packages. Ainsi, vous pouvez copier-coller le code du chunk suivant au début de vos scripts ou dans un chunk de setup dans vos documents R Markdown/Notebook. SciViews::R() library(broom) # Au lieu de MASS::corresp(, nf = 2), nous préférons ca::ca() ca &lt;- ca::ca scale_axes &lt;- function(data, aspect.ratio = 1) { range_x &lt;- range(data[, 1]) span_x &lt;- abs(max(range_x) - min(range_x)) range_y &lt;- range(data[, 2]) span_y &lt;- abs(max(range_y) - min(range_y)) if ((span_y / aspect.ratio) &gt; span_x) { # Adjust range_x span_x_2 &lt;- span_y / aspect.ratio / 2 range_x_mid &lt;- sum(range_x) / 2 range_x &lt;- c(range_x_mid - span_x_2, range_x_mid + span_x_2) } else { # Adjust range_y span_y_2 &lt;- span_x * aspect.ratio / 2 range_y_mid &lt;- sum(range_y) / 2 range_y &lt;- c(range_y_mid - span_y_2, range_y_mid + span_y_2) } list(x = range_x, y = range_y) } plot3d &lt;- rgl::plot3d plot3d.ca &lt;- ca:::plot3d.ca autoplot.ca &lt;- function(object, choices = 1L:2L, type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;biplot&quot;), col = &quot;black&quot;, fill = &quot;gray&quot;, aspect.ratio = 1, repel = FALSE, ...) { type = match.arg(type) res &lt;- switch(type, screeplot = object %&gt;.% # Classical screeplot `[[`(., &quot;sv&quot;) %&gt;.% tibble(Dimension = 1:length(.), sv = .) %&gt;.% chart(data = ., sv^2 ~ Dimension) + geom_col(col = col, fill = fill) + labs(y = &quot;Inertia&quot;), altscreeplot = object %&gt;.% # screeplot represented by dots and lines `[[`(., &quot;sv&quot;) %&gt;.% tibble(Dimension = 1:length(.), sv = .) %&gt;.% chart(data = ., sv^2 ~ Dimension) + geom_line(col = col) + geom_point(col = &quot;white&quot;, fill = col, size = 2, shape = 21, stroke = 3) + labs(y = &quot;Inertia&quot;), biplot = { # We want to use the function plot.ca(), but without plotting the base plot # So, we place it in a specific environment where all base plot functions are # fake and do nothing (we just want to collect points coordinates at the end) env &lt;- new.env() env$plot_ca &lt;- ca:::plot.ca environment(env$plot_ca) &lt;- env env$plot &lt;- function(...) NULL env$box &lt;- function(...) NULL env$abline &lt;- function(...) NULL env$axis &lt;- function(...) NULL env$par &lt;- function(...) NULL env$points &lt;- function(...) NULL env$lines &lt;- function(...) NULL env$.arrows &lt;- function(...) NULL env$text &lt;- function(...) NULL env$strwidth &lt;- function(...) NULL env$strheight &lt;- function(...) NULL contribs &lt;- paste0(&quot;Dimension &quot;, 1:length(object$sv), &quot; (&quot;, round(object$sv^2 / sum(object$sv^2) * 100, 1), &quot;%)&quot;)[choices] res &lt;- env$plot_ca(object, dim = choices, ...) rows &lt;- as.data.frame(res$rows) rows$Type &lt;- &quot;rows&quot; rows$Labels &lt;- object$rownames cols &lt;- as.data.frame(res$cols) cols$Type &lt;- &quot;cols&quot; cols$Labels &lt;- object$colnames res &lt;- bind_rows(rows, cols) names(res) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;type&quot;, &quot;labels&quot;) lims &lt;- scale_axes(res, aspect.ratio = aspect.ratio) nudge &lt;- (lims$x[2] - lims$x[1]) / 100 res &lt;- chart(data = res, y ~ x %col=% type %label=% labels) + geom_hline(yintercept = 0, col = &quot;gray&quot;) + geom_vline(xintercept = 0, col = &quot;gray&quot;) + coord_fixed(ratio = 1, xlim = lims$x, ylim = lims$y, expand = TRUE) + theme(legend.position = &quot;none&quot;) + labs(x = contribs[1], y = contribs[2]) if (isTRUE(repel)) { res &lt;- res + geom_point() + ggrepel::geom_text_repel() } else {# Use text res &lt;- res + geom_point() + geom_text(hjust = 0, vjust = 0, nudge_x = nudge, nudge_y = nudge) } res } ) res } chart.ca &lt;- function(data, choices = 1L:2L, ..., type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;boiplot&quot;), env = parent.frame()) autoplot.ca(data, choices = choices, ..., type = type, env = env) class(chart.ca) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) 7.2.2 Enquête sur la science L’idée de départ de l’AFC est de pouvoir travailler sur des données qualitatives en les transformant en variables quantitatives via une astuce de calcul, et ensuite de réaliser une ACP sur ce tableau quantitatif pour en représenter l’information visuellement sur une carte. Les enquêtes (mais pas seulement) génèrent souvent des quantités importantes de variables qualitatives qu’il faut ensuite analyser. En effet, des sondage proposent la plupart du temps d’évaluer une question sur une échelle de plusieurs niveaux du genre “tout-à-fait d’accord”, “d’accord”, “pas d’accord”, “pas du tout d’accord”, ce qui donnerait une variable facteur à quatre niveaux ordonnés. Une grande enquête mondiale a été réalisée en 1993 pour déterminer (entre autre) ce que la population pense de la science en général. Quatre questions sont posées (section 4 de ce questionnaire, voir aussi ici) : A. Les gens croient trop souvent à la science, et pas assez aux sentiments et à la foi B. En général, la science moderne fait plus de mal que de bien C. Tout changement dans la nature apporté par les êtres humains risque d’empirer les choses D. La science moderne va résoudre nos problèmes relatifs à l’environement sans faire de grands changements à notre mode de vie Les réponses possibles sont : 1 = tout-à-fait d’accord à 5 = pas du tout d’accord. Le jeu de données wg93 du package ca reprend les réponses données par les allemands de l’ouest à ces questions. De plus, les caractéristiques suivantes des répondants sont enregistrées : le sexe (1 = homme, 2 = femme), l’âge (1 = 18-24, 2 = 25-34, 3 = 35-44, 4 = 45-54, 5 = 55-64, 6 = 65+) le niveau d’éducation (1 = primaire, 2 = second. partim, 3 = secondaire, 4 = univ. partim, 5 = univ. cycle 1, 6 = univ. cycle 2+) wg &lt;- read(&quot;wg93&quot;, package = &quot;ca&quot;) wg # # A tibble: 871 x 7 # A B C D sex age edu # &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; # 1 2 3 4 3 2 2 3 # 2 3 4 2 3 1 3 4 # 3 2 3 2 4 2 3 2 # 4 2 2 2 2 1 2 3 # 5 3 3 3 3 1 5 2 # 6 3 4 4 5 1 3 2 # 7 3 4 2 4 2 5 2 # 8 3 4 4 2 1 3 3 # 9 3 2 2 1 1 3 2 # 10 3 3 2 2 1 3 2 # # … with 861 more rows Ceci est un tableau cas par variables avec sept variables facteurs et 871 cas. Nous commençons par réencoder les niveaux des variables pour plus de clarté : wg %&gt;.% mutate(., A = recode(A, `1` = &quot;++&quot;, `2` = &quot;+&quot;, `3` = &quot;0&quot;, `4` = &quot;-&quot;, `5` = &quot;--&quot;), B = recode(B, `1` = &quot;++&quot;, `2` = &quot;+&quot;, `3` = &quot;0&quot;, `4` = &quot;-&quot;, `5` = &quot;--&quot;), C = recode(C, `1` = &quot;++&quot;, `2` = &quot;+&quot;, `3` = &quot;0&quot;, `4` = &quot;-&quot;, `5` = &quot;--&quot;), D = recode(D, `1` = &quot;++&quot;, `2` = &quot;+&quot;, `3` = &quot;0&quot;, `4` = &quot;-&quot;, `5` = &quot;--&quot;), sex = recode(sex, `1` = &quot;H&quot;, `2`= &quot;F&quot;), age = recode(age, `1` = &quot;18-24&quot;, `2` = &quot;25-34&quot;, `3` = &quot;35-44&quot;, `4` = &quot;45-54&quot;, `5` = &quot;55-64&quot;, `6` = &quot;65+&quot;), edu = recode(edu, `1` = &quot;primaire&quot;, `2` = &quot;sec. part&quot;, `3` = &quot;secondaire&quot;, `4` = &quot;univ. part&quot;, `5` = &quot;univ. cycle 1&quot;, `6` = &quot;univ. cycle 2&quot;) ) -&gt; wg wg # # A tibble: 871 x 7 # A B C D sex age edu # &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; # 1 + 0 - 0 F 25-34 secondaire # 2 0 - + 0 H 35-44 univ. part # 3 + 0 + - F 35-44 sec. part # 4 + + + + H 25-34 secondaire # 5 0 0 0 0 H 55-64 sec. part # 6 0 - - -- H 35-44 sec. part # 7 0 - + - F 55-64 sec. part # 8 0 - - + H 35-44 secondaire # 9 0 + + ++ H 35-44 sec. part # 10 0 0 + + H 35-44 sec. part # # … with 861 more rows Par exemple, si nous nous posons la question de l’impact de l’éducation sur l’impression que la science est néfaste (question B), nous pourrons faire l’analyse suivante : Étape 1 : calcul du tableau de contingence à double entrée croisant les réponses à la question B avec le niveau d’éducation des répondants : table(wg$B, wg$edu) # # primaire sec. part secondaire univ. part univ. cycle 1 univ. cycle 2 # ++ 6 34 19 6 4 2 # + 10 93 47 12 5 7 # 0 11 95 55 18 11 15 # - 7 112 82 37 16 27 # -- 4 44 39 21 13 19 Étape 2 : test de Chi2 d’indépendance, voir section 8.2 du cours de SDD I. L’hypothèse nulle du test est quel les deux variables sont indépendantes l’une de l’autre. L’hypothèse alternative est qu’une dépendance existe. Si la réponse aux questions est indépendante du niveau d’éducation (pas de rejet de H0), notre analyse est terminée. Sinon, il faut approfondir… Choisissons notre seuil \\(\\alpha\\) à 5% avant de réaliser notre test. chisq.test(wg$B, wg$edu) # Warning in chisq.test(wg$B, wg$edu): Chi-squared approximation may be # incorrect # # Pearson&#39;s Chi-squared test # # data: wg$B and wg$edu # X-squared = 42.764, df = 20, p-value = 0.002196 L’avertissement nous prévient qu’une approximation a du être réalisée, mais le test reste utilisable si la valeur est loin du seuil \\(/alpha\\). La valeur p de 0,2% est très inférieure à \\(\\alpha\\). Nous rejettons H0. Il y a dépendance entre le niveau d’éducation et la réponse à la question B. OK, mais comment se fait cette dépendance ? C’est ici que l’AFC nous vient en aide. -Étape 3 : Si l’hypothèse nulle est rejettée, il faut analyser plus en profondeur. L’AFC va présenter la dépendance de manière claire. La fonction ca() accepte un jeu de données dans l’argument data = et une formule qui spécifie dans le terme de droite les deux variables à croiser séparées par un signe + (~ fact1 + fact2). Donc : wg_b_edu &lt;- ca(data = wg, ~ B + edu) wg_b_edu # # Principal inertias (eigenvalues): # 1 2 3 4 # Value 0.043989 0.004191 0.000914 4e-06 # Percentage 89.59% 8.54% 1.86% 0.01% # # # Rows: # ++ + 0 - -- # Mass 0.081515 0.199770 0.235362 0.322618 0.160735 # ChiDist 0.286891 0.274685 0.095627 0.141176 0.341388 # Inertia 0.006709 0.015073 0.002152 0.006430 0.018733 # Dim. 1 -1.140162 -1.293374 -0.406059 0.591115 1.593838 # Dim. 2 -2.211930 0.641674 -0.409771 0.976000 -1.034695 # # # Columns: # primaire sec. part secondaire univ. part univ. cycle 1 # Mass 0.043628 0.433984 0.277842 0.107922 0.056257 # ChiDist 0.427367 0.164446 0.036882 0.279471 0.341163 # Inertia 0.007968 0.011736 0.000378 0.008429 0.006548 # Dim. 1 -1.722505 -0.773101 0.115183 1.302120 1.428782 # Dim. 2 -3.523982 0.381160 0.336612 0.235984 -2.516238 # univ. cycle 2 # Mass 0.080367 # ChiDist 0.417941 # Inertia 0.014038 # Dim. 1 1.962905 # Dim. 2 0.135512 Nous retrouvons les valeurs propres (eigenvalues) qui représentent l’inertie exprimée sur les différents axes, avec le pourcentage juste en dessous. Les deux tableaux suivants détaillent les calculs. Il n’est pas indispensable de comprendre tout ce qui s’y trouve, mais notez que les composantes du calcul du Chi2 pour chaque niveau des variables sont reprises à la ligne ChiDist. Nous y reviendrons. La méthode summary() ainsi que le graphique des éboulis via chart$scree() nous donnent une information plus claire sur la contribution de la variabilité totale sur les différents axes. Cela nous aide à décider si le plan réduit aux deux premiers axes que nous utiliserons ensuite est adéquat ou non (pourcentage suffisant = données bien représentées dans ce plan). summary(wg_b_edu, scree = TRUE, rows = FALSE, columns = FALSE) # # Principal inertias (eigenvalues): # # dim value % cum% scree plot # 1 0.043989 89.6 89.6 ********************** # 2 0.004191 8.5 98.1 ** # 3 0.000914 1.9 100.0 # 4 4e-06000 0.0 100.0 # -------- ----- # Total: 0.049097 100.0 chart$scree(wg_b_edu) Ici, les deux premiers axes cumulent plus de 98% de la variation totale. Nous pouvons donc continuer en toute confiance. Le biplot va ici projetter les individus sur la carte (les individus étant en fait les différents niveaux des deux variables, obtenus après réalisation de deux ACP, l’une sur les colonnes et l’autre sur les lignes). Les deux groupes sont représentés par des couleurs différentes, mais sur une même carte. C’erst la fonction chart$biplot() qui réalise ce graphique. chart$biplot(wg_b_edu, choices = c(1, 2)) L’interprétation se fait d’abord sur les points d’une couleur (niveau de la première variable), puis sur ceux de l’autre (niveaux de la seconde variable), et enfin, conjointement. Voici ce que cela donne ici : Les niveaux d’éducation en rouge sont globalement représentés essentiellement de gauche à droite dans un ordre croissant de “études primaires” jusqu’à “études universitaires de 2éme cycle ou plus”. Les réponses à la question B (en bleu turquoise) sont également rangés globalement de gauche à droite (avec une légère inversion entre + et ++) depuis ceux qui sont tout-à-fait d’accord que la science moderne est néfaste à la gauche jusqu’à ceux qui ne le sont pas du tout à la droite. Conjointement, on va comparer les points rouges et les bleus et regarder ceux qui se situent dans une direction similaire par rapport au centre d’inertie à la coordonnée {0, 0} matérialisée par la croix grise20. Ainsi, nous constatons que ceux qui ont le niveau éducatif le plus faible (niveau primaire) ont globalement plutôt répondu qu’ils sont tout-à-fait d’accord (++), alors qu’à l’opposé, les universitaires ne sont pas du tous d’accord avec l’affirmation (--). L’essentiel se lit ici effectivement sur un seul axe (dimension 1) qui capture environ 90% de la variation totale. Le second axe vient moduler légèrement ce classement avec 8,5% de variabilité, mais sans tout bouleverser. Ici la lecture de la carte générée par l’AFC est limpide… Et manifestement, nous pouvions légitimement conclure à l’époque qu’un effort éducatif et d’information était nécessaire auprès de la population la moins éduquée pour leur faire prendre conscience de l’intérêt de la science moderne (à moins que cette dernière remarque ne soit un parti pris… d’un universtaire :-). 7.2.3 Des acariens sinon rien Un deuxième exemple illustre une utilisation légèrement différente de l’AFC. Il s’agit des tableaux de type “espèces par stations”. Ces tableaux contiennent différentes espèces ou autres groupes taxonomiques dénombrés dans plusieurs échantillons prélevés à des stations différentes. Le dénombrement des espèces peut être quantitatif (nombre d’individus observés par unité de surface ou de volume), semi-quantitatif (peu, moyen, beaucoup), ou même binaire (présence ou absence de l’espèce). Ces tableaux sont assimilables, en réalité, à des tableaux de contingence à double entrée qui croisent deux variables qualitatives “espèce” et “station”. C’est comme si la première étape avait déjà été réalisée dans l’analyse précédente du jeu de données wg. Naturellement, comme il ne s’agit pas réellement d’un tableau de contingence, nous ne réaliserons pas le test Chi2 d’indépendance ici, et nous passerons directement à l’étape de l’AFC. L’exemple choisi concerne des populations d’acariens dans de la litière (acarien se dit “mite” en anglais, d’où le nom du jeu de données issu du package vegan). mite &lt;- read(&quot;mite&quot;, package = &quot;vegan&quot;) skimr::skim(mite) # Skim summary statistics # n obs: 70 # n variables: 35 # # ── Variable type:integer ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 hist # Brachy 0 70 70 8.73 10.08 0 3 4.5 11.75 42 ▇▂▁▂▁▁▁▁ # Ceratoz1 0 70 70 1.29 1.46 0 0 1 2 5 ▇▆▁▃▁▁▁▁ # Ceratoz3 0 70 70 1.3 2.2 0 0 0 2 9 ▇▁▁▁▁▁▁▁ # Eupelops 0 70 70 0.64 0.99 0 0 0 1 4 ▇▃▁▁▁▁▁▁ # FSET 0 70 70 1.86 3.18 0 0 0 2 12 ▇▂▁▁▁▁▁▁ # Galumna1 0 70 70 0.96 1.73 0 0 0 1 8 ▇▁▁▁▁▁▁▁ # HMIN 0 70 70 4.91 8.47 0 0 0 4.75 36 ▇▁▁▁▁▁▁▁ # HMIN2 0 70 70 1.96 3.92 0 0 0 2.75 20 ▇▂▁▁▁▁▁▁ # HPAV 0 70 70 8.51 7.56 0 4 6.5 12 37 ▇▇▃▃▁▁▁▁ # HRUF 0 70 70 0.23 0.62 0 0 0 0 3 ▇▁▁▁▁▁▁▁ # LCIL 0 70 70 35.26 88.85 0 1.25 13 44 723 ▇▁▁▁▁▁▁▁ # Lepidzts 0 70 70 0.17 0.54 0 0 0 0 3 ▇▁▁▁▁▁▁▁ # LRUG 0 70 70 10.43 12.66 0 0 4.5 17.75 57 ▇▂▂▁▁▁▁▁ # MEGR 0 70 70 2.19 3.62 0 0 1 3 17 ▇▂▁▁▁▁▁▁ # Miniglmn 0 70 70 0.24 0.79 0 0 0 0 5 ▇▁▁▁▁▁▁▁ # MPRO 0 70 70 0.16 0.47 0 0 0 0 2 ▇▁▁▁▁▁▁▁ # NCOR 0 70 70 1.13 1.65 0 0 0.5 1.75 7 ▇▃▂▂▁▁▁▁ # NPRA 0 70 70 1.89 2.37 0 0 1 2.75 10 ▇▂▂▁▁▁▁▁ # ONOV 0 70 70 17.27 18.05 0 5 10.5 24.25 73 ▇▃▂▁▁▁▁▁ # Oppiminu 0 70 70 1.11 1.84 0 0 0 1.75 9 ▇▁▁▁▁▁▁▁ # Oribatl1 0 70 70 1.89 3.43 0 0 0 2.75 17 ▇▁▁▁▁▁▁▁ # PHTH 0 70 70 1.27 2.17 0 0 0 2 8 ▇▁▁▁▁▁▁▁ # PLAG2 0 70 70 0.8 1.79 0 0 0 1 9 ▇▁▁▁▁▁▁▁ # PPEL 0 70 70 0.17 0.54 0 0 0 0 3 ▇▁▁▁▁▁▁▁ # Protopl 0 70 70 0.37 1.61 0 0 0 0 13 ▇▁▁▁▁▁▁▁ # PWIL 0 70 70 1.09 1.71 0 0 0 1 8 ▇▁▁▁▁▁▁▁ # RARD 0 70 70 1.21 2.78 0 0 0 1 13 ▇▂▁▁▁▁▁▁ # SLAT 0 70 70 0.4 1.23 0 0 0 0 8 ▇▁▁▁▁▁▁▁ # SSTR 0 70 70 0.31 0.97 0 0 0 0 6 ▇▁▁▁▁▁▁▁ # Stgncrs2 0 70 70 0.73 1.83 0 0 0 0 9 ▇▁▁▁▁▁▁▁ # SUCT 0 70 70 16.96 13.89 0 7.25 13.5 24 63 ▇▇▆▅▂▁▁▁ # Trhypch1 0 70 70 2.61 6.14 0 0 0 2 29 ▇▁▁▁▁▁▁▁ # Trimalc2 0 70 70 2.07 5.79 0 0 0 0 33 ▇▁▁▁▁▁▁▁ # TVEL 0 70 70 9.06 10.93 0 0 3 19 42 ▇▁▁▂▁▁▁▁ # TVIE 0 70 70 0.83 1.47 0 0 0 1 7 ▇▁▁▁▁▁▁▁ 35 espèces d’acariens sont dénombrés en 70 stations différentes, et il n’y a aucunes valeurs manquantes. Comme il ne peut y avoir aucun total de ligne ou de colonne égal à zéro pour le calcul des Chi2 (sinon, on aurait une division par zéro), nous vérifions cela de la façon suivante : rowSums(mite) # [1] 140 268 186 286 199 209 162 126 123 166 216 213 177 269 100 97 90 # [18] 118 118 184 117 172 81 80 123 120 173 111 111 96 130 93 136 194 # [35] 111 133 139 189 94 157 81 140 148 60 158 154 121 113 107 148 91 # [52] 112 145 49 58 108 8 121 90 127 42 13 86 88 112 116 781 111 # [69] 184 121 colSums(mite) # Brachy PHTH HPAV RARD SSTR Protopl MEGR MPRO # 611 89 596 85 22 26 153 11 # TVIE HMIN HMIN2 NPRA TVEL ONOV SUCT LCIL # 58 344 137 132 634 1209 1187 2468 # Oribatl1 Ceratoz1 PWIL Galumna1 Stgncrs2 HRUF Trhypch1 PPEL # 132 90 76 67 51 16 183 12 # NCOR SLAT FSET Lepidzts Eupelops Miniglmn LRUG PLAG2 # 79 28 130 12 45 17 730 56 # Ceratoz3 Oppiminu Trimalc2 # 91 78 145 De plus, l’AFC est très sensible à la présence de valeurs extrêmes. Il faut être particulièrement attentif à ne pas avoir trop de disparité, surtout en présence d’espèces très abondantes versus d’autres très rares. Si quelques échantillons contiennent une quantité particulièrement large d’items, cela peut aussi être problématique. Pour des dénombrements ou du semi-quantitatif à plus d’une dizaine de niveaux, la boite de dispersion parallèle est un bon graphique, mais nous devons tronsformer le tableau de données en format long avec gather() pour pouvoir le faire, et ensuite, nous présentons les espèces sur l’axe des ordonnées avec coord_flip() : mite %&gt;.% gather(., key = &quot;species&quot;, value = &quot;n&quot;) %&gt;.% chart(., n ~ species) + geom_boxplot() + coord_flip() Aïe ! Nous avons ici une magnifique valeur extrême, ainsi que des différences trop nettes entre les espèces les plus abondantes et les plus rares. Nous devons y remédier avant de faire notre AFC. Deux solutions sont possibles : Soit nous dégradons l’information vers du semi-quantitatif en définissant des niveaux d’abondance, du genre 0, 1-10, 10-20, 20-30, 31+. Nous pouvons réaliser cela avec la fonction cut() illustrée ici sur un exemple fictif. sample &lt;- c(0, 12, 500, 25, 5) cut(sample, breaks = c(-1, 0, 10, 20, 30, Inf)) # [1] (-1,0] (10,20] (30,Inf] (20,30] (0,10] # Levels: (-1,0] (0,10] (10,20] (20,30] (30,Inf] Nous voyons que cut() a ici remplacé nos données quantitatives dans sample() en une variable qualitative à cinq niveaux clairement libellés. Notez l’astuce du groupe (-1,0] pour capturer les valeurs nulles dans un groupe séparé. Autre solution, nous transformons les données pour réduire l’écart entre les extrêmes. Typiquement, une transformation \\(log(x + 1)\\) est efficace pour cela, et fonctionne bien en présence de valeurs nulles aussi. Nous utiliserons cette dernière technique pour notre jeu de données mite. Enfin, nous nous assurons que les station sont bien numérotées de 1 à 70 en lignes (rownames()). mite2 &lt;- log1p(as.data.frame(mite)) # Ajouter le numéro des stations explicitement rownames(mite2) &lt;- 1:nrow(mite2) Le graphique boite de dispersion parallèle montre plus d’homogénéité maintenant : mite2 %&gt;.% gather(., key = &quot;species&quot;, value = &quot;log_n_1&quot;) %&gt;.% chart(., log_n_1 ~ species) + geom_boxplot() + coord_flip() A noter que ce type de graphique ne convient pas pour les données semi-quantitatives ou qualitatives. Nous pouvons réaliser le graphique suivant à la place dans ce cas pour visualiser la distribution des espèces dans les différentes stations (qui fonctionne aussi en quantitatif) : mite2 %&gt;.% gather(., key = &quot;species&quot;, value = &quot;n&quot;) %&gt;.% mutate(., station = rep(1:nrow(mite), ncol(mite))) %&gt;.% chart(., species ~ station %fill=% n) + geom_raster() Maintenant que nos données sont vérifiées, nettoyées (phase de préparation), et décrites correctement (phase descriptive), nous pouvons réaliser notre AFC et explorer les particularités de ce jeu de données en vue d’une étude plus approfondie ultérieure (phase exploratoire). L’utilisation de ca() sur un tableau de type contingence à double entrée déjà prêt est ultra-simple. Il suffit de fournir le tableau en question comme seul argument à la fonction. mite2_ca &lt;- ca(mite2) Examinons la variabilité sur les différents axes, numériquement et à l’aide du graphe des éboulis : summary(mite2_ca, scree = TRUE, rows = FALSE, columns = FALSE) # # Principal inertias (eigenvalues): # # dim value % cum% scree plot # 1 0.366209 31.5 31.5 ******** # 2 0.132783 11.4 42.9 *** # 3 0.072315 6.2 49.1 ** # 4 0.065787 5.7 54.7 * # 5 0.055872 4.8 59.5 * # 6 0.048122 4.1 63.7 * # 7 0.041834 3.6 67.3 * # 8 0.039072 3.4 70.6 * # 9 0.032183 2.8 73.4 * # 10 0.031300 2.7 76.1 * # 11 0.028809 2.5 78.6 * # 12 0.026949 2.3 80.9 * # 13 0.024701 2.1 83.0 * # 14 0.022729 2.0 84.9 # 15 0.020618 1.8 86.7 # 16 0.018029 1.5 88.3 # 17 0.016836 1.4 89.7 # 18 0.014850 1.3 91.0 # 19 0.014217 1.2 92.2 # 20 0.012553 1.1 93.3 # 21 0.010879 0.9 94.2 # 22 0.010412 0.9 95.1 # 23 0.009703 0.8 96.0 # 24 0.008187 0.7 96.7 # 25 0.007294 0.6 97.3 # 26 0.006689 0.6 97.9 # 27 0.005343 0.5 98.3 # 28 0.004576 0.4 98.7 # 29 0.004020 0.3 99.1 # 30 0.003828 0.3 99.4 # 31 0.002732 0.2 99.6 # 32 0.001955 0.2 99.8 # 33 0.001656 0.1 99.9 # 34 0.000776 0.1 100.0 # -------- ----- # Total: 1.163821 100.0 chart$scree(mite2_ca, fill = &quot;cornsilk&quot;) La variabilité décroit rapidement sur les deux premiers axes, et ensuite plus lentement. Les deux premières dimensions ne capturent qu’environ 43% de la variabilité totale. Toutefois, la variation moins brutale par après justifie de ne garder que deux axes. Malgré la représentativité relativement faible dans ce plan, nous verrons que l’AFC reste interprétable et peut fournir des informations utiles, même dans ce cas. Voici la carte (biplot) : chart$biplot(mite2_ca, choices = c(1, 2)) Nous observons une forme en fer à cheval de la distribution des points. Attention ! Ceci est un artéfact lié au fait que la distance du Chi2 a tendance à rapprocher les extrêmes (“qui se ressemblent dans la différence”, en quelque sorte) alors que notre souhait aurait plutôt été de les placer aux extrémités du graphique. Il faut donc analyser les données comme une transition progressive d’un état A vers un état B le long de la forme en fer à cheval. Lorsqu’il y a beaucoup de points, ce graphique tend à être très encombré et illisible. L’argument repel = TRUE vient parfois aider en allant placer les labels de telle façon qu’ils se chevauchent le moins possible. Par contre, la forme du nuage de point est du coup un peu moins visible. chart$biplot(mite2_ca, choices = c(1, 2), repel = TRUE) Nous vous laissons le soin d’effectuer l’analyse complète des points bleus (stations) entre eux, ensuite les points rouges (espèces) entre eux et enfin, conjointement. Notez les stations 44, 59, 65 et 67 qui se détachent et qui sont caractérisées par des espèces qu’on trouve préférentiellement là-bas : Trhypch1 et Trimalc2. Les espèces et station proches du centre d’inertie, au contraire, ne montrent aucune particularité. 7.2.4 Principe de l’AFC Maintenant que nous avons découvert la façon de réaliser et d’interpréter une AFC, il est temps d’en comprendre les rouages. Rappelez-vous que la distance du Chi2 quantifie l’écart entre des effectifs observés \\(a_i\\) et des effectifs théoriques \\(\\alpha_i\\) comme : \\[\\chi^2=\\sum{\\frac{(a_i - \\alpha_i)^2}{\\alpha_i}}\\] Il y a un terme par cellule du tableau de contingence (que nous appelerons les contributions au Chi2), et par ailleurs, sous l’hypothèse nulle d’indépendance des variables, les \\(\\alpha_i\\) sont connus et dépendent uniquement des sommes des lignes et des colonnes du tableau. En effet, nous avons : \\[\\alpha_i = \\frac{\\textrm{total ligne} . \\textrm{total colonne}}{\\textrm{total général}}\\] Donc, lors du calcul du Chi2, nous passons par une étape de transformation du tableau de contingence à double entrée où nous substituons les effectifs observés (des entiers, donc du quantitatif discret) par les contributions respectives au Chi2. Or ces contributions sont des valeurs réelles (continues) nulles ou positives. Nous obtenons donc un tableau contenant des valeurs numériques continues qui peut être traité par une ACP classique. L’ACP va reprojetter les lignes du tableau (qui, rappelons-le, sont les différents niveaux d’une des deux variables qualitative étudiée) dans un espace réduit. Dans ce contexte, le graphique représentant les variables n’a pas grand intérêt, puique ces variables sont en réalité fictives, ou si vous préférez, “bricolées”. Par contre, la position des individus les uns par rapport aux autres, et par rapport au centre d’inertie du graphique a une signification importante et interprétable. Enfin, comme il n’y a aucune raison a priori d’utiliser une variable en ligne et l’autre en colonne, nous pouvons également transposer la table de contingence (les lignes deviennent les colonnes et vice versa). Si nous réalisons une nouvelle ACP sur ce tableau transposé, nous allons reprojetter les niveaux de l’autre variable qualitative sur un autre espace réduit. Si nous prenons le même nombre de composantes principales des deux côtés, nous aurons la même part de variance reprise dans les deux projections. De plus, nous pouvons superposer les deux représentations en faisant coïncider les deux centres d’inertie en {0, 0}. La difficulté, comme pour tout biplot, est d’arriver à mettre à l’échelle les points d’une représentation par rapport à ceux de l’autre. Cette mise à l’échelle permet d’interpréter les points de couleurs différentes conjointement : des points proches sur le biplot dénotent des correspondances entre les niveaux respectifs des deux variables étudiées, d’où le nom de la méthode, analyse factorielle des correspondances. Au final, l’AFC apparait donc comme une variante de l’ACP qui utilise la distance du Chi2 à la place de la distance euclidienne dans l’ACP classique, et qui reflète la symétrie du tableau de contingence (lignes/colonnes versus colonnes/lignes). Il en résulte la possiblité d’analyser ainsi des données qualitatives pour lesquelles la distance et la statistique du Chi2 ont précisément été inventées. Pour en savoir plus Une courte introduction de l’AFC en vidéo avec résolution d’un exemple volontairement simpliste pour faire comprendre la signification du graphique obtenu et la façon de l’interpréter. Une autre explication qui utilise les packages FactoMineR et factoextra et qui va plus loin dans les concepts (notamment la qualité de la représentation des points via leur “cos2”, les différents types de biplots, et l’ajout de données supplémentaires). A vous de jouer ! Réalisez le début du projet spécifique lié au module 7. Ce module couvre la matière entière du module 7. Le lien pour réaliser ce projet se trouve en début du module 7 Ce projet doit être terminé à la fin de ce module Complétez votre projet sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Complétez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea/blob/master/README.md Le README est un rappel des consignes, il ne s’agit aucunement du lien pour débuter le travail Attention : les valeurs sur les axes ne sont pas à interpréter quantitativement. Elles ne portent pas une information utile ici.↩ "],
["acces-aux-bases-de-donnees.html", "7.3 Accès aux bases de données", " 7.3 Accès aux bases de données Puisque nous traitons de jeux de données multivariés potentiellement très gros, il devient important de pouvoir accéder aux données stockées de manière plus structurée que dans un fichier au format CSV ou Excel, par exemple. Les bases de données, et notamment les bases de données relationnelles, sont prévue pour stocker de grandes quantités de données de manière structurée et pour pouvoir en extraire la partie qui nous intéresse à l’aide de requêtes. Nous allons ici étudier les rudiments indispensables pour nous permettre de réaliser de telles requêtes depuis R. SQL est un langage dédié aux requêtes et la manipulation de bases de données relationnelles, constituées de tables (équivalent à des tableaux cas par variables en statistiques) reliées entre elles par une ou plusieurs clés. Par exemple, le champ auteur d’une liste de livres dans la table Livres renvoie (est lié à) vers le champs nom d’une autre table Ecrivains qui fournit plus de détails sur chaque auteur de livres. Il existe différents moteurs de bases de données relationnelles. Les plus courants sont : SQLite, MySQL/MariaDB, PosgreSQL, SQL Server, Oracle, … La plupart de ces solutions nécessitent d’installer un serveur de base de données centralisé. Cependant, SQLite, est une solution légère qui permet d’explorer le language SQL (prononcez “S.Q.L.” ou “Sequel”), y compris avec des petites bases de données test en mémoire ou contenues dans un fichier. 7.3.1 Installation de SQLite Dans la SciViews Box, les drivers SQLite pour la version 2 et la version 3 sont préinstallés. Sous R, vous pouvez utiliser le package RSQLite pour accéder à des bases de données qui sont de simples fichiers sur le disque. Cependant, l’onglet Connections dans RStudio n’est pas compatible avec RSQLite. Il fonctionne, par contre avec les drivers odbc qui sont un format commun de drivers pour différentes bases de donnes dont SQLite. Les drivers SQLite, MySQL et PosgreSQL sont préinstallés dans la SciViews Box. Nous utiliserons également une interfaces graphique vers SQLite : DB Browser for SQLite disponible depuis le menu principal Applications, dans la section Development. 7.3.2 Base de données en mémoire La simplicité de SQLite tient au fait qu’il n’est pas nécessaire d’installer un serveur de bases de données pour l’utiliser. La version la plus simple permet même de travailler directement en mémoire. Ainsi, nous pouvons facilement placer le contenu d’un jeu de données comme mtcars et tester ensuite des requêtes SQL (l’équivalent des fonctions d’extraction et de remaniement de tableau dans dplyr) sur ces données en mémoire. Voici un petit aperçu qui vous montre comment créer, et puis manipuler une base de données SQLite en mémoire depuis R. library(&#39;RSQLite&#39;) # Base de données en mémoire con &lt;- dbConnect(RSQLite::SQLite(), dbname = &quot;:memory:&quot;) # Ne contient encore rien dbListTables(con) # character(0) # Ajoute une table dbWriteTable(con, &quot;mtcars&quot;, mtcars) dbListTables(con) # [1] &quot;mtcars&quot; # Que contient cette table? dbListFields(con, &quot;mtcars&quot;) # [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; # [11] &quot;carb&quot; # Lire toute la table dbReadTable(con, &quot;mtcars&quot;) # mpg cyl disp hp drat wt qsec vs am gear carb # 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 # 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 # 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 # 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 # 5 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 # 6 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 # 7 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 # 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 # 9 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 # 10 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 # 11 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 # 12 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 # 13 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 # 14 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 # 15 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 # 16 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 # 17 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 # 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 # 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 # 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 # 21 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 # 22 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 # 23 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 # 24 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 # 25 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 # 26 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 # 27 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 # 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 # 29 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 # 30 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 # 31 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 # 32 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 # Effectuer une requête SQL sur la table res &lt;- dbSendQuery(con, &quot;SELECT * FROM mtcars WHERE cyl = 4&quot;) dbFetch(res) # mpg cyl disp hp drat wt qsec vs am gear carb # 1 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 # 2 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 # 3 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 # 4 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 # 5 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 # 6 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 # 7 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 # 8 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 # 9 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 # 10 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 # 11 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 dbClearResult(res) # On peut aussi récupérer les données morceau par morceau res &lt;- dbSendQuery(con, &quot;SELECT * FROM mtcars WHERE cyl = 4&quot;) while (!dbHasCompleted(res)) { chunk &lt;- dbFetch(res, n = 5) print(nrow(chunk)) } # [1] 5 # [1] 5 # [1] 1 dbClearResult(res) # Se déconnecter de la base de données dbDisconnect(con) 7.3.3 Base de données dans un fichier La SciViews Box contient une base de données test dans /home/sv/shared/database.sqlite. Vous pouvez facilement vous connecter dessus via l’onglet Connections de RStudio. Vous pouvez également vous y connecter via du code R directement. Notez que la même syntaxe est utilisée pour créer une nouvelle base de données si le fichier n’existe pas encore au moment de la connexion. library(&#39;RSQLite&#39;) con &lt;- dbConnect(SQLite(), dbname = &quot;/home/sv/shared/database.sqlite&quot;) Voici quelques instructions typiques pour interroger cette base de données depuis R : # Liste les tables présentes dans la base de données dbListTables(con) # [1] &quot;iris&quot; &quot;mtcars&quot; &quot;versicolor&quot; # Extraction de données à l&#39;aide d&#39;une requête SQL (setosa &lt;- dbGetQuery(con, &quot;SELECT * FROM iris WHERE Species is &#39;setosa&#39;&quot;)) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3.0 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5.0 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa # 7 4.6 3.4 1.4 0.3 setosa # 8 5.0 3.4 1.5 0.2 setosa # 9 4.4 2.9 1.4 0.2 setosa # 10 4.9 3.1 1.5 0.1 setosa # 11 5.4 3.7 1.5 0.2 setosa # 12 4.8 3.4 1.6 0.2 setosa # 13 4.8 3.0 1.4 0.1 setosa # 14 4.3 3.0 1.1 0.1 setosa # 15 5.8 4.0 1.2 0.2 setosa # 16 5.7 4.4 1.5 0.4 setosa # 17 5.4 3.9 1.3 0.4 setosa # 18 5.1 3.5 1.4 0.3 setosa # 19 5.7 3.8 1.7 0.3 setosa # 20 5.1 3.8 1.5 0.3 setosa # 21 5.4 3.4 1.7 0.2 setosa # 22 5.1 3.7 1.5 0.4 setosa # 23 4.6 3.6 1.0 0.2 setosa # 24 5.1 3.3 1.7 0.5 setosa # 25 4.8 3.4 1.9 0.2 setosa # 26 5.0 3.0 1.6 0.2 setosa # 27 5.0 3.4 1.6 0.4 setosa # 28 5.2 3.5 1.5 0.2 setosa # 29 5.2 3.4 1.4 0.2 setosa # 30 4.7 3.2 1.6 0.2 setosa # 31 4.8 3.1 1.6 0.2 setosa # 32 5.4 3.4 1.5 0.4 setosa # 33 5.2 4.1 1.5 0.1 setosa # 34 5.5 4.2 1.4 0.2 setosa # 35 4.9 3.1 1.5 0.2 setosa # 36 5.0 3.2 1.2 0.2 setosa # 37 5.5 3.5 1.3 0.2 setosa # 38 4.9 3.6 1.4 0.1 setosa # 39 4.4 3.0 1.3 0.2 setosa # 40 5.1 3.4 1.5 0.2 setosa # 41 5.0 3.5 1.3 0.3 setosa # 42 4.5 2.3 1.3 0.3 setosa # 43 4.4 3.2 1.3 0.2 setosa # 44 5.0 3.5 1.6 0.6 setosa # 45 5.1 3.8 1.9 0.4 setosa # 46 4.8 3.0 1.4 0.3 setosa # 47 5.1 3.8 1.6 0.2 setosa # 48 4.6 3.2 1.4 0.2 setosa # 49 5.3 3.7 1.5 0.2 setosa # 50 5.0 3.3 1.4 0.2 setosa La dernière instruction nécessite quelques explications supplémentaires. La fonction dbGetQuery() envoie une requête sur la base en langage SQL. Ici, nous indiquons les colonnes que nous souhaitons récupérer avec le mot clé SELECT. L’utilisation de * indique que nous voulons toutes les colonnes, sinon, on nomme celles que l’on veut à la place. Ensuite, le mot clé FROM nous indique depuis qulle table, ici celle nommée iris, et enfin, le mot clé WHERE introduit une expression de condition qui va filtrer les lignes de la table à récupérer, par exemple 'Sepal.Length' &gt; 1.5 ou comme ici Species is 'setosa'. Il est également possible d’effectuer une requête SQL directement dans un chunk. A la place d’indiquer ```{r}, on indiquera ```{sql, connection=con}, et nous pourrons alors directement indiquer la requête SQL dans le chunk : SELECT * FROM iris WHERE Species is &#39;setosa&#39; Tableau 7.1: Displaying records 1 - 10 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa Par défaut, cette requête est imprimée dans le document, et son résultat est perdu ensuite. Il est cependant possible de l’enregistrer sous un nom avec l’option de chunk output.var=. Dans ce cas, rien n’est imprimé, mais comme le résultat de la requête est contenu dans l’objet créé, il est facile de le manipuler dans R ensuite plus loin dans notre document. SELECT * FROM iris WHERE Species is &#39;virginica&#39; Ensuite, dans un chunk R, vous pouvez manipuler la table contenue dans virginica : nrow(virginica) # [1] 50 summary(virginica) # Sepal.Length Sepal.Width Petal.Length Petal.Width # Min. :4.900 Min. :2.200 Min. :4.500 Min. :1.400 # 1st Qu.:6.225 1st Qu.:2.800 1st Qu.:5.100 1st Qu.:1.800 # Median :6.500 Median :3.000 Median :5.550 Median :2.000 # Mean :6.588 Mean :2.974 Mean :5.552 Mean :2.026 # 3rd Qu.:6.900 3rd Qu.:3.175 3rd Qu.:5.875 3rd Qu.:2.300 # Max. :7.900 Max. :3.800 Max. :6.900 Max. :2.500 # Species # Length:50 # Class :character # Mode :character # # # Ne pas oublier de se déconnecter de la base de données une fois terminé. dbDisconnect(con) 7.3.4 Driver ODBC dans RStudio RStudio facilite l’utilisation de bases de données à condition d’utiliser un driver “compatible”. Nous avons installé un tel driver ODBC pour les bases de données SQLite. Pour nous connecter à /home/sv/shared/database.sqlite depuis RStudio, nous entrons dans l’onglet Connections et nous cliquons sur le bouton New Connection, ou nous cliquons sur la connection correspondante si elle est déjà créée dans la liste. Pour créer une nouvelle connection, sélectionnons SQLite3. Ensuite, nous rentrons Database=/home/sv/shared/database.sqlite comme paramètre dans la fenêtre suivante. Le bouton Test permet de vérifier que R/RStudio peut se connecter à cette base de données. Ensuite, dans Connect from:, vous pouvez choisir où vous voulez placer l’instruction de connexion. L’option Clipboard est intéressante. Elle place l’instruction dans le presse-papier et vous pouvez alors décider vous-même où la placer. Nous la placerons dans un chunk R dans notre notebook. library(odbc) con &lt;- dbConnect(odbc::odbc(), .connection_string = &quot;Driver={SQLite3};Database=/home/sv/shared/database.sqlite&quot;) Une fois connecté, vous pouvez voir le contenu de la base de données dans l’onglet Connections. Pour ajouter une table que nous remplissons à partir des données issues du jeu de données mtcars, nous écririons dans R : dbWriteTable(con, &quot;mtcars&quot;, mtcars) A partir de ce moment, vous pouvez voir votre table mtcars (il faut peut être cliquer sur le bouton en forme de flèche qui se mord la queue pour rafraichir l’affichage). Comme dans Environnement, si vous cliquez sur la flèche dans un rond bleu devant le nom de la table, vous pouvez voir les colonnes qu’elle contient. En cliquant sur l’icône tableau à droite, vous visualisez la table directement dans RStudio. A part cela, vous travaillez avec cette base de données dans R en utilisant l’objet con comme d’habitude, et vous pouvez aussi utiliser directement des chunks sql. 7.3.5 Utilisation de DB Browser Lancez DB Browser. Connectez-vous à /home/sv/shared/database.sqlite Vous avez un accès visuel à votre base de données. Explorez les différentes possibilités du logiciel. DB Browser for SQLite 7.3.6 Utilisation de dplyr Les fonctions du package dplyr fonctionnent aussi très bien sur des bases de données. Les commandes sont converties en interne en requêtes SQL. Il suffit d’utiliser collect() à la fin pour exécuter la requête. dbfile &lt;- &quot;/home/sv/shared/database.sqlite&quot; my_db &lt;- src_sqlite(dbfile) # Utiliser create = TRUE pour la créer my_db # src: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # tbls: iris, mtcars, versicolor my_table &lt;- tbl(my_db, sql(&quot;SELECT * FROM iris&quot;)) (df2 &lt;- collect(my_table)) # # A tibble: 150 x 5 # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3 1.4 0.2 setosa # 3 4.7 3.2 1.3 0.2 setosa # 4 4.6 3.1 1.5 0.2 setosa # 5 5 3.6 1.4 0.2 setosa # 6 5.4 3.9 1.7 0.4 setosa # 7 4.6 3.4 1.4 0.3 setosa # 8 5 3.4 1.5 0.2 setosa # 9 4.4 2.9 1.4 0.2 setosa # 10 4.9 3.1 1.5 0.1 setosa # # … with 140 more rows Voici maintenant ce que cela donne en utilisant les verbes de dplyr. La fonction explain() permet d’expliquer ce qui est fait. # Sélectionner des variables select(my_table, Sepal.Width, Petal.Length:Species) # # Source: lazy query [?? x 4] # # Database: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; # 1 3.5 1.4 0.2 setosa # 2 3 1.4 0.2 setosa # 3 3.2 1.3 0.2 setosa # 4 3.1 1.5 0.2 setosa # 5 3.6 1.4 0.2 setosa # 6 3.9 1.7 0.4 setosa # 7 3.4 1.4 0.3 setosa # 8 3.4 1.5 0.2 setosa # 9 2.9 1.4 0.2 setosa # 10 3.1 1.5 0.1 setosa # # … with more rows # Filtrer les fleurs à gros pétales filter(my_table, Petal.Length &gt; 1.5) # # Source: lazy query [?? x 5] # # Database: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; # 1 5.4 3.9 1.7 0.4 setosa # 2 4.8 3.4 1.6 0.2 setosa # 3 5.7 3.8 1.7 0.3 setosa # 4 5.4 3.4 1.7 0.2 setosa # 5 5.1 3.3 1.7 0.5 setosa # 6 4.8 3.4 1.9 0.2 setosa # 7 5 3 1.6 0.2 setosa # 8 5 3.4 1.6 0.4 setosa # 9 4.7 3.2 1.6 0.2 setosa # 10 4.8 3.1 1.6 0.2 setosa # # … with more rows # Réarranger les lignes par longueur de pétale croissante et largeur de sépale décroissant arrange(my_table, Petal.Length, desc(Sepal.Width)) # # Source: SQL [?? x 5] # # Database: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # # Ordered by: Petal.Length, desc(Sepal.Width) # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; # 1 4.6 3.6 1 0.2 setosa # 2 4.3 3 1.1 0.1 setosa # 3 5.8 4 1.2 0.2 setosa # 4 5 3.2 1.2 0.2 setosa # 5 5.4 3.9 1.3 0.4 setosa # 6 5.5 3.5 1.3 0.2 setosa # 7 5 3.5 1.3 0.3 setosa # 8 4.7 3.2 1.3 0.2 setosa # 9 4.4 3.2 1.3 0.2 setosa # 10 4.4 3 1.3 0.2 setosa # # … with more rows # Créer une nouvelle variables mutate(my_table, logPL = log10(Petal.Length)) # # Source: lazy query [?? x 6] # # Database: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # Sepal.Length Sepal.Width Petal.Length Petal.Width Species logPL # &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; # 1 5.1 3.5 1.4 0.2 setosa 0.146 # 2 4.9 3 1.4 0.2 setosa 0.146 # 3 4.7 3.2 1.3 0.2 setosa 0.114 # 4 4.6 3.1 1.5 0.2 setosa 0.176 # 5 5 3.6 1.4 0.2 setosa 0.146 # 6 5.4 3.9 1.7 0.4 setosa 0.230 # 7 4.6 3.4 1.4 0.3 setosa 0.146 # 8 5 3.4 1.5 0.2 setosa 0.176 # 9 4.4 2.9 1.4 0.2 setosa 0.146 # 10 4.9 3.1 1.5 0.1 setosa 0.176 # # … with more rows # Résumer les données explain(summarise(my_table, taille = median(Petal.Length))) # &lt;SQL&gt; # SELECT MEDIAN(`Petal.Length`) AS `taille` # FROM (SELECT * FROM iris) # # &lt;PLAN&gt; # addr opcode p1 p2 p3 p4 p5 comment # 1 0 Init 0 12 0 00 NA # 2 1 Null 0 1 2 00 NA # 3 2 OpenRead 1 3 0 5 00 NA # 4 3 Rewind 1 8 0 00 NA # 5 4 Column 1 2 3 00 NA # 6 5 RealAffinity 3 0 0 00 NA # 7 6 AggStep0 0 3 1 median(1) 01 NA # 8 7 Next 1 4 0 01 NA # 9 8 AggFinal 1 1 0 median(1) 00 NA # 10 9 Copy 1 4 0 00 NA # 11 10 ResultRow 4 1 0 00 NA # 12 11 Halt 0 0 0 00 NA # 13 12 Transaction 0 0 45 0 01 NA # 14 13 Goto 0 1 0 00 NA Il est possible de réaliser des choses plus complexes ! On peut naturellement chainer tout cela avec le pipe %&gt;.%, ou combiner les requêtes comme on veut. Rien n’est fait avant de collect()er les résultats. my_table %&gt;.% filter(., Petal.Length &gt; 1.5) %&gt;.% select(., Petal.Length, Sepal.Width, Species) %&gt;.% mutate(., logPL = log10(Petal.Length)) -&gt; query1 query2 &lt;- arrange(query1, Petal.Length, desc(Sepal.Width)) query2 # # Source: lazy query [?? x 4] # # Database: sqlite 3.22.0 [/media/sf_shared/database.sqlite] # # Ordered by: Petal.Length, desc(Sepal.Width) # Petal.Length Sepal.Width Species logPL # &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; # 1 1.6 3.8 setosa 0.204 # 2 1.6 3.5 setosa 0.204 # 3 1.6 3.4 setosa 0.204 # 4 1.6 3.4 setosa 0.204 # 5 1.6 3.2 setosa 0.204 # 6 1.6 3.1 setosa 0.204 # 7 1.6 3 setosa 0.204 # 8 1.7 3.9 setosa 0.230 # 9 1.7 3.8 setosa 0.230 # 10 1.7 3.4 setosa 0.230 # # … with more rows # Récupérer le résultat res &lt;- collect(query2) res # # A tibble: 113 x 4 # Petal.Length Sepal.Width Species logPL # &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; # 1 1.6 3.8 setosa 0.204 # 2 1.6 3.5 setosa 0.204 # 3 1.6 3.4 setosa 0.204 # 4 1.6 3.4 setosa 0.204 # 5 1.6 3.2 setosa 0.204 # 6 1.6 3.1 setosa 0.204 # 7 1.6 3 setosa 0.204 # 8 1.7 3.9 setosa 0.230 # 9 1.7 3.8 setosa 0.230 # 10 1.7 3.4 setosa 0.230 # # … with 103 more rows Enfin, la fonction sql_translate() du package dbplyr va indiquer comment une instruction R est convertie en code SQL équivalent. C’est très pratique aussi pour apprendre SQL quand on connait R ! dbplyr::translate_sql(x^3 &lt; 15 || y &gt; 20) # &lt;SQL&gt; POWER(&quot;x&quot;, 3.0) &lt; 15.0 OR &quot;y&quot; &gt; 20.0 dbplyr::translate_sql(mean(x)) # Warning: Missing values are always removed in SQL. # Use `avg(x, na.rm = TRUE)` to silence this warning # &lt;SQL&gt; avg(&quot;x&quot;) OVER () dbplyr::translate_sql(mean(x, na.rm = TRUE)) # &lt;SQL&gt; avg(&quot;x&quot;) OVER () # Tout ne fonctionne pas, car R offre plus de possibilités que SQL dbplyr::translate_sql(plot(x)) #??? # &lt;SQL&gt; PLOT(&quot;x&quot;) dbplyr::translate_sql(mean(x, trim = TRUE)) # Error in mean(x, trim = TRUE): unused argument (trim = TRUE) Voilà pour ce très rapide tour d’horizon des différentes façons de manipuler des bases de données avec R et RStudio. En pratique, revenez sur cette section, et approfondissez vos connaissances via les ressources proposées ci-dessous lorsque vous serez confrontés “en vrai” à des données présentées dans une base de données relationnelle. Pour en savoir plus RStudio et bases de données : tout un site web dédié à l’accès aux bases de données depuis RStudio (en anglais). La documentation de DB Browser for SQLite (en anglais). Une introduction des requêtes SQL dans R un peu plus développée (en anglais). Un tutorial SQL avec des exercices (en anglais). Un cours en ligne sur SQL par vidéos par la Kahn Academy (en anglais). Un autre tutorial complet sur SQL. Remarquez qu’il en existe beaucoup. Faites une recherche via Google et choisissez le tutorial qui vous plait le plus. Les dix commandements d’une base de données réussie. Il s’agit ici plutôt de la création d’une base de données que de la requête sur une base de données existante… mais tôt ou tard, vous créerez vos propres bases de données et ces conseils vous seront alors utiles. A vous de jouer ! Réalisez le début du projet spécifique lié au module 7. Ce module couvre la matière entière du module 7. Le lien pour réaliser ce projet se trouve en début du module 7 Ce projet doit être terminé à la fin de ce module "],
["afm.html", "Module 8 AFM, biodiversité et Open Data", " Module 8 AFM, biodiversité et Open Data Objectifs Être capable d’analyser des données présentes dans des tableaux multiples et/ou hétérogènes simultanément à l’aide de l’analyse factorielle multiple Comprendre et pouvoir utiliser en pratique les principaux indices de biodiversité Se sensibiliser à l’“Open Data” : quand et pourquoi rendre ses données ouvertes ? Comment le faire correctement (format, métadonnées, dictionnaire des données, licences, principe FAIR, DMP, …) Prérequis Les modules 5, 6 et 7 doivent être assimilés avant d’attaquer le présent module. A vous de jouer ! En lien avec ce module vous avez une série d’exercices à réaliser. Il vous faut réaliser un projet spécifique et dédié uniquement au module 08. Vous avez à votre disposition une assignation GitHub Classroom. Pour l’année académique 2019-2020, les URLs à utiliser pour accéder à votre tâche sont les suivants : Cours de Sciences des données II à Mons : https://classroom.github.com/a/ytq74RNB Pour les autres utilisateurs de ce livre, veuillez faire un “fork” du dépôt sdd2_module08. Si vous souhaitez accéder à une version précédente particulière de l’exercice, sélectionnez la release correspondante à l’année que vous recherchez. Lisez le README afin de prendre connaissance de l’exercice. Ce projet doit être terminé à la fin de ce module "],
["analyse-factorielle-multiple-afm.html", "8.1 Analyse factorielle multiple (AFM)", " 8.1 Analyse factorielle multiple (AFM) Jusqu’ici nous avons étudié différentes techniques pour explorer des données multivariées soit quantitatives (ACP), soit qualitatives (AFC). Nous n’avons pas abordé encore la question de données mixtes avec des tableaux qui contiennent à la fois des variables quantitatives et des données qualitatives. Une première approche consiste à convertir des variables afin d’homogénéiser le tableau. La conversion ne peut se faire qu’en dégradant l’information, soit dans le sens quantitatif continu -&gt; quantitatif discret -&gt; qualitatif ordonné -&gt; qualitatif non ordonné -&gt; binaire. Ainsi, dans un tableau contenant quelques variables quantitives, nous pouvons créer des classes pour convertir les variables quantitatives en qualitatives. C’est par exemple ce que nous faisons quand nous remplaçons les indices de masse corporelle IMC dans nos données de biométrie humaine (quantitatif) en classes (tableau des classes proposées par l’OMS21). IMC [kg/m2] Classification OMS &lt; 16,5 sous-poids sévère 16 - 17 sous-poids modéré 17 - 18,5 sous-poids léger 18.5 - 25 normal 25 - 30 préobésité 30 - 35 obésité classe I 35 - 40 obésité classe II &gt; 40 obésité classe III A l’extrême, il est toujours possible d’encoder n’importe quelle variable sous forme binaire. C’est ce qu’on appelle le codage disjonctif complet (voir, par exemple, ici). Cette approche a non seulement l’avantage de transformer des variables différentes en 0 ou 1 dans un tableau homogène, mais permet aussi de comparer plus de deux variables qualitatives selon une extension de l’AFC dit Analyse en Composantes Multiples ou ACM que nous ne verrons pas dans ce cours mais qu’il est important de mentionner pour mémo ici22. L’analyse factorielle multiple (AFM, multiple factorial analysis ou MFA en anglais) va permettre d’analyser simultanément plusieurs tableaux de données multivariés, et éventuellement mélanger des tableaux quantititfs et qualitatifs (pour autant que les variables soient homogènes au sein de chaque tableau). 8.1.1 AFM dans SciViews::R L’AFM n’est pas encore intégrée dans SciViews::R. Le code suivant permet de réaliser notre analyse via l’interface habituelle, soit mfa(data = ...., formula) pour construire l’analyse (calculs en internes basés sur FactoMineR::MFA()), et ensuite nous utiliserons summary() et chart$type() pour les graphiques, où type pourra être “screeplot”, “altscreeplot”, “loadings”, “scores”, “groups”, “axes”, “frequences” ou “ellipses” (nous verrons plus loin ce que représentent tous ces graphiques). Vous pouvez copier-coller le code du chunk suivant au début de vos scripts ou dans un chunk de vos documents R Markdown/Notebook pour l’utiliser. # Code mfa() version 2 SciViews::R() mfa &lt;- function(data, formula, nd = 5, suprow = NA, ..., graph = FALSE) { # TODO: rework Call in the final MFA object if (is.na(suprow)) suprow &lt;- NULL # MFA uses NULL instead of NA! if (!rlang::is_formula(formula)) stop(&quot;&#39;formula&#39; must be a formula object&quot;) if (!is.null(rlang::f_lhs(formula))) stop(&quot;&#39;formula cannot have left-hand side (must be ~ n1 * type...&quot;) params &lt;- get_groups(list(expr = rlang::f_rhs(formula))) # data must be a data frame if (!inherits(data, &quot;data.frame&quot;)) stop(&quot;&#39;data&#39; must be a data.frame&quot;) data &lt;- as.data.frame(data) # No tibble or data.table! # Number of columns in data must match specifications in the formula if (ncol(data) != sum(params$groups)) stop(&quot;You must specify groups in &#39;formula&#39; for all the columns in &#39;data&#39;&quot;) # Call FactoMineR::MFA() with corresponding arguments FactoMineR::MFA(base = data, group = params$groups, type = params$types, ind.sup = suprow, ncp = nd, name.group = params$names, num.group.sup = params$suppl, ..., graph = graph) } # x is a list with: expr, groups, types, names, suppl # Note: this is a mess, I need to clean up this code! get_groups &lt;- function(x) { items &lt;- as.list(x$expr) # We should have here &#39;+&#39;, &#39;-&#39; or something else in case of last expression if (items[[1]] == &quot;+&quot;) { x$suppl &lt;- c(FALSE, x$suppl) # Not a supplementary variable x$expr &lt;- items[[2]] # Second item is next expression # Third item is the information for that group # (n, or n * type, or n * type %as% name) item &lt;- as.list(items[[3]]) } else if (items[[1]] == &quot;-&quot;) { x$suppl &lt;- c(TRUE, x$suppl) # A supplementary variable x$expr &lt;- items[[2]] # Second item is next expression # Third item is the information for that group # (n, or n * type, or n * type %as% name) item &lt;- as.list(items[[3]]) } else { # Last expression x$expr &lt;- NULL item &lt;- items } if (length(item) &lt; 3) { # either n, or +n or -n if (length(item) == 2) { x$suppl &lt;- switch(as.character(item[[1]]), &quot;-&quot; = c(TRUE, x$suppl), &quot;+&quot; = c(FALSE, x$suppl), stop(&quot;Bad &#39;formula&#39;, see help&quot;)) n &lt;- item[[2]] } else n &lt;- item[[1]] if (!is.numeric(n)) stop(&quot;Bad &#39;formula&#39;, see help&quot;) x$groups &lt;- c(as.integer(n, x$groups)) x$types &lt;- c(&quot;s&quot;, x$types) x$names &lt;- c(NA, x$names) } else if (item[[1]] == &quot;*&quot;) { # n * type or n * type %as% name if (length(item[[2]]) &gt; 2) stop(&quot;Bad &#39;formula&#39;, see help&quot;) if (length(item[[2]]) == 2) { x$suppl &lt;- switch(as.character(item[[2]][[1]]), &quot;-&quot; = c(TRUE, x$suppl), &quot;+&quot; = c(FALSE, x$suppl), stop(&quot;Bad &#39;formula&#39;, see help&quot;)) n &lt;- item[[2]][[2]] } else n &lt;- item[[2]] if (!is.numeric(n)) stop(&quot;Bad &#39;formula&#39;, see help&quot;) x$groups &lt;- c(as.integer(n), x$groups) # Right-hand side after &#39;*&#39;: type or type %as% name subitem &lt;- as.list(item[[3]]) if (subitem[[1]] == &quot;%as%&quot;) { # type %as name x$types &lt;- c(as.character(subitem[[2]]), x$types) x$names &lt;- c(as.character(subitem[[3]]), x$names) } else if (length(subitem) == 1 &amp;&amp; is.name(subitem[[1]])) { # type only x$types &lt;- c(as.character(subitem[[1]]), x$types) x$names &lt;- c(NA, x$names) } else stop(&quot;Bad &#39;formula&#39;, see help&quot;) # Error } else if (item[[1]] == &quot;%as%&quot;) { # n %as% name if (length(item[[2]]) == 2) { x$suppl &lt;- switch(as.character(item[[2]][[1]]), &quot;-&quot; = c(TRUE, x$suppl), &quot;+&quot; = c(FALSE, x$suppl), stop(&quot;Bad &#39;formula&#39;, see help&quot;)) n &lt;- item[[2]][[2]] } else n &lt;- item[[2]] if (!is.numeric(n)) stop(&quot;Bad &#39;formula&#39;, see help&quot;) x$groups &lt;- c(as.integer(n), x$groups) x$types &lt;- c(&quot;s&quot;, x$types) # type by default (s) x$names &lt;- c(as.character(item[[3]]), x$names) } else stop(&quot;Bad &#39;formula&#39;, see help&quot;) # Error # Is there another expression to evaluate? if (!is.null(x$expr)) { x &lt;- get_groups(x) } else { # Get correct vector for suppl if (length(x$suppl) &lt; length(x$groups)) x$suppl &lt;- c(FALSE, x$suppl) # We need the group indices for the supplementary variables instead x$suppl &lt;- (1:length(x$groups))[x$suppl] if (!length(x$suppl)) x$suppl &lt;- NULL # Fix names def_names &lt;- paste(&quot;group&quot;, 1:length(x$groups), sep = &quot;.&quot;) names &lt;- x$names names[is.na(names)] &lt;- def_names[is.na(names)] x$names &lt;- names } x } #form &lt;- ~ -4 %as% Environnement -24*f %as% Plancton +1*f +18 #get_groups(list(expr = rlang::f_rhs(form))) # TODO: methods for broom... autoplot.MFA &lt;- function(object, type = c(&quot;screeplot&quot;, &quot;altscreeplot&quot;, &quot;loadings&quot;, &quot;scores&quot;, &quot;groups&quot;, &quot;axes&quot;, &quot;frequences&quot;, &quot;ellipses&quot;), choices = 1L:2L, name = deparse(substitute(object)), col = &quot;black&quot;, fill = &quot;gray&quot;, title, ..., env) { type = match.arg(type) if (missing(title)) title &lt;- paste(name, type, sep = &quot; - &quot;) res &lt;- switch(type, screeplot = object %&gt;.% # Classical screeplot tibble::tibble(eig = .$eig[, 1], PC = 1:nrow(.$eig)) %&gt;.% chart(data = ., eig ~ PC) + geom_col(col = col, fill = fill) + labs(y = &quot;Eigenvalues&quot;, title = title), altscreeplot = object %&gt;.% # screeplot represented by dots and lines tibble::tibble(eig = .$eig[, 1], PC = 1:nrow(.$eig)) %&gt;.% chart(data = ., eig ~ PC) + geom_line(col = col) + geom_point(col = &quot;white&quot;, fill = col, size = 2, shape = 21, stroke = 3) + labs(y = &quot;Eigenvalues&quot;, title = title), loadings = object %&gt;.% # Plots of the variables plot(., axes = choices, choix = &quot;var&quot;, title = title, ...), scores = object %&gt;.% # Plot of the individuals plot(., axes = choices, choix = &quot;ind&quot;, title = title, ...), groups = object %&gt;.% # Plot of the groups plot(., axes = choices, choix = &quot;group&quot;, title = title, ...), axes = object %&gt;.% # Plot of the loadings for the various groups plot(., axes = choices, choix = &quot;axes&quot;, title = title, ...), frequences = object %&gt;.% # Plot of the correpondance analyses plot(., axes = choices, choix = &quot;freq&quot;, title = title, ...), ellipses = object %&gt;.% # Plot of the individuals with ellipses FactoMineR::plotellipses(., axes = choices, title = title, ...), stop(&quot;Unrecognized type, must be &#39;screeplot&#39;, &#39;altscreeplot&#39;, &#39;loadings&#39;, &#39;scores&#39;, &#39;groups&#39;, &#39;axes&#39;, &#39;frequences&#39;, or &#39;ellipses&#39;&quot;) ) if (inherits(res, &quot;ggplot&quot;)) { res } else { invisible(res) } } chart.MFA &lt;- function(data, choices = 1L:2L, name = deparse(substitute(data)), ..., type = NULL, env = parent.frame()) autoplot.MFA(data, choices = choices, name = name, ..., type = type, env = env) class(chart.MFA) &lt;- c(&quot;function&quot;, &quot;subsettable_type&quot;) Nous allons aborder cette technique puissante mais un peu plus complexe qu’est l’AFM sur la base d’un jeu de données que nous connaissons bien maintenant via nos exercices précédents : la communauté de plancton le long d’un transect entre Nice et la Corse. 8.1.2 Plancton en Méditerranée Les jeux de données marphy et marbio dans le package R pastecs présentent deux tableaux de données complémentaires. Le premier, marphy reprend les mesures environnementales de base (température, salinité et densité des masses d’eaux), ainsi qu’une information indirecte sur la concentration en phytoplancton : la fluorescence de la chlorophylle. Le second tableau marbio reprend les effectifs observés dans 24 groupes de zooplancton mesurés sur des échantillons issus des mêmes stations que marphy. Ces stations sont réparties de manière équidistante le long d’un transect entre le continent (vers Nice, station n°1) et la Corse (à Calvi, station n°68). Exemple d’organismes appartenant au zooplancton (Matt Wilson &amp; Jay Clark, NOAA NMFS AFSC, licence : domaine public) Jusqu’à présent, nous avions utilisé ces deux tableaux séparément. Notre objectif ici est de les analyser ensemble. Nous allons commencer par considérer ces deux tableaux comme quantitatifs. Dans ce cas, l’AFM réalisera deux ACP, une par tableau. Ensuite, elle pondèrera les données de chacun en fonction de la part de variance sur le premier axe respectif. Pour finir, elle réalisera une ACP globale avec toutes les données qui conservera la structure de chacun des deux tableaux initiaux grâce à la pondération appliquée en interne. Les données environmentales dans marphy étant mesurées dans des unités différentes, nous les standardiserons. Concernant le plancton dans marbio, nous allons pondérer les espèces rares par rapport aux espèces abondantes en tranformant \\(log(x + 1)\\) les abondances observées avec la fonction log1p(). Enfin, nous rajouterons les différentes masses d’eaux identifiées par ailleurs (voir l’aide ?pastecs::marphy) dans marzones que nous utiliserons comme troisième tableau de variables supplémentaires (un tableau que nous n’utilisons pas dans l’analyse, mais que nous représenterons dans les graphiques afin d’aider à leur interprétation). Voici donc la constitution d’un gros tableau qui reprend toutes les données dans la variable mar : marzones &lt;- factor(c( rep(&quot;périphérique&quot;, 16), rep(&quot;divergente1&quot;, 8), rep(&quot;convergente&quot;, 5), rep(&quot;frontale&quot;, 11), rep(&quot;divergente2&quot;, 5), rep(&quot;centrale&quot;, 23)), levels = c(&quot;périphérique&quot;, &quot;divergente1&quot;, &quot;convergente&quot;, &quot;frontale&quot;, &quot;divergente2&quot;, &quot;centrale&quot;)) bind_cols( read(&quot;marphy&quot;, package = &quot;pastecs&quot;), log1p(read(&quot;marbio&quot;, package = &quot;pastecs&quot;))) %&gt;.% mutate(., Zone = marzones) -&gt; mar Ce tableau n’a aucunes valeurs manquantes et il contient 29 variables : names(mar) # [1] &quot;Temperature&quot; &quot;Salinity&quot; # [3] &quot;Fluorescence&quot; &quot;Density&quot; # [5] &quot;Acartia&quot; &quot;AdultsOfCalanus&quot; # [7] &quot;Copepodits1&quot; &quot;Copepodits2&quot; # [9] &quot;Copepodits3&quot; &quot;Copepodits4&quot; # [11] &quot;Copepodits5&quot; &quot;ClausocalanusA&quot; # [13] &quot;ClausocalanusB&quot; &quot;ClausocalanusC&quot; # [15] &quot;AdultsOfCentropages&quot; &quot;JuvenilesOfCentropages&quot; # [17] &quot;Nauplii&quot; &quot;Oithona&quot; # [19] &quot;Acanthaires&quot; &quot;Cladocerans&quot; # [21] &quot;EchinodermsLarvae&quot; &quot;DecapodsLarvae&quot; # [23] &quot;GasteropodsLarvae&quot; &quot;EggsOfCrustaceans&quot; # [25] &quot;Ostracods&quot; &quot;Pteropods&quot; # [27] &quot;Siphonophores&quot; &quot;BellsOfCalycophores&quot; # [29] &quot;Zone&quot; Il y a trois groupes distrincts : les 4 premières colonnes comme variables quantitatives continues pour l’environnement, les 24 colonnes suivantes comme quantitatives pour le plancton, la dernière colonne qui caractérise les masses d’eaux (zone) comme variable qualitative supplémentaire. Représentation des trois groupes du tableau dans les couleurs qui seront utilisées ensuite dans l’AFM. Avant de pouvoir réaliser notre AFM, nous devons comprendre comment écrire la formule qui va décrire nos données. Pour chaque groupe, nous ajouterons un terme à la formule qui contient trois composante : Le nombre n de colonnes constituant le groupe (les colonnes du tableau composite sont comptées de gauche à droite), Le type de variable considérée. Quatre types sont possibles. Les variables continues à standardiser avant d’être utilisées dans une ACP sont de type s. Les variables quantitative également à utiliser selon l’ACP mais sans standardisation sont de type c. Les données qualitatives assimilables à un tableau de contingence à double entrée (données de fréquences d’observations, dénombrements, etc.) sont de type f et seront traitées comme une AFC, et enfin, les variables qualitatives classiques sont de type n et seront traitées selon une analyse factorielle multiple AFM. Le nom que l’on veut donner à chaque groupe de variables pour l’identifier dans l’analyse. Un groupe est décrit dans la formule comme n * type %as% nom (donc avec les opérateurs * et %as%). Chaque groupe inclu dans l’analyse est précédé d’un signe plus + (le premier plus peut être omis de la formule). Les groupes supplémentaires (à soustraire à l’analyse) sont précédés d’un signe moins -. Et enfin, tous les groupes sont décrits à la droite de la formule, donc, après le ~. Dans notre cas concret ici, cela donne : ~ 4*s %as% environnement + 24*c %as% plancton - 1*n %as% zone L’analyse se fait en appelant mfa(data = ...., formula). Nous assignons le résultat du calcul à mar_mfa : mar_mfa &lt;- mfa(data = mar, ~ 4*s %as% environment + 24*c %as% plancton - 1*n %as% zone) Le résumé de l’analyse renvoyé par summary() donne beaucoup d’information. summary(mar_mfa) # # Call: # FactoMineR::MFA(base = data, group = params$groups, type = params$types, # ind.sup = suprow, ncp = nd, name.group = params$names, num.group.sup = params$suppl, # graph = graph) # # # Eigenvalues # Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 # Variance 1.801 1.145 0.215 0.121 0.116 0.103 # % of var. 45.228 28.768 5.409 3.047 2.908 2.575 # Cumulative % of var. 45.228 73.996 79.405 82.452 85.360 87.935 # Dim.7 Dim.8 Dim.9 Dim.10 Dim.11 Dim.12 # Variance 0.092 0.063 0.057 0.047 0.032 0.030 # % of var. 2.320 1.578 1.430 1.179 0.804 0.754 # Cumulative % of var. 90.256 91.834 93.264 94.443 95.247 96.001 # Dim.13 Dim.14 Dim.15 Dim.16 Dim.17 Dim.18 # Variance 0.028 0.021 0.019 0.016 0.013 0.012 # % of var. 0.710 0.537 0.477 0.397 0.328 0.298 # Cumulative % of var. 96.712 97.249 97.726 98.124 98.452 98.750 # Dim.19 Dim.20 Dim.21 Dim.22 Dim.23 Dim.24 # Variance 0.011 0.010 0.008 0.005 0.005 0.004 # % of var. 0.276 0.242 0.211 0.123 0.115 0.097 # Cumulative % of var. 99.026 99.267 99.479 99.602 99.717 99.814 # Dim.25 Dim.26 Dim.27 Dim.28 # Variance 0.003 0.003 0.001 0.000 # % of var. 0.080 0.070 0.037 0.000 # Cumulative % of var. 99.893 99.963 100.000 100.000 # # Groups # Dim.1 ctr cos2 Dim.2 ctr cos2 Dim.3 # environment | 0.864 47.958 0.408 | 0.967 84.430 0.511 | 0.056 # plancton | 0.937 52.042 0.781 | 0.178 15.570 0.028 | 0.160 # ctr cos2 # environment 25.844 0.002 | # plancton 74.156 0.023 | # # Supplementary group # Dim.1 cos2 Dim.2 cos2 Dim.3 cos2 # zone | 0.849 0.144 | 0.753 0.113 | 0.220 0.010 | # # Individuals (the 10 first) # Dim.1 ctr cos2 Dim.2 ctr cos2 Dim.3 # 1 | -3.029 7.493 0.788 | -0.153 0.030 0.002 | -1.203 # 2 | -2.826 6.522 0.907 | 0.100 0.013 0.001 | -0.613 # 3 | -2.986 7.280 0.923 | -0.015 0.000 0.000 | -0.558 # 4 | -2.817 6.479 0.917 | -0.040 0.002 0.000 | -0.324 # 5 | -2.988 7.290 0.913 | 0.116 0.017 0.001 | -0.530 # 6 | -2.850 6.635 0.889 | 0.319 0.131 0.011 | -0.470 # 7 | -2.561 5.357 0.796 | 0.807 0.835 0.079 | -0.306 # 8 | -2.434 4.837 0.761 | 0.676 0.586 0.059 | -0.012 # 9 | -2.192 3.923 0.651 | 0.832 0.889 0.094 | 0.200 # 10 | -1.526 1.902 0.708 | 0.355 0.161 0.038 | 0.642 # ctr cos2 # 1 9.881 0.124 | # 2 2.569 0.043 | # 3 2.126 0.032 | # 4 0.716 0.012 | # 5 1.915 0.029 | # 6 1.511 0.024 | # 7 0.638 0.011 | # 8 0.001 0.000 | # 9 0.272 0.005 | # 10 2.815 0.125 | # # Continuous variables (the 10 first) # Dim.1 ctr cos2 Dim.2 ctr cos2 Dim.3 # Temperature | -0.526 7.649 0.277 | -0.756 24.810 0.571 | 0.281 # Salinity | 0.676 12.621 0.457 | -0.718 22.364 0.515 | -0.084 # Fluorescence | 0.600 9.929 0.359 | 0.737 23.603 0.544 | -0.065 # Density | 0.802 17.758 0.643 | -0.561 13.654 0.314 | -0.147 # Acartia | 0.007 0.000 0.000 | 0.711 2.381 0.384 | 0.410 # AdultsOfCalanus | 1.721 8.883 0.873 | -0.084 0.033 0.002 | 0.102 # Copepodits1 | 1.662 8.288 0.866 | -0.245 0.283 0.019 | 0.265 # Copepodits2 | 0.708 1.503 0.381 | -0.041 0.008 0.001 | 0.400 # Copepodits3 | 0.764 1.750 0.625 | 0.272 0.349 0.079 | 0.104 # Copepodits4 | 0.801 1.925 0.654 | 0.229 0.246 0.053 | 0.147 # ctr cos2 # Temperature 18.192 0.079 | # Salinity 1.641 0.007 | # Fluorescence 0.989 0.004 | # Density 5.022 0.022 | # Acartia 4.215 0.128 | # AdultsOfCalanus 0.262 0.003 | # Copepodits1 1.764 0.022 | # Copepodits2 4.003 0.121 | # Copepodits3 0.271 0.012 | # Copepodits4 0.544 0.022 | # # Supplementary categories # Dim.1 cos2 v.test Dim.2 cos2 v.test Dim.3 # périphérique | -2.084 0.974 -7.052 | 0.257 0.015 1.090 | 0.117 # divergente1 | -0.404 0.291 -0.900 | 0.185 0.061 0.516 | 0.306 # convergente | 0.558 0.083 0.959 | 1.747 0.810 3.765 | 0.246 # frontale | 1.225 0.484 3.282 | 1.181 0.450 3.968 | -0.313 # divergente2 | 0.478 0.129 0.822 | -0.918 0.474 -1.978 | -0.419 # centrale | 0.779 0.373 3.399 | -0.988 0.599 -5.403 | 0.000 # cos2 v.test # périphérique 0.003 1.143 | # divergente1 0.166 1.968 | # convergente 0.016 1.221 | # frontale 0.032 -2.424 | # divergente2 0.099 -2.084 | # centrale 0.000 -0.002 | Ne nous intéressons pour l’instant qu’au tableau Eignenvalues qui reprend les valeurs propres, c’est-à-dire, la part de variance sur les axes de l’ACP globale réalisée. Nous voyons qu’une vairance cumulée de 74% (Cumulative % of var.) est représentée sur les deux premiers axes Dim.1 et Dim.2. Le graphe des éboulis nous représente la partition de la variance sur les différents axes comme pour l’ACP : chart$scree(mar_mfa, fill = &quot;cornsilk&quot;) Ici, on voit très clairement que deux axes suffisent. La suite de l’analyse se fait à peu près comme une ACP, mais des données et graphes supplémentaires nous permet d’interpréter les groupes de variables les uns par rapport aux autres. Le graphique des variables s’obtient comme pour l’ACP avec chart$loadings(). Il est ic plus encombré car nous avons 28 variables. La direction des différents vecteurs par rapport aux dimensions de l’AFM et entre eux nous indique la corrélation directe (même sens), inverse (opposition), ou l’absence de corrélation (vecteurs orthogonaux). Ce graphique permet aussi d’orienter le plan de l’AFM pour nos graphiques ultérieurs. chart$loadings(mar_mfa, choices = c(1, 2)) Toutes les variables environnementales sont très bien représentées dans le premier plan de l’AFM (normes quasiment de un). Nous observons une corrélation inverse entre la température (en bas à gauche du premier plan de l’AFM) et la fluorescence (qui pointe en haut à droite du plan). La salinité et la densité sont corrélées positivement entre elles, mais ne le sont pas avec les deux autres variables environnementales. Les salinités et densités élevées sont en bas à droite sur le plan de l’AFM. Pour le plancton, nous avons pas mal de choses car plusieurs vecteurs ont une norme importante (nous ignorons les autres, trop mal représentés dans le premier plan de l’AFM). Il faut pointer l’opposition entre des groupes comme GasteropodsLarvae, EchinodermLarvae, Cladocerans et Pteropods qui pointent vers la gauche, et les copépodes (Calanus, Clausocalanus, CopepoditsX, etc.) qui pointent vers la droite. Le découpage selon le second axe pour le plancton est moins net. D’autre part, les variables environnementales pour lesquelles deux dimensions fortes se dégagent sont placées en oblique par rapport à la dimension forte du plancton sur Dim 1. cela signifie que les deux dimensions environnementales sont partiellement corrélées avec la présence de ces groupes planctoniques. L’AFM est en fait un compromis réalisé entre des analyses indépendantes faites sur chaque groupe. Le graphique suivant montre comment ce compromis a été construit : chart$axes(mar_mfa, choices = c(1, 2)) Ce graphique est similaire au précédent, si ce n’est qu’au lieu de projetter les variables initiales dans le plan de l’AFM, il projette les composantes principales des différentes ACP séparées dans ce même plan. Il montre comment un compromis entre les différentes ACP est réalisé dans l’AFM globale. Ici, nous voyons que ce compromis est relativement proche de l’ACP plancton (Dim.1 et Dim.2 plancton en vert pointant dans la même direction que les axes de l’AFM). Nous voyons aussi qie les deux premières dimensions de l’ACP environnement (Dim.1 et Dim.2 environnement en rouge) pointent en oblique par rapport à ces mêmes axes. La variable supplémentaire zone est également projettée, mais elle est moins corrélée avec les autres axes (sauf sa Dim. 5 avec l’axe 1). Tout comme pour l’ACP, nous pouvons maintenant interpréter le graphique dans l’espace des individus, normalement avec chart$scores(). chart$scores(mar_mfa, choices = c(1, 2)) Cependant, nous pouvons aussi profiter de la variable supplémentaire zone pour colorer les points en fonction des masses d’eaux et représenter des ellipses pour chaque masse d’eau sur le graphique. La variante avec ces ellipses se réalise avec chart$ellipses() et il est plus clair dans pareil cas. chart$ellipses(mar_mfa, choices = c(1, 2), keepvar = &quot;Zone&quot;) Les différentes zones sont bien individualisées, à l’exception de : Certaines stations en bordure de zones (par exemple, la 16 est dans la zone de convergence1 sur le graphique alors qu’elle est libellée comme zone périphérique). La dynamique des masses d’eaux est complexe autour d’une zone frontale comme ici. Il y a des turbulences, et les frontières ne sont toujours très nettes. Ce graphique propose clairement un découpage légèrement différent… intéressant ! La zone de divergence2 n’est pas fondamentalement différente de la zone centrale. Notez qu’on a une progression le long du transect de la gauche (Nice) vers le centre, puis vers le haut à droite (le front) et puis vers le bas à droite (Calvi). Si on interpète ce graphique des individus en fonction de l’orientation donnée sur base des variables, on peut dire que : Nice (zone périphérique) s’individualise très fort du reste, et c’est le plancton qui explique essentiellement cette différence avec les groupes identifiés tirant vers la gauche (GasteropodsLarvae, EchinodermLarvae, Cladocerans et Pteropods). Nice est aussi à une température et fluorescence moyenne. Toutes les autres zones sont caractérisées par une communauté planctonique différente de Nice, mais homogène, avec la zone divergente1 qui est logiquement intermédiaire (zone de mélange des masses d’eaux périphérique et frontale). La zone frontale est caractérisée par la température la plus basse et la fluorescence la plus élevée alors que la salinité est moyenne. C’est logique : on a une remontée d’eaux froides des profondeurs riches en nutriments que le phytoplancton utilise (fluorescence élevée signifie phytoplancton élevé). La zone centrale en Corse à Calvi est caractérisée par la salinité la plus élevée, une température élevée et une fluorescence basse, caractéristiques d’une zone oligothrophe. Comme nous l’avons déjà noté, la communauté planctonique est similaire à ce qu’on rencontre au large au milieu du transect, et elle tranche sensiblement avec celle rencontrée du côté niçois. Revenons sur la façon dans l’AFM est construite (le fameux “compromis”), et son interprétation par rapport aux masses d’eaux. Le graphique suivant montre les relations entre les groupes. chart$groups(mar_mfa, choices = c(1, 2)) Ce graphique représente les groupes proches des axes dont ils sont responsables dans la structure générale de l’AFM. Nous voyons ici plancton proche de l’axe 1. Part contre, environnement est bien représenté (car il est loind du centre), mais est entre l’axe 1 et l’axes 2. Nous avons déjà noté cela précédemment. La variable supplémentaire zone est aussi représentée à proximité de la structure qui lui correspond le mieux. Nous voyons que zone est plus proche d’environnement, ce qui signifie que les masses d’eau sont plutôt structurées en rapport avec les variables environnementales qu’en rapport avec les communautés planctoniques. 8.1.3 AFM avec données mixtes Jusqu’ici, nous avons réalisé une AFM avec uniquement des données quantitatives, mais l’AFM peut aussi traiter des données qualitatives, sous forme de tableaux de contingence à double entrée, ou sous forme de variables facteurs. Nous allons tester cette variante sans sortir de notre exemple du plancton méditerranéen. En effet, le tableau plancton peut aussi être considéré comme un tableau de contingence à double entrée. Il nous suffit de changer le type de c à f (pour l’analyse des fréquences) dans la formule pour traiter à présent le plancton sous la forme d’une AFC dans mar_mfa2 : mar_mfa2 &lt;- mfa(data = mar, ~ 4*s %as% environment + 24*f %as% plancton - 1*n %as% zone) Rappelons-nous que l’AFC se ramène à une étape de son analyse à une ACP. C’est comme cela qu’il est possible de l’introduire dans l’AFM : à partir de l’étape ACP, les pondérations sont calculées et ajustées, et ensuite, une ACP globale est réalisée en la mélangeant avec les autres ACP. Au lieu d’imprimer tout à l’aide de summary(), si nous souhaitons seulement les premières valeurs propres, nous pouvons aussi les extraire de l’objet comme ceci : head(mar_mfa2$eig) # eigenvalue percentage of variance cumulative percentage of variance # comp 1 1.78441859 47.260175 47.26017 # comp 2 1.03313254 27.362427 74.62260 # comp 3 0.18926229 5.012596 79.63520 # comp 4 0.12752685 3.377538 83.01274 # comp 5 0.10759137 2.849548 85.86228 # comp 6 0.08901514 2.357558 88.21984 Ici, les deux premiers axes comptent pour 74,6% de variance cumulée. Le graphe des éboulis confirme que deux axes suffisent : chart$scree(mar_mfa2, fill = &quot;cornsilk&quot;) Le graphique des variables ne montre plus que celles relatives à l’ACP, donc seulement le groupe environnement. Les conclusions à leur sujet sont les mêmes, mais attention, le premier plan de la nouvelle AFM se présente en miroir haut-bas par rapport à notre première analyse23. chart$loadings(mar_mfa2, choices = c(1, 2)) chart$axes(mar_mfa2, choices = c(1, 2)) Les conclusions sont similaires sur les axes. Voyons maintenant l’espace des individus, toujours avec les couleurs et les ellipses par zones : chart$ellipses(mar_mfa2, choices = c(1, 2), keepvar = &quot;Zone&quot;) Encore une fois ici, le résultat est sensiblement le même (gardez à l’esprit que ce graphique est la visualisation en miroir haut-bas par rapport à l’analyse précédente). Les groupes montrent toujours le groupe zone plus proche d’environnement que de plancton. chart$groups(mar_mfa2, choices = c(1,2)) La partie AFC de l’analyse est accessible via chart$frequences() : chart$frequences(mar_mfa2, choices = c(1, 2)) Ici aussi les quatre classes de plancton EchinodermLarvae, GasteropodsLarvae, Clodocerans et Pteropods s’individualisent à gauche, contre tous les autres à droite. Également les stations proches de Nice se retrouvent à la gauche, et toutes les autres à droite. Nous voyons ici que le traitement sous forme ACP ou sous forme AFC (pour le comptage exhaustif d’espèces ou de classes taxonomiques par station, les deux sont envisageables) ne donne pas de résultats franchement différents ici. Il n’en est pas forcément toujours le cas. Veuillez à bien choisir vos types de variables dans l’AFM. Pour en savoir plus Des slides (en anglais) qui détaillent les calculs sous-jacents à l’AFM, ainsi que d’autres calculs non abordés ici (le RV, les points partiels, …), Une vidéo d’introduction à l’AFM en français (8:37, première partie d’une série de 4 vidéos), La résolution de l’exemple sur les vins de Loire présenté dans la vidéo ci-dessus dans R (en anglais). Le site de FactoMineR qui implménete la fonction MFA() sur laquelle notre analyse se base, Un exemple d’application en biologie avec des données de génomique faisant intervenir une AFM, Numerical Ecology with R traite en détail de nombreuses méthodes multivarées avec illustrations dans R (ouvrage recommandé pour tous ceux qui explorent des données multivariées en écologie, en anglais). A vous de jouer ! Réalisez le début du projet spécifique lié au module 08. Le lien pour réaliser ce projet se trouve au début du module Ce projet doit être terminé à la fin de ce module OMS = Organisation Mondiale de la Santé, voir ici.↩ Si vous êtes intéressé par l’ACM et le codage disjonctif complet, voyez cette vidéo.↩ Les ACP sont définies au signe près. or une inversion de signe dans une dimension a pour effet de faire apparaitre les graphiques en miroir.↩ "],
["indices-de-biodiversite.html", "8.2 Indices de biodiversité", " 8.2 Indices de biodiversité Au premier abord, la notion de biodiversité est simple. Elle représente toute la variété des formes de la vie des gènes jusqu’à l’écosystème. Cependant réduire toute cette richesse à un seul chiffre n’est pas quelque chose d’évident. Les scientifiques vont donc avoir recours à ce que l’on appelle des indices de diversité pour tenter de la quantifier. Ces indices sont des paramètres qui sont fréquemment utilisés pour obtenir des informations sur l’état de communautés végétales ou animales, leur viabilité ou leur évolution dans le temps par exemple. Ils permettront également d’apprécier la diversité qui peut exister entre des zones ou des milieux différents. Le choix d’un indice dépendra de la taille de l’échantillon, du type de données et de la résolution spatiale. Dès lors, il existe différentes expressions de la diversité : la diversité alpha (\\(\\alpha\\)) ou diversité locale, entre les différents relevés à l’intérieur d’une zone (diversité intrazone), la diversité bêta (\\(\\beta\\)) : diversité de relevé entre des zones différentes (diversité interzone), la diversité gamma (\\(\\gamma\\)) qui représente la richesse spécifique globale. 8.2.1 Diversité alpha 8.2.1.1 Richesse spécifique Le premier indice et le plus simple à calculer correspond à la richesse spécifique (S) et représente le nombre d’espèce que compte une communauté sans tenir compte de l’abondance relative de chacune des espèces. Vous pouvez l’obtenir très facilement avec la fonction vegan::specnumber(). Le jeu de données BCI du package vegan contient les valeurs de dénombrement obtenues suite au recensement des arbres sur 50 parcelles de 1 hectare sur l’île de Barro Colorado (BCI = Barro Colorado Island). Le nombre total d’espèces recensée sur les 50 parcelles est de 225. Prenons, un sous-ensemble de cinq parcelles de ce jeu de données et cherchons à identifier la richesse spécifique pour chacune d’entre-elles. SciViews::R bci &lt;- read(&quot;BCI&quot;, package = &quot;vegan&quot;) #library(vegan) #data(&quot;BCI&quot;) set.seed(2003) bci_sub &lt;- sample_n(bci, 5) # Exploration partielle des données (15 premières espèces) skimr::skim(bci_sub[, 1:15]) # Skim summary statistics # n obs: 5 # n variables: 15 # # ── Variable type:integer ────────────────────────────────────────────────────────────────────────────────────────────── # variable missing complete n mean sd p0 p25 p50 p75 p100 # Abarema.macradenia 0 5 5 0 0 0 0 0 0 0 # Acalypha.diversifolia 0 5 5 0 0 0 0 0 0 0 # Acalypha.macrostachya 0 5 5 0 0 0 0 0 0 0 # Adelia.triloba 0 5 5 0.6 0.89 0 0 0 1 2 # Aegiphila.panamensis 0 5 5 0.4 0.55 0 0 0 1 1 # Alchornea.costaricensis 0 5 5 3.2 3.9 1 1 1 3 10 # Alchornea.latifolia 0 5 5 0 0 0 0 0 0 0 # Alibertia.edulis 0 5 5 0 0 0 0 0 0 0 # Allophylus.psilospermus 0 5 5 0.8 0.84 0 0 1 1 2 # Alseis.blackiana 0 5 5 17.8 5.17 13 14 17 19 26 # Amaioua.corymbosa 0 5 5 0 0 0 0 0 0 0 # Anacardium.excelsum 0 5 5 0.2 0.45 0 0 0 0 1 # Andira.inermis 0 5 5 0.6 0.89 0 0 0 1 2 # Annona.spraguei 0 5 5 0.4 0.55 0 0 0 1 1 # Vachellia.melanoceras 0 5 5 0 0 0 0 0 0 0 # hist # ▁▁▁▇▁▁▁▁ # ▁▁▁▇▁▁▁▁ # ▁▁▁▇▁▁▁▁ # ▇▁▁▂▁▁▁▂ # ▇▁▁▁▁▁▁▅ # ▇▂▁▁▁▁▁▂ # ▁▁▁▇▁▁▁▁ # ▁▁▁▇▁▁▁▁ # ▇▁▁▇▁▁▁▃ # ▇▁▃▃▁▁▁▃ # ▁▁▁▇▁▁▁▁ # ▇▁▁▁▁▁▁▂ # ▇▁▁▂▁▁▁▂ # ▇▁▁▁▁▁▁▅ # ▁▁▁▇▁▁▁▁ # Calcul de la Richesse spécifique pour chacune des parcelles vegan::specnumber(bci_sub) # [1] 102 87 84 84 109 Comme on peut le voir, cet indice permet de connaitre le nombre d’espèce présent sur chaque parcelle. Mais est-ce que chaque espèce est présente de manière équitable ? Ou est-ce que certaines espèces sont plus abondante que d’autres ? Cet indice ne tient pas compte de cette abondance dans son calcul et ne permet donc pas de répondre à ces questions. Par contre, l’indice de Shannon peut nous aider à répondre à cette question. 8.2.1.2 Indice de Shannon L’indice de Shannon ou de Shannon-Weaver, introduit en écologie comme une mesure de la stabilité des communautés, prend en compte lors de son calcul la richesse et l’abondance relative des espèces contrairement à la richesse spécifique. La formule mathématique de l’indice de Shannon est la suivante : \\[H = - \\sum_{i=1}^S p_i \\ log_b\\ p_i\\] où : - \\(p_i\\) représente l’abondance proportionnelle de l’espèce et est compris entre 0 et 1 : \\(p_i = \\frac{n_i}{N}\\) - \\(S\\) est la richesse spécifique - \\(b\\) la base du logarithme - \\(n_i\\) est le nombre d’individus d’une espèce dans l’échantillon - \\(N\\) est le nombre total d’individu de toutes les espèces dans l’échantillon : \\(N = \\sum_{i=1}^S n_i\\) L’indice H de Shannon varie donc en fonction du nombre d’espèce et de la proportion relative de ces différentes espèces. H vaudra 0 quand l’échantillon ne contient qu’une seule espèce et augmente lorsque le nombre d’espèce augmente. Plus l’indice H est élevé, plus la diversité est grande. H sera maximal et vaudra \\(log_b S\\) quand toutes les espèces sont également représentées. Pour calculer l’indice de Shannon avec R, vous pouvez utiliser la fonction vegan::diversity(). Celle-ci demande comme argument : x : des données sur une communauté sous la forme d’un vecteur ou d’une matrice index : le choix d’un indice de biodiversité, ici &quot;shannon&quot; (valeur pas défaut, donc, pouvant être omis ici) base : la base du logarithme lors du calcul de l’indice de Shannon. Par défaut, la fonction utilise le logarithme népérien. bci_sub_h &lt;- vegan::diversity(bci_sub) bci_sub_h # [1] 3.920918 3.859814 3.698414 3.848471 4.013094 8.2.1.3 Indice d’équitabilité de Piélou L’indice de Shannon est rarement utilisé seul. Il est souvent accompagné de l’indice d’équitabilité de Piélou qui permet de mesurer la répartition des individus au sein des espèce. Il s’agit là d’un paramètre plus rigoureux et très utile pour comparer des dominances potentielles entre sites puisqu’il est indépendant de la richesse spécifique. Il traduit donc le degré de diversité qui est atteint par un peuplement et se calcul comme suit : \\[J = \\frac{H}{H_{max}}\\] où : - \\(H\\) correspond à l’indice de Shannon - \\(H_{max}\\) correspond à la valeur de la diversité théorique maximale (\\(log_b\\ S\\)) - \\(S\\) est la richesse spécifique La valeur de l’indice d’équitabilité de Piélou (J) varie donc entre 0 et 1 où 0 correspond à la dominance d’une des espèces et 1 à l’équirépartition des individus entre les différentes espèces. Il n’existe pas de fonction pour calculer cette indice dans R mais on peut le calculer facilement à partir de l’indice de Shannon que l’on vient de calculer et avec la fonction vegan::specnumber() pour connaitre la richesse spécifique. bci_sub_s &lt;- vegan::specnumber(bci_sub) bci_sub_s # [1] 102 87 84 84 109 bci_sub_j &lt;- bci_sub_h / ln(bci_sub_s) bci_sub_j # [1] 0.8477709 0.8642843 0.8347024 0.8685692 0.8554245 Il est important de noter que ces deux indices restent dépendants de la taille des échantillons et sont sensibles aux espèces rares. 8.2.1.4 Indice de Simpson Il existe plusieurs indices permettant d’évaluer la biodiversité parmi lesquels on retrouve l’indice de Simpson. Cet indice aussi appelé indice de dominance mesure la probabilité que deux individus tirés au hasard à partir d’un échantillon appartiennent à la même espèce. Avec cet indice, on donne plus de poids aux espèces abondantes par rapport aux espèces rares. Dès lors, l’ajout d’une espèce rare à un échantillon ne modifiera pratiquement pas la valeur de l’indice de diversité. \\[D = \\sum_{i=1}^S p_i^2\\] où : \\(p_i\\) représente l’abondance proportionnelle de l’espèce et est compris entre 0 et 1 : \\(p_i = \\frac{n_i}{N}\\) \\(S\\) est la richesse spécifique Sous cette forme, l’indice est inversement proportionnel à la diversité. La formulation suivante a donc été proposée pour que l’indice soit directement représentatif de la diversité. \\[E = 1 - \\sum_{i=1}^S p_i^2\\] L’indice de Simpson varie dans l’intervalle \\([0,1[\\). Cet indice tend donc vers 0 lorsque la diversité est minimale et vers \\(1 - \\frac{1}{S}\\) lorsque la diversité est maximale. Il vaudra donc 0 si une seule espèce est présente et donc une probabilité de \\(p_i = 1\\) et \\(1 - \\frac{1}{S}\\) si les différentes espèces présentes ont la même probabilité \\(p_i = \\frac{1}{S}\\). Le calcul de cet indice dans R se fait aussi avec la fonction vegan::diversity(, index = &quot;simpson&quot;). bci_sub_e &lt;- vegan::diversity(bci_sub, index = &quot;simpson&quot;) bci_sub_e # [1] 0.9672083 0.9658398 0.9550599 0.9683393 0.9655820 8.2.2 Diversité beta La diversité \\(\\beta\\) traduit la diversité inter-formation. Pour identifier le degré de similarité, d’association entre des groupes ou la diversité de différenciation des espèces entre différents habitats, on utilise les coefficient ou indice de similarité ou de similitude. Le choix adéquat d’un indice de similarité, n’est pas évident et la question à se poser est de savoir si le fait qu’une espèce soit absente ou non contribue à augmenter la dissimilarité. Il existe plusieurs indices de similarité qui s’appliquent sur des données de présence-absence comme l’indice de Jaccard que nous allons vous présenter ici. Cet indice permet une comparaison entre deux sites car il évalue la ressemblance en calculant le rapport entre les espèces communes aux deux sites et celles propre à chaque relevé. La formule est la suivante : \\[I = \\frac{N_c}{N_1 + N2 – N_c}\\] où : \\(N_c\\) correspond au nombre de taxon commun entre les deux sites \\(N_1\\) et \\(N_2\\) le nombre de taxons présents sur le site 1 et 2, respectivement Les valeurs de l’indice varient entre 0 lorsque les deux sites n’ont aucune espèce en commun, et 1 quand les deux sites ont toutes leurs espèce en commun. Dès lors plus la valeur est proches de 1, plus les sites sont similaire. Dans R, l’indice qui est calculé avec la fonction vegan::vegdist() est un indice de dissimilarité. L’indice de similarité est complémentaire à l’indice de dissimilarité et se calcule comme suit : \\[ similarité = 1 – dissimilarité\\] Regardons comment colculer l’indice de Jaccard pour nos cinq parcelles avec R : 1 - vegan::vegdist(bci_sub, method = &quot;jaccard&quot;, binary = TRUE) # 1 2 3 4 # 2 0.4765625 # 3 0.5000000 0.4869565 # 4 0.4418605 0.6132075 0.4867257 # 5 0.5629630 0.5193798 0.5196850 0.5078125 Pour en savoir plus L’introduction des indices de diversité dans vegan par son auteur (en anglais), Un document très détaillé décrivant les différents indices de diversité en français, Numerical Ecology with R traite en détail de nombreuses méthodes multivarées avec illustrations dans R (ouvrage recommandé pour tous ceux qui explorent des données multivariées en écologie, en anglais). A vous de jouer ! Réalisez la suite du projet spécifique lié au module 08. Le lien pour réaliser ce projet se trouve au début du module Ce projet doit être terminé à la fin de ce module Complétez votre projet sur le transect entre Nice et Calvi débuté lors du module 5. Lisez attentivement le README (Ce dernier a été mis à jour). Il serait intéressant de connaitre la station avec la haute diversité. Comment évolue la diversité le long du transect ? Complétez votre projet. Lisez attentivement le README. La dernière version du README est disponible via le lien suivant : https://github.com/BioDataScience-Course/spatial_distribution_zooplankton_ligurian_sea/blob/master/README.md Le README est un rappel des consignes, il ne s’agit aucunement du lien pour débuter le travail "],
["open-data.html", "8.3 Open Data", " 8.3 Open Data L’“Open Data” ou données ouvertes est un phénomène en pleine expansion. Vous y serez certainement confrontés dans votre carrière. Autant bien comprendre de quoi il s’agit et rendre vos données accessibles et publiques de la meilleure façon qui soit. Cette dernière partie est une brève introduction en la matière, qui devrait vous permettre de bien démarrer. 8.3.1 Gestion des données Lors de la préparation d’une expérience, vous devez réfléchir à un plan d’expérience. Vous avez donc défini des protocoles d’expérience, le nombre de réplicas,… Vous devez cependant intégrer à votre réflexion, un plan de gestion de vos données. Dans ce plan, vous aurez à définir l’acquisition, la description ou encore le partage des données. 8.3.1.1 Principe FAIR Pour assurer une gestion cohérente des données scientifiques, il faut respecter le plus possible l’acronyme en anglais FAIR : Findable, Accessible, Interoperable, Reusable (Wilkinson et al. 2016). Acronyme en anglais Acronyme en français Description Findable Facile à trouver Les données ont besoin d’un code unique et persistant pour les retrouver Accessible Accessible Les données et surtout les métadonnées avec une license sont mises à disposition. Interoperable Intéropérable Les données et les métadonnées doivent respecter les standards internationaux Reusable Réutilisable Les données doivent être réutilisable grâce à des métadonnées riches et des licenses claires 8.3.1.1.1 Facile à retrouver (Findable) Vos données et vos métadonnées détaillées doivent être facile à retrouver. Vous devez donc fournir un identifiant unique et permanent. Il existe de nombreux identifiants comme ISBN, ISSN, DOI, … Dans le cadre de la recherche scientifique, Le Digital Object Identifier (DOI) est la méthode standardisée conseillée. Vous avez déjà été confronté à des DOI. Par exemple, le DOI suivant https://doi.org/10.1038/sdata.2016.18 fait référence à l’article The FAIR Guiding Principles for scientific data management and stewardship. Ce code est unique et persistant. Ce code va toujours renvoyer vers cet article de la revue Scientific Data. Imaginons que la revue Scientific Data disparaissent, le DOI sera toujours associé à cet article. Ainsi, la publication pourra être retrouvée ailleurs sur Internet grâce à ce DOI. Le DOI ne couvre pas que les articles scientifiques, il est également utilisé pour les données. Par exemple, le DOI suivant https://doi.org/10.5281/zenodo.3711592 fait référence au données intitulé. Dataset: Number of diagnoses with coronavirus disease (COVID-19) in The Netherlands publié par Zenodo. Nous reviendrons dans les sections suivantes sur zenodo. 8.3.1.1.2 Accessible (Accessible) Les données et les métadonnées que vous collectez doivent de plus en plus souvent être rendue disponible. Certaines revues scientifiques requièrent la mise à disposition des données. Les recherches financées par des fonds publics (nationaux, Européen,…) requièrent également la mise à disposition des données. Les données ne doivent pas être disponible à tous. Par contre les métadonnées doivent l’être. Il est, de plus, important de préciser la procédure afin d’obtenir les données. Il parait presque logique et évident de mettre à disposition ces données afin d’en faire profiter la recherche académique dans son ensemble. La recherche va progresser plus rapidement si les chercheurs collaborent. Un scientifique seul dans son laboratoire ne peut pas espérer progresser plus rapidement que 20 scientifiques qui collaborent et utilisent les données. Il ne s’agit cependant pas de donner ces données sans aucune sécurité. En effet, il serait frustrant de travailler très dur sur un sujet précis et qu’un autre scientifique vole le fruit de ces nombreuses heures de travail et publie un article avant vous. Il existe une solution pour spécifier les droits d’utilisation de vos données. Vous devez associer une licence à vos données et métadonnées. Vous avez très certainement déjà entendu parlé des licences Creative Commons. Il est de plus en plus courant de voir apparaitre ce genre d’information sur des sites web comme CC0, CC-by, ou encore CC-by-sa. Vous êtes peut être plus familié avec les logos ci-dessous : . Que se cache-t’il derrière ces logos ? Nous allons détailler ensemble ces abréviations. Nous pouvons résumer cela de manière simple en se posant deux questions : Souhaitez-vous autoriser le partage des adaptations de votre œuvre ? Oui Non Oui, sous condition de partage dans les mêmes conditions. Autorisez-vous les utilisations commerciales de votre œuvre ? Oui Non Ces deux questions proviennent de l’outil mis à disposition sur le site creative commons https://creativecommons.org/choose/ pour définir la licence la plus adaptée pour vous. Dans le cadre de la recherche, tout n’est pas si simple. Vous devez tenir compte l’avis de vos supérieurs hiérarchiques, de votre institution et de la ou des institutions qui financent le travail. La bonne pratique est donc de discuter avec l’ensemble des acteurs pour décider de la bonne licence à employer, le plus tôt possible, c’est-à-dire, déjà lors de l’élaboration du projet de recherche. Repartons de notre jeu de données sur le COVID-19 : Dataset: Number of diagnoses with coronavirus disease (COVID-19) in The Netherlands. Nous pouvons voir que l’auteur a décidé d’employer la licence Ce(tte) œuvre est mise à disposition selon les termes de la Licence Creative Commons Attribution 4.0 International. Il est donc autorisé d’adapter l’œuvre et de l’utiliser à des fins commerciales. Il s’agit d’une licence très peu contraignantes. Il suffit simplement de créditer l’auteur de l’oœuvre originale. Il existe également des licences plus spécifiques aux bases de données, de la moins contraignante à la plus contraingante PDDL , ODC-by et ODbL 8.3.1.1.3 Intéropérable (Interoperable) Les données sont associées à des métadonnées riches. Sur base des métadonnées, les données doivent être utilisable, compréhensible et combinable avec d’autres données. Ce principe est difficile à mettre en place et requiert donc une réflexion approfondie. 8.3.1.1.4 Réutilisable (Reusable) Les données doivent autant que possible être associées à des métadonnées riches avec une licence claire afin de pouvoir être réutilisées, nous l’avons déjà vu. Vous avez certainement le sentiment que ces quatre principes se mélangent un peu. En effet, ils insistent avec des petites nuances sur des concepts particuliers. En Résumé : \\[Données \\ inutilisables = données \\ seules\\] \\[Données \\ utilisables = données + contexte\\] Le contexte c’est : un code unique et persistant associé aux données et au contexte, une description du projet associée aux données, des metodonnées riches (y compris un dictionnaire des données24), la licence associée aux données? Dans le cadre de vos futures recherches, un outil comme Zenodo est très intéressant pour publier vos donnes sur l’Internet tout en suivant au mieux le principe FAIR 8.3.1.2 DMP Afin de respecter ces principes FAIR, des outils ont été développés. Il s’agit des plans de gestion des données (ou Data Management Plan, DMP). L’Université de Mons dispose d’un DMP. Cet outil est partagé par l’ensemble des universités de Belgique. Lorsque vous allez concevoir un plan d’expérience, n’oubliez pas de concevoir votre plan de gestion des données en même temps. Voici une checklist pour un plan de gestion des données efficaces http://www.dcc.ac.uk/sites/default/files/documents/resource/DMP_Checklist_2013.pdf A vous de jouer ! Réalisez la fin du projet spécifique lié au module 08. Le lien pour réaliser ce projet se trouve au début du module Ce projet doit être terminé à la fin de ce module 8.3.2 Utilisation de données ouvertes (Open Data) Il existe de nombreux sites qui regroupent un ensemble de données que nous allons appeler Open Data. Nous avons parlé précédemment de Zenodo mais de nombreuses bases de données sont également disponibles comme le Portail européen de données, Portail belge de données, … Afin de connaitre la qualité des données voici une checklist très utile pour appréhender des données ouvertes. Vous devez être capable de trouver facilement : But des données Code unique et persistant des données Licence des données Format des données Qualité des données A nouveau, vous vous rendez compte que nous revenons à notre principe FAIR expliqué plus haut. Prenons notre exemple sur le Dataset: Number of diagnoses with coronavirus disease (COVID-19) in The Netherlands et appliquons notre checklist. Le but des données Une description des données est proposée. Le nom de l’auteur est spécifié. Il est également précisé la date de publication avec la version des données. Le 16 mars 2020, la version est v2020.3.16. Les données sont également associées à un dépôt Github qui les traitent : J535D165/CoronaWatchNL Un code unique et persistant Ces données ont un DOI : La licence Les données sont mise à disposition avec la licence Creative Commons Attribution 4.0 International le format Les données sont proposées sous le format csv. Ce format est un standard très employé. Il est à privilégier par rapport au format .xls ou .xlsx. Il est à la fois interopérable et réutilisable facilement. La qualité Ce dernier critère est le plus difficile à déterminer. Une première chose à vérifier concerne les métadonnées associées à chaque variable. Nous pouvons voir que l’auteur peut encore améliorer les métadonnées associées à ses données. Le nom des variables reste cependant tout à fait compréhensible. Comme vous venez de le voir, Zenodo de part sa structuration permet de remplir très simplement cette checklist. Pour en savoir plus Qu’est ce que l’Open Data ? Aide sur l’interprétation et le choix des licences Choisir la bonne licence Open Source Guide sur les licences open data Des stockages spécifiques ont été mis en place pour les données scientifiques comme Zenodo (dépot des données hebergé par le CERN), Dataverse, ou encore Figshare Data Management Plan Le Principe FAIR expliqué par l’Observatoire Global du Saint-Laurent. Article scientifique sur le FAIR plan : The FAIR Guiding Principles for scientific data management and stewardship L’outil institutionnel de l’Université de Mons afin de réaliser un plan de gestion de données est disponible DMPonline.be Références "],
["svbox.html", "A Installation de la SciViews Box", " A Installation de la SciViews Box Pour ce cours SDD 2, nous utiliserons la même SciViews Box que pour le cours 1… mais actualisée (version de l’année). Vous allez donc devoir installer la nouvelle version. La procédure n’a changé que sur des points de détails. Référez-vous à l’appendice A1 du cours SDD 1. Vous pouvez conserver l’ancienne SciViews Box en parallèle avec cette nouvelle version, mais vérifiez si vous avez assez d’espace sur le disque dur pour contenir les deux simultanément. Comptez par sécurité 20Go par version. Si vous manquez de place, vous pouvez éliminer l’ancienne version avant d’installer la nouvelle (vos projets ne seront pas effacés). "],
["migration-des-projets.html", "A.1 Migration des projets", " A.1 Migration des projets Concernant les projets réalisés dans une version précédente de la SciViews Box, ceux-ci restent disponibles, même si vous éliminez l’ancienne. Plusieurs cas de figure se présentent : Vous conserver deux ou plusieurs version de la SciViews Box en parallèle. Dans ce cas, nous conseillons fortement de garder chaque projet accessible à partir de la version dans laquelle il a été créé. Seulement les projets que vous décidez de migrer explicitement (voir ci-dessous) seront à déplacer dans le dossier shared de la nouvelle SciViews Box. Vous aurez à faire cette manipulation, par exemple, si vous devez recommencer un cours l’année suivante afin d’être en phase (même version de la svbox) par rapport à vos nouveaux collègues. Vous ne conservez que la dernière version de la SciViews Box, mais ne devez pas accéder fréquemment vos anciens projets, et dans ce cas, vous pouvez réinstaller temporairement l’ancienne version de svbox. Dans ce cas, ne migrez pas vos anciens projets. Éliminez simplement l’ancienne svbox, tout en laisant vos projets intacts dans son répertoire shared. Lors de la réinstallation de l’ancienne svbox, vous retrouverez alors tous vos anciens projets intactes. Vous ne conservez pas d’ancienne version de la svbox et vous ne souhaitez pas devoir la réinstaller. Il est possible de migrer vos anciens projets en les déplaçant de l’ancien répertoire shared vers le nouveau. Soyez toutefois conscients que vos documents R Markdown et scripts R ne fonctionneront pas forcément dans la nouvelle svbox et qu’une adaptation sera peut-être nécessaire ! "],
["configuration-git-et-github.html", "A.2 Configuration Git et Github", " A.2 Configuration Git et Github A chaque nouvelle installation de la SciViews Box, vous devez la reconfigurer via la boite de dialogue SciViews Box Configuration. En particulier, il est très important d’indiquer correctement votre identifiant et email Git (zone encadrée en rouge dans la copie d’écran ci-dessous). Assurez-vous (si ce n’est déjà fait) que vous possédez un compte Github valide. Vous pouvez cliquer sur le bouton Go to Github par facilté dans la même boite de dialogue. Choisissez de manière judicieuse votre login. Vous pourriez être amenés à l’utiliser bien plus longtemps que vous ne le pensez, y compris plus tard dans votre carrière. Donc, lisez les conseils ci-dessous (inspirés et adaptés de Happy Git and Github for the UseR - Register a Github Account : Incluez votre nom réel. Les gens aiment savoir à qui ils ont affaire. Rendez aussi votre nom/login facile à deviner et à retenir. Philippe Grosjean a comme login phgrosjean, par exemple. Vous pouvez réutiliser votre login d’autres contextes, par exemple Twitter ou Slack (ou Facebook). Choisissez un login que vous pourrez échanger de manière confortable avec votre futur boss. Un login plus court est préférable. Soyez unique dans votre login, mais à l’aide d’aussi peu de caractères que possible. Github propose parfois des logins en auto-complétion. Examinez ce qu’il propose. Rendez votre login invariable dans le temps. Par exemple, n’utilisez pas un login lié à votre université (numéro de matricule, ou nom de l’université inclue dans le login). Si tout va bien votre login vous suivra dans votre carrière, … donc, potentiellement loin de l’université où vous avez fait vos études. N’utilisez pas de logins qui sont aussi des mots ayant une signification particulière en programmation, par exemple, n’utilisez pas NA, même si c’est vos initiales ! Une fois votre compte Github créé, et votre login/email pour votre identification Git correctement enregistrés dans la SciViews Box, vous devez pouvoir travailler, faire des “pushs”, des “pulls” et des “commits”25. Cependant, RStudio vous demandera constamment vos logins et mots de passe… à la longue, c’est lassant ! La procédure ci-dessous vous enregistre une fois pour toutes sur votre compte Github dans RStudio. A.2.1 Compte Github dans RStudio RStudio offre la possibilité d’enregistrer une clé publique/privée dans votre SciViews Box afin de vous enregistrer sur Github de manière permanente. L’avantage, c’est que vous ne devrez plus constamment entrer votre login et mot de passe à chaque opération sur Github ! Nous vous le conseillons donc vivement. Entrez dans Rstudio Server, et allez dans le menu Tools -&gt; Global Options.... Ensuite, cliquez dans la rubrique Git/SVN dans la boite de dialogue. Ensuite, cliquez sur le bouton Create RSA key.... La phrase de passe n’est pas nécessaire (il est même préférable de la laisser vide si vous voulez utiliser Github sans rien devoir taper à chaque fois). Cliquez sur le bouton Create. Vous obtenez alors une fenêtre similaire à celle ci-dessous (bien sûr avec des données différentes). Ceci confirme que votre clé cryptographique a été créée localement. Fermez cette fenêtre pour revenir à la boite de dialogue de configuration de RStudio Server. Dans la boite de dialogue de configuration de RStudio Server, section Git/SVN cliquez sur le lien View public key qui apparait une fois la clé créée : La clé apparait dans une fenêtre, déjà présélectionnée. Copiez-là dans le presse-papier (Ctrl-C ou clic bouton droit et sélection de Copy dans le menu contextuel), puis fermez cette fenêtre. Dans votre navigateur web favori, naviguez vers https://github.com, loggez-vous, et accédez aux paramètres de votre compte Github (menu déroulant en haut à droite, entrée Settings) : Dans les paramètres de votre compte, cliquez sur la rubrique SSH and GPG keys, ensuite sur le bouton vert New SSH key Collez-y votre clé à partir du presse-papier dans la zone Key. Vous pouvez lui donner un nom évocateur dans le champ Title. Ensuite, cliquez sur Add SSH key. Déloggez, puis reloggez-vous dans RStudio Server pour que les changements soient pris en compte. La prochaine action sur Github depuis RStudio pourrait encore déclencher la demande de votre login et mot de passe, mais ensuite, les opérations devraient se faire directement. Si vous éprouvez toujours des difficultés à faire collaborer R et RStudio avec Git et Github, voyez Happy Git and Github for the UseR (en anglais) qui explique les différentes procédures bien plus en détails. Vérifiez toujours lors de votre premier commit que Github vous reconnait bien. Pour cela, naviguez vers le dépôt où vous avez commité avec votre explorateur web, et vérifiez l’identité prise en compte lors de votre commit.↩ "],
["references.html", "Références", " Références "]
]
